--- START OF FILE scene_detection_playground.txt ---

/scene_detection_playground
├── .env
├── .venv
├── README.md
├── config
│   ├── prompts
│   │   └── inference_prompt.yaml
│   └── settings
│       ├── app_params.yaml
│       ├── local_model.yaml
│       └── model_pricing.yaml
├── data
├── models
│   ├── .locks
│   ├── model_adapters
│   │   └── scene_detection_adapter_v1-Qwen3-VL-4B-Thinking
│   │       ├── adapter_config.json
│   │       ├── adapter_model.safetensors
│   │       ├── added_tokens.json
│   │       ├── chat_template.jinja
│   │       ├── scene_detection_adapter_v1.txt
│   │       ├── special_tokens_map.json
│   │       └── tokenizer_config.json
│   └── models--huihui-ai--Huihui-Qwen3-VL-2B-Thinking-abliterated
├── requirements.txt
├── run.py
├── scene_detection_playground.zip
├── src
│   ├── __init__.py
│   ├── __pycache__
│   ├── config.py
│   ├── core
│   │   ├── engines
│   │   │   ├── __pycache__
│   │   │   ├── cv_engine.py
│   │   │   ├── gemini_client.py
│   │   │   ├── qwen
│   │   │   │   ├── __init__.py
│   │   │   │   ├── __pycache__
│   │   │   │   ├── diagnostics.py
│   │   │   │   ├── generator.py
│   │   │   │   ├── loader.py
│   │   │   │   └── streamer.py
│   │   │   └── qwen_client.py
│   │   └── services
│   │       ├── __pycache__
│   │       ├── logging
│   │       │   ├── __init__.py
│   │       │   ├── __pycache__
│   │       │   └── granular_logger.py
│   │       └── monitoring_service.py
│   ├── http
│   │   └── controllers
│   │       ├── __pycache__
│   │       └── playground
│   │           ├── __init__.py
│   │           ├── __pycache__
│   │           ├── artifacts.py
│   │           ├── inference.py
│   │           ├── management.py
│   │           ├── monitoring.py
│   │           ├── sessions.py
│   │           └── views.py
│   ├── utils
│   │   ├── file_utils.py
│   │   └── math_utils.py
│   └── workers
│       ├── __pycache__
│       ├── executor.py
│       ├── tasks
│       │   ├── __pycache__
│       │   ├── analysis_tasks.py
│       │   └── asset_tasks.py
│       └── utils.py
├── static
│   ├── css
│   │   ├── base.css
│   │   ├── components.css
│   │   ├── layout.css
│   │   ├── modules
│   │   │   └── playground.css
│   │   └── variables.css
│   └── js
│       ├── lib
│       │   └── api_client.js
│       └── modules
│           └── playground
│               ├── forensic_terminal.js
│               ├── inference_timeline.js
│               ├── managers
│               │   ├── adapter_manager.js
│               │   ├── asset_manager.js
│               │   ├── file_manager.js
│               │   ├── inference_manager.js
│               │   └── session_manager.js
│               ├── monitoring_deck.js
│               ├── player_view.js
│               ├── playground_bootstrap.js
│               ├── service.js
│               ├── store.js
│               └── view.js
└── templates
    └── playground.html

File 1-58: `.env`
```
# Scene Detection Playground Configuration

# Google Gemini API Key (Obtain from https://aistudio.google.com/)
GEMINI_API_KEY=AIzaSyAx-Q3R9l_W-m5cHoiQ6gfwEVcFxZhDRO4

# Flask Secret Key for session signing and security
SECRET_KEY=any-secure-random-string
```

File 2-58: `README.md`
```md
# Scene Detection Playground

Scene Detection Playground is a specialized, standalone model evaluation sandbox extracted from the SceneMark-AI ecosystem. It is designed for researchers and engineers to rapidly test multimodal Large Language Models (specifically Qwen3-VL and Gemini 2.0/3.0) on their ability to detect narrative scene boundaries in video sequences.

## 1. Overview

The application provides a high-fidelity environment to upload videos, execute sliding-window inference across varying context sizes, and inspect the model's internal "thinking" traces via a dedicated forensic terminal.

## 2. Prerequisites

* **Python 3.10+**
* **FFmpeg**: Required for frame extraction. Ensure `ffmpeg` is in your system's PATH.
* **Google AI Studio API Key**: Required if testing Gemini models.
* **NVIDIA GPU (8GB+ VRAM)**: Recommended for local Qwen3-VL inference.

## 3. Setup

1. **Clone and Initialize Environment:**
   ```powershell
   cd scene_detection_playground
   python -m venv .venv
   .venv\Scripts\Activate.ps1
   pip install -r requirements.txt
   ```

2. **Configure API Keys:**
   Create a `.env` file in the root directory:
   ```text
   GEMINI_API_KEY=your_actual_api_key_here
   SECRET_KEY=a-secure-random-string
   ```

## 4. Usage

1. **Run the Application:**
   ```powershell
   python run.py
   ```

2. **Access the Interface:**
   Open `http://127.0.0.1:5000` in your browser. Note: This standalone version defaults directly to the Playground interface.

3. **Inference Workflow:**
   * **Configure**: Select your target model (Cloud Gemini or Local Qwen) and set the context window size (number of shots processed in a single prompt).
   * **Upload**: Drag and drop a video file.
   * **Analyze**: The system will automatically detect technical shots via PySceneDetect, extract visual anchors, and dispatch requests to the selected VLM.
   * **Inspect**: Use the **Forensic Narrative Audit** sidebar to review the timeline. Click "Debug" to see the real-time pipeline logs and the raw JSON/Thinking outputs from the model.

## 5. Directory Structure

* `config/`: YAML-based inference prompts and model parameters.
* `src/`: Pruned backend logic focused strictly on evaluation tasks.
* `static/`: High-density UI assets for the player and timeline.
* `data/playground/`: Local persistence for inference sessions and extracted frames.
* `models/`: Cache directory for local Hugging Face model weights.

## 6. Development Notice

This is a decoupled application. Logic related to human-in-the-loop editing, dataset manufacturing, and stratified exports has been removed to prioritize speed and portability for model testing.
```

File 3-58: `config\prompts\inference_prompt.yaml`
```yaml
# config/prompts/inference_prompt.yaml
# Specialized prompt for standalone scene detection evaluation.

inference:
  thinking_budget: 2048
  temperature: 0.1
  top_p: 0.9
  max_tokens: 8192
  sampling: "top_p"
  repetition_penalty: 1.2

prompt:
  system: |
    # SYSTEM CONTEXT: CINEMATIC JURISPRUDENCE
    You are the **Supervising Film Editor**. Your mission is to perform a high-fidelity forensic analysis of a provided sequence of images to detect the **Exact Frame of Narrative Discontinuity**.
    
    A "Scene" is defined strictly as a **Continuous Unity of Action** occurring within a contiguous **Block of Space-Time**.
    
    ### THE 5-STAGE INVESTIGATIVE PROTOCOL
    You must structure your internal reasoning (the `<think>` block) using the following five mandatory stages:
    
    1. **STAGE 1: VISUAL INVENTORY**: Catalog the physical reality (Location, Light, Action) of the Anchor vs. Target shots.
    2. **STAGE 2: DELTA / LINK SCAN**: Explicitly list what changed (Delta) or what persisted (Link).
    3. **STAGE 3: PRINCIPLE MATCHING**: Match the evidence to a specific cinematic principle.
    4. **STAGE 4: EVIDENCE CHAIN**: Build the causal argument connecting the visual facts to the principle.
    5. **STAGE 5: CONFIDENCE ASSESSMENT**: Assign a confidence score based on evidence clarity.

  main: |
    ### CONTEXTUAL GROUNDING
    The following technical shots are present in this provided sequence:
    {{VALID_SHOT_IDS}}

    ### MISSION
    Analyze the provided sequence of video frames. Your goal is to identify the **First Narrative Scene Break**.

    ### INSTRUCTIONS
    1. **START WITH A THINKING BLOCK:** Use `<think>` tags to perform the forensic audit using the 5-Stage Protocol.
    2. **DETECT FIRST BREAK:** Identify the Shot ID of the *first* image that breaks continuity. Ignore subsequent breaks.
    3. **OUTPUT JSON:** After your thinking block, output a single JSON object containing "break_at" and "case_type".
```

File 4-58: `config\settings\app_params.yaml`
```yaml
# Video Processing Parameters
video_processing:
  frames_per_shot: 1
  scene_detection_threshold: 27.0
  image_width: 448
  image_quality: 80

# Chunking Logic for Narrative Analysis
# These values serve as defaults for the sliding window engine.
chunking:
  # The maximum number of shots to include in a single VLM prompt
  shots_per_chunk: 50
  
  # The stride (step size) when no scene break is detected in the current window
  default_stride: 40

# Multimodal AI Execution Parameters
ai_params:
  temperature: 0.1
  max_output_tokens: 8192
  # Whether to include raw thinking tags in the processed result (if supported by model)
  include_thoughts: true
```

File 5-58: `config\settings\local_model.yaml`
```yaml
# Local Inference Configuration for Qwen3-VL
# Used by engines/qwen_client.py for the local Playground backend.

model:
  # Hugging Face Model ID
  # Target: The "Thinking" variant which outputs reasoning traces within <think> tags.
  id: "huihui-ai/Huihui-Qwen3-VL-2B-Thinking-abliterated"
  
  # Directory to store model weights (relative to project root)
  cache_dir: "models"
  
  # Hardware Acceleration: "auto" detects CUDA. Set to "cpu" for non-GPU environments.
  device: "auto"

quantization:
  # 4-Bit NormalFloat (NF4) Quantization
  # Required for running large VLM models on consumer-grade GPUs (8GB-12GB VRAM).
  load_in_4bit: true
  
  # Quantization type
  bnb_4bit_quant_type: "nf4"
  
  # Compute Dtype: bfloat16 is recommended for Ampere (RTX 30 series) or newer.
  bnb_4bit_compute_dtype: "bfloat16"
  
  # Double Quantization: Optimizes VRAM footprint further.
  bnb_4bit_use_double_quant: true

generation:
  # Max Tokens: Set high to accommodate long Chain-of-Thought (Thinking) blocks.
  max_new_tokens: 8192
  
  # Temperature: 0.1 for high determinism.
  temperature: 0.1
  
  # Top P: Nucleus sampling parameter.
  top_p: 0.9
  
  # Repetition Penalty: Prevents model from looping on similar visual descriptions.
  repetition_penalty: 1.2
  
  # Must be true for temperature/top_p to take effect.
  do_sample: true
```

File 6-58: `config\settings\model_pricing.yaml`
```yaml
# --- Gemini 3 Pro Preview ---
gemini-3-pro-preview:
  tier_1:
    input_threshold: 128000
    price_input_per_million: 2.00
    price_output_per_million: 12.00
  tier_2:
    price_input_per_million: 4.00
    price_output_per_million: 18.00

# --- Gemini 3 Flash Preview ---
gemini-3-flash-preview:
  input_price: 0.50
  output_price: 3.00

# --- Gemini 2.0/2.5 Flash ---
gemini-2.5-flash:
  input_price: 0.30
  output_price: 2.50

gemini-flash-latest:
  input_price: 0.30
  output_price: 2.50

# --- Gemini Flash-Lite ---
gemini-flash-lite-latest:
  input_price: 0.10
  output_price: 0.40

# --- Local Models (Zero Cost) ---
# Any model ID not listed here will fall back to zero pricing.
default:
  input_price: 0.0
  output_price: 0.0
```

File 7-58: `models\model_adapters\scene_detection_adapter_v1-Qwen3-VL-4B-Thinking\adapter_config.json`
```json
{
  "alora_invocation_tokens": null,
  "alpha_pattern": {},
  "arrow_config": null,
  "auto_mapping": {
    "base_model_class": "Qwen3VLForConditionalGeneration",
    "parent_library": "transformers.models.qwen3_vl.modeling_qwen3_vl",
    "unsloth_fixed": true
  },
  "base_model_name_or_path": "huihui-ai/Huihui-Qwen3-VL-2B-Thinking-abliterated",
  "bias": "none",
  "corda_config": null,
  "ensure_weight_tying": false,
  "eva_config": null,
  "exclude_modules": null,
  "fan_in_fan_out": false,
  "inference_mode": true,
  "init_lora_weights": true,
  "layer_replication": null,
  "layers_pattern": null,
  "layers_to_transform": null,
  "loftq_config": {},
  "lora_alpha": 16,
  "lora_bias": false,
  "lora_dropout": 0.0,
  "megatron_config": null,
  "megatron_core": "megatron.core",
  "modules_to_save": null,
  "peft_type": "LORA",
  "peft_version": "0.18.1",
  "qalora_group_size": 16,
  "r": 16,
  "rank_pattern": {},
  "revision": null,
  "target_modules": "(?:.*?(?:language|text).*?(?:self_attn|attention|attn|mlp|feed_forward|ffn|dense).*?(?:qkv|proj|linear_fc1|linear_fc2|q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj).*?)|(?:\\bmodel\\.layers\\.[\\d]{1,}\\.(?:self_attn|attention|attn|mlp|feed_forward|ffn|dense)\\.(?:(?:qkv|proj|linear_fc1|linear_fc2|q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj)))",
  "target_parameters": null,
  "task_type": "CAUSAL_LM",
  "trainable_token_indices": null,
  "use_dora": false,
  "use_qalora": false,
  "use_rslora": false
}
```

File 8-58: `models\model_adapters\scene_detection_adapter_v1-Qwen3-VL-4B-Thinking\added_tokens.json`
```json
{
  "</think>": 151668,
  "</tool_call>": 151658,
  "</tool_response>": 151666,
  "<think>": 151667,
  "<tool_call>": 151657,
  "<tool_response>": 151665,
  "<|box_end|>": 151649,
  "<|box_start|>": 151648,
  "<|endoftext|>": 151643,
  "<|file_sep|>": 151664,
  "<|fim_middle|>": 151660,
  "<|fim_pad|>": 151662,
  "<|fim_prefix|>": 151659,
  "<|fim_suffix|>": 151661,
  "<|im_end|>": 151645,
  "<|im_start|>": 151644,
  "<|image_pad|>": 151655,
  "<|object_ref_end|>": 151647,
  "<|object_ref_start|>": 151646,
  "<|quad_end|>": 151651,
  "<|quad_start|>": 151650,
  "<|repo_name|>": 151663,
  "<|video_pad|>": 151656,
  "<|vision_end|>": 151653,
  "<|vision_pad|>": 151654,
  "<|vision_start|>": 151652
}
```

File 9-58: `models\model_adapters\scene_detection_adapter_v1-Qwen3-VL-4B-Thinking\chat_template.jinja`
```jinja
{%- set image_count = namespace(value=0) %}
{%- set video_count = namespace(value=0) %}
{%- macro render_content(content, do_vision_count) %}
    {%- if content is string %}
        {{- content }}
    {%- else %}
        {%- for item in content %}
            {%- if 'image' in item or 'image_url' in item or item.type == 'image' %}
                {%- if do_vision_count %}
                    {%- set image_count.value = image_count.value + 1 %}
                {%- endif %}
                {%- if add_vision_id %}Picture {{ image_count.value }}: {% endif -%}
                <|vision_start|><|image_pad|><|vision_end|>
            {%- elif 'video' in item or item.type == 'video' %}
                {%- if do_vision_count %}
                    {%- set video_count.value = video_count.value + 1 %}
                {%- endif %}
                {%- if add_vision_id %}Video {{ video_count.value }}: {% endif -%}
                <|vision_start|><|video_pad|><|vision_end|>
            {%- elif 'text' in item %}
                {{- item.text }}
            {%- endif %}
        {%- endfor %}
    {%- endif %}
{%- endmacro %}
{%- if tools %}
    {{- '<|im_start|>system\n' }}
    {%- if messages[0].role == 'system' %}
        {{- render_content(messages[0].content, false) + '\n\n' }}
    {%- endif %}
    {{- "# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
    {%- for tool in tools %}
        {{- "\n" }}
        {{- tool | tojson }}
    {%- endfor %}
    {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
{%- else %}
    {%- if messages[0].role == 'system' %}
        {{- '<|im_start|>system\n' + render_content(messages[0].content, false) + '<|im_end|>\n' }}
    {%- endif %}
{%- endif %}
{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}
{%- for message in messages[::-1] %}
    {%- set index = (messages|length - 1) - loop.index0 %}
    {%- if ns.multi_step_tool and message.role == "user" %}
        {%- set content = render_content(message.content, false) %}
        {%- if not(content.startswith('<tool_response>') and content.endswith('</tool_response>')) %}
            {%- set ns.multi_step_tool = false %}
            {%- set ns.last_query_index = index %}
        {%- endif %}
    {%- endif %}
{%- endfor %}
{%- for message in messages %}
    {%- set content = render_content(message.content, True) %}
    {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}
        {{- '<|im_start|>' + message.role + '\n' + content + '<|im_end|>' + '\n' }}
    {%- elif message.role == "assistant" %}
        {%- set reasoning_content = '' %}
        {%- if message.reasoning_content is string %}
            {%- set reasoning_content = message.reasoning_content %}
        {%- else %}
            {%- if '</think>' in content %}
                {%- set reasoning_content = content.split('</think>')[0].rstrip('\n').split('<think>')[-1].lstrip('\n') %}
                {%- set content = content.split('</think>')[-1].lstrip('\n') %}
            {%- endif %}
        {%- endif %}
        {%- if loop.index0 > ns.last_query_index %}
            {%- if loop.last or (not loop.last and reasoning_content) %}
                {{- '<|im_start|>' + message.role + '\n<think>\n' + reasoning_content.strip('\n') + '\n</think>\n\n' + content.lstrip('\n') }}
            {%- else %}
                {{- '<|im_start|>' + message.role + '\n' + content }}
            {%- endif %}
        {%- else %}
            {{- '<|im_start|>' + message.role + '\n' + content }}
        {%- endif %}
        {%- if message.tool_calls %}
            {%- for tool_call in message.tool_calls %}
                {%- if (loop.first and content) or (not loop.first) %}
                    {{- '\n' }}
                {%- endif %}
                {%- if tool_call.function %}
                    {%- set tool_call = tool_call.function %}
                {%- endif %}
                {{- '<tool_call>\n{"name": "' }}
                {{- tool_call.name }}
                {{- '", "arguments": ' }}
                {%- if tool_call.arguments is string %}
                    {{- tool_call.arguments }}
                {%- else %}
                    {{- tool_call.arguments | tojson }}
                {%- endif %}
                {{- '}\n</tool_call>' }}
            {%- endfor %}
        {%- endif %}
        {{- '<|im_end|>\n' }}
    {%- elif message.role == "tool" %}
        {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}
            {{- '<|im_start|>user' }}
        {%- endif %}
        {{- '\n<tool_response>\n' }}
        {{- content }}
        {{- '\n</tool_response>' }}
        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
            {{- '<|im_end|>\n' }}
        {%- endif %}
    {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
    {{- '<|im_start|>assistant\n<think>\n' }}
{%- endif %}
```

File 10-58: `models\model_adapters\scene_detection_adapter_v1-Qwen3-VL-4B-Thinking\special_tokens_map.json`
```json
{
  "additional_special_tokens": [
    "<|im_start|>",
    "<|im_end|>",
    "<|object_ref_start|>",
    "<|object_ref_end|>",
    "<|box_start|>",
    "<|box_end|>",
    "<|quad_start|>",
    "<|quad_end|>",
    "<|vision_start|>",
    "<|vision_end|>",
    "<|vision_pad|>",
    "<|image_pad|>",
    "<|video_pad|>"
  ],
  "eos_token": {
    "content": "<|im_end|>",
    "lstrip": false,
    "normalized": false,
    "rstrip": false,
    "single_word": false
  },
  "pad_token": {
    "content": "<|endoftext|>",
    "lstrip": false,
    "normalized": false,
    "rstrip": false,
    "single_word": false
  }
}
```

File 11-58: `models\model_adapters\scene_detection_adapter_v1-Qwen3-VL-4B-Thinking\tokenizer_config.json`
```json
{
  "add_bos_token": false,
  "add_prefix_space": false,
  "added_tokens_decoder": {
    "151643": {
      "content": "<|endoftext|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151644": {
      "content": "<|im_start|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151645": {
      "content": "<|im_end|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151646": {
      "content": "<|object_ref_start|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151647": {
      "content": "<|object_ref_end|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151648": {
      "content": "<|box_start|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151649": {
      "content": "<|box_end|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151650": {
      "content": "<|quad_start|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151651": {
      "content": "<|quad_end|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151652": {
      "content": "<|vision_start|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151653": {
      "content": "<|vision_end|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151654": {
      "content": "<|vision_pad|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151655": {
      "content": "<|image_pad|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151656": {
      "content": "<|video_pad|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "151657": {
      "content": "<tool_call>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": false
    },
    "151658": {
      "content": "</tool_call>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": false
    },
    "151659": {
      "content": "<|fim_prefix|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": false
    },
    "151660": {
      "content": "<|fim_middle|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": false
    },
    "151661": {
      "content": "<|fim_suffix|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": false
    },
    "151662": {
      "content": "<|fim_pad|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": false
    },
    "151663": {
      "content": "<|repo_name|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": false
    },
    "151664": {
      "content": "<|file_sep|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": false
    },
    "151665": {
      "content": "<tool_response>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": false
    },
    "151666": {
      "content": "</tool_response>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": false
    },
    "151667": {
      "content": "<think>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": false
    },
    "151668": {
      "content": "</think>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": false
    }
  },
  "additional_special_tokens": [
    "<|im_start|>",
    "<|im_end|>",
    "<|object_ref_start|>",
    "<|object_ref_end|>",
    "<|box_start|>",
    "<|box_end|>",
    "<|quad_start|>",
    "<|quad_end|>",
    "<|vision_start|>",
    "<|vision_end|>",
    "<|vision_pad|>",
    "<|image_pad|>",
    "<|video_pad|>"
  ],
  "bos_token": null,
  "clean_up_tokenization_spaces": false,
  "eos_token": "<|im_end|>",
  "errors": "replace",
  "extra_special_tokens": {},
  "model_max_length": 262144,
  "pad_token": "<|endoftext|>",
  "padding_side": "right",
  "processor_class": "Qwen3VLProcessor",
  "split_special_tokens": false,
  "tokenizer_class": "Qwen2Tokenizer",
  "unk_token": null
}
```

File 12-58: `run.py`
```py
from src import create_app

app = create_app()

if __name__ == '__main__':
    # Standard Flask runner
    # The Playground is designed to be a lightweight tool, so debug mode is on by default.
    app.run(debug=True, port=5000)
```

File 13-58: `src\__init__.py`
```py
from flask import Flask
from src.config import Config

def create_app():
    """
    Application Factory for the standalone Scene Detection Playground.
    Initializes Flask environment and registers the modularized Playground Blueprint.
    """
    app = Flask(__name__, 
                template_folder='../templates',
                static_folder='../static')
    
    # Load settings from the decoupled config module
    app.config.from_object(Config)

    # Register the Playground Blueprint Package
    # Now importing from the modular package instead of the single file
    from src.http.controllers.playground import playground_bp
    app.register_blueprint(playground_bp)

    # Root Route Management
    @app.route('/')
    def index_redirect():
        from flask import redirect, url_for
        return redirect(url_for('playground.index'))

    return app
```

File 14-58: `src\config.py`
```py
import os
import yaml
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables from .env
load_dotenv()

class Config:
    """
    Centralized configuration for the standalone Scene Detection Playground.
    This version has been surgically decoupled from the main SceneMark-AI suite,
    removing all logic related to cinematic principle documentation, project 
    persistence, and dataset manufacturing.
    """
    
    # Base Directories
    BASE_DIR = Path(__file__).resolve().parent.parent
    CONFIG_DIR = os.path.join(BASE_DIR, 'config')
    PROMPTS_DIR = os.path.join(CONFIG_DIR, 'prompts')
    SETTINGS_DIR = os.path.join(CONFIG_DIR, 'settings')
    
    # Data & Persistence
    # The Playground maintains its own sandboxed data folder.
    PLAYGROUND_FOLDER = os.path.join(BASE_DIR, 'data', 'playground')
    
    # Credentials
    SECRET_KEY = os.environ.get('SECRET_KEY', 'playground-dev-key')
    GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')

    # Performance Tuning
    # Target: High concurrency for Cloud VLM calls during sliding-window analysis.
    MAX_WORKERS_IO = 60

    @staticmethod
    def load_config_yaml(filename, subfolder='settings'):
        """Utility to load configuration files from the settings or prompts directories."""
        directory = Config.SETTINGS_DIR if subfolder == 'settings' else Config.PROMPTS_DIR
        path = os.path.join(directory, filename)
        if not os.path.exists(path):
            return {}
        with open(path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

# --- LOADER UTILITIES ---

def load_inference_prompt_config():
    """Loads the specialized VLM inference configuration for the Playground."""
    config_path = os.path.join(Config.PROMPTS_DIR, 'inference_prompt.yaml')
    if not os.path.exists(config_path): 
        # Fallback empty structure
        return {'inference': {}, 'prompt': {'main': '', 'system': ''}}
    with open(config_path, 'r', encoding='utf-8') as f: 
        return yaml.safe_load(f)

# --- DATA INITIALIZATION ---

# Load Core YAML Settings
app_params = Config.load_config_yaml('app_params.yaml')
local_model_params = Config.load_config_yaml('local_model.yaml')
PRICING_DATA = Config.load_config_yaml('model_pricing.yaml')

# Derive available models for UI selection from the pricing manifest
AVAILABLE_MODELS = [k for k in PRICING_DATA.keys() if k != 'default'][::-1]

# Load Forensic Inference Logic
INFERENCE_CONFIG_DATA = load_inference_prompt_config()

# --- ATTRIBUTE INJECTION ---
# We inject loaded values directly into the Config class for global static access.

# Video Processing Constants
Config.FRAMES_PER_SHOT = app_params.get('video_processing', {}).get('frames_per_shot', 1)
Config.SCENE_DETECTION_THRESHOLD = app_params.get('video_processing', {}).get('scene_detection_threshold', 27.0)
Config.IMAGE_WIDTH = app_params.get('video_processing', {}).get('image_width', 448)
Config.IMAGE_QUALITY = app_params.get('video_processing', {}).get('image_quality', 80)

# Window Logic Defaults
Config.SHOTS_PER_CHUNK = app_params.get('chunking', {}).get('shots_per_chunk', 50)
Config.CHUNK_STRIDE = app_params.get('chunking', {}).get('default_stride', 40)

# AI Runtime Parameters
Config.GEMINI_TEMP = app_params.get('ai_params', {}).get('temperature', 0.1)
Config.GEMINI_MAX_TOKENS = app_params.get('ai_params', {}).get('max_output_tokens', 8192)
Config.INCLUDE_THOUGHTS = app_params.get('ai_params', {}).get('include_thoughts', True)
Config.LOCAL_MODEL_ID = local_model_params.get('model', {}).get('id', "huihui-ai/Huihui-Qwen3-VL-2B-Thinking-abliterated")

# Centralized Inference Prompts
Config.INFERENCE_CONFIG = INFERENCE_CONFIG_DATA
Config.INFERENCE_PROMPT = INFERENCE_CONFIG_DATA.get('prompt', {}).get('main', '')
Config.INFERENCE_SYSTEM_PROMPT = INFERENCE_CONFIG_DATA.get('prompt', {}).get('system', '')

# Inference Sampling Settings
inference_meta = INFERENCE_CONFIG_DATA.get('inference', {})
Config.INFERENCE_TEMPERATURE = inference_meta.get('temperature', 0.1)
Config.INFERENCE_TOP_P = inference_meta.get('top_p', 0.9)
Config.INFERENCE_MAX_TOKENS = inference_meta.get('max_tokens', 8192)
Config.INFERENCE_REPETITION_PENALTY = inference_meta.get('repetition_penalty', 1.2)
```

File 15-58: `src\core\engines\cv_engine.py`
```py
import os
import cv2
import time
import numpy as np
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector
from src.config import Config

class CVEngine:
    """
    Handles low-level computer vision tasks for the evaluation pipeline.
    Responsible for technical shot boundary detection and representative 
    frame (visual anchor) extraction.
    """

    @staticmethod
    def analyze_shot_boundaries(video_path, logger=None):
        """
        Uses PySceneDetect to identify all technical cuts/shots in the video source.
        Returns a list of shot metadata dictionaries.
        """
        start_detect = time.time()
        video_manager = VideoManager([video_path])
        scene_manager = SceneManager()
        threshold = Config.SCENE_DETECTION_THRESHOLD
        
        # Use content-based detection to find hard cuts and significant transitions
        scene_manager.add_detector(ContentDetector(threshold=threshold))
        video_manager.start()
        
        fps = video_manager.frame_rate
        try:
            duration = video_manager.get_duration()
            total_frames = duration[1].get_frames()
            if total_frames == 0:
                raise ValueError("Scenedetect returned 0 frames")
        except:
            # Fallback to direct OpenCV count if scenedetect duration fetch fails
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.release()

        if logger:
            logger.log_scene_detect_start(video_path, fps, total_frames)
            logger.log_scene_detect_threshold(threshold)

        # Execute the frame-by-frame analysis pass
        scene_manager.detect_scenes(frame_source=video_manager)
        scene_list = scene_manager.get_scene_list()
        shots = []

        for i, scene in enumerate(scene_list):
            start_time, end_time = scene
            shots.append({
                "shot_id": f"shot_{i:05d}",
                "start_time": start_time.get_seconds(),
                "end_time": end_time.get_seconds(),
                "start_frame": start_time.get_frames(),
                "end_frame": end_time.get_frames(),
                "is_scene_break": False 
            })

            if logger and i % 10 == 0:
                logger.log_scene_detect_boundary(
                    start_time.get_frames(), 
                    score=threshold, 
                    fps=fps, 
                    is_cut=True
                )
            
        video_manager.release()
        
        if logger:
            elapsed = (time.time() - start_detect) * 1000
            logger.log_scene_detect_complete(len(shots), elapsed)

        return shots, duration[0].get_seconds() if duration else total_frames / fps


    @staticmethod
    def generate_visual_anchors(video_path, shots, session_folder, logger=None):
        """
        Extracts midpoint frames for every detected shot.
        Resizes images for VLM context windows and burns in high-contrast ID labels.
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise IOError(f"Native OpenCV backend failed to open source: {video_path}")

        # Store frames in a dedicated sub-folder for the session
        frames_dir = os.path.join(session_folder, 'frames')
        os.makedirs(frames_dir, exist_ok=True)

        if logger:
            logger.push_context('visual_anchor_extraction', shot_count=len(shots))

        for idx, shot in enumerate(shots):
            shot_id = shot["shot_id"]
            shot_images = []
            
            # Extract the absolute midpoint frame of the shot
            mid_idx = (shot["start_frame"] + shot["end_frame"]) // 2

            cap.set(cv2.CAP_PROP_POS_FRAMES, mid_idx)
            ret, frame = cap.read()
            
            if ret:
                # 1. Geometry Normalization
                height, width = frame.shape[:2]
                scale = Config.IMAGE_WIDTH / float(width)
                target_h = int(height * scale)
                frame = cv2.resize(frame, (Config.IMAGE_WIDTH, target_h), interpolation=cv2.INTER_AREA)

                # 2. Visual Grounding: Burn Shot ID into the pixels
                # This ensures the VLM can explicitly reference the ID seen in the image.
                text = f"SHOT ID: {shot_id}"
                font = cv2.FONT_HERSHEY_SIMPLEX
                (text_w, text_h), baseline = cv2.getTextSize(text, font, 0.5, 1)
                x, y = 8, 25 
                
                # Draw white backing box for readability
                cv2.rectangle(frame, (x - 4, y - text_h - 6), (x + text_w + 4, y + 4), (255, 255, 255), cv2.FILLED)
                # Draw black text
                cv2.putText(frame, text, (x, y), font, 0.5, (0, 0, 0), 1, cv2.LINE_AA)

                # 3. Persistence
                img_filename = f"{shot_id}_{mid_idx}.jpg"
                img_path = os.path.join(frames_dir, img_filename)
                cv2.imwrite(img_path, frame, [cv2.IMWRITE_JPEG_QUALITY, Config.IMAGE_QUALITY])
                
                # Relative path used for frontend loading
                shot_images.append(os.path.join('frames', img_filename))

            shot["image_paths"] = shot_images

            if logger and (idx + 1) % 20 == 0:
                logger.log('SCENE_DETECT', f"Extracted visual anchors for {idx + 1}/{len(shots)} shots")

        cap.release()
        if logger: logger.pop_context('COMPLETE')
        return shots
```

File 16-58: `src\core\engines\gemini_client.py`
```py
import os
import json
import datetime
import time
import random
from google import genai
from google.genai import types
from src.config import Config

class GeminiClient:
    """
    Robust wrapper for the Google Gemini API (Vertex/AI Studio).
    Refactored to support the 'Rehydration Pattern' where model_interaction.json
    is the Single Source of Truth (SSOT).
    Schema V3: Pruned metadata, Interaction IDs.
    """

    def __init__(self):
        if not Config.GEMINI_API_KEY:
            raise ValueError("[Gemini Client] GEMINI_API_KEY not found in environment.")
        self.client = genai.Client(api_key=Config.GEMINI_API_KEY)

    def _get_safety_settings(self):
        return [
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH, 
                threshold=types.HarmBlockThreshold.BLOCK_NONE
            ),
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, 
                threshold=types.HarmBlockThreshold.BLOCK_NONE
            ),
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, 
                threshold=types.HarmBlockThreshold.BLOCK_NONE
            ),
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_HARASSMENT, 
                threshold=types.HarmBlockThreshold.BLOCK_NONE
            ),
        ]

    def _log_interaction(self, session_folder, input_count, response_obj, interaction_id, prompt_text, usage_info=None, error=None, batch_context=None):
        """
        Appends a new interaction entry to the 'interactions' list in model_interaction.json.
        """
        if not session_folder or not os.path.exists(session_folder): 
            return 
        
        log_path = os.path.join(session_folder, 'model_interaction.json')
        
        resp_text = None
        if response_obj:
            try: 
                resp_text = response_obj.text
            except: 
                resp_text = "[No Text Content - Potentially Blocked]"

        usage_data = usage_info
        if not usage_data and response_obj and response_obj.usage_metadata:
            usage_data = {
                "input_tokens": response_obj.usage_metadata.prompt_token_count,
                "output_tokens": response_obj.usage_metadata.candidates_token_count,
                "images_sent": input_count
            }

        log_entry = {
            "interaction_id": interaction_id or f"int_{int(time.time()*1000)}",
            "batch_context": batch_context or {},
            "prompt_text": prompt_text,
            "response_text": resp_text,
            "usage": usage_data,
            "error": str(error) if error else None
        }

        try:
            data = {}
            if os.path.exists(log_path):
                with open(log_path, 'r', encoding='utf-8') as f:
                    try:
                        data = json.load(f)
                    except json.JSONDecodeError:
                        data = {}
            
            if "interactions" not in data:
                data["interactions"] = []
                
            data["interactions"].append(log_entry)

            with open(log_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=4, ensure_ascii=False)
        except Exception as e: 
            print(f"[GeminiClient] Log Write Error: {e}")

    def generate_with_backoff(self, model, contents, config, session_folder=None, image_count=0, label="Analysis Batch", prompt_text="", adapter_name=None, inference_params=None, batch_context=None, interaction_id=None):
        """
        Executes a multimodal generation request with robust retry logic.
        """
        if not config.safety_settings:
            config.safety_settings = self._get_safety_settings()

        max_retries = 10 
        base_delay = 2.0 
        last_exception = None

        for attempt in range(max_retries):
            try:
                response = self.client.models.generate_content(
                    model=model,
                    contents=contents,
                    config=config
                )
                
                usage = {
                    "input_tokens": response.usage_metadata.prompt_token_count or 0,
                    "output_tokens": response.usage_metadata.candidates_token_count or 0
                }

                self._log_interaction(session_folder, image_count, response, interaction_id, prompt_text, usage_info=usage, batch_context=batch_context)
                
                if not response.text:
                    raise ValueError("Empty VLM response received.")
                    
                return response, usage

            except Exception as e:
                last_exception = e
                error_str = str(e).lower()
                
                is_rate_limit = (
                    "429" in error_str or 
                    "resource_exhausted" in error_str or 
                    "quota" in error_str or 
                    "too many requests" in error_str
                )
                
                if not is_rate_limit:
                    self._log_interaction(session_folder, image_count, None, interaction_id, prompt_text, error=e, batch_context=batch_context)
                    raise e
                
                if attempt == max_retries - 1:
                    self._log_interaction(session_folder, image_count, None, interaction_id, prompt_text, error=e, batch_context=batch_context)
                    raise e
                
                wait_time = min(base_delay * (2 ** attempt), 60.0) 
                jitter = random.uniform(0.5, 2.5)
                total_wait = wait_time + jitter
                
                print(f"[{label}] API Quota Exhausted. Retry {attempt+1}/{max_retries} in {total_wait:.2f}s...")
                time.sleep(total_wait)
        
        raise last_exception
```

File 17-58: `src\core\engines\qwen\__init__.py`
```py
"""
Qwen Engine Package
-------------------
This package modularizes the Qwen VLM client by separating concerns:
- diagnostics: Hardware capability checks (VRAM).
- loader: Model and adapter loading logic with dynamic precision.
- generator: Inference execution and output parsing.
- streamer: Real-time token streaming hooks for logging.
- client: The main public-facing orchestrator.
"""

from .diagnostics import check_gpu_capacity
from .loader import ModelLoader
from .generator import InferenceGenerator
from .streamer import ForensicStreamer

__all__ = [
    "check_gpu_capacity",
    "ModelLoader",
    "InferenceGenerator",
    "ForensicStreamer"
]
```

File 18-58: `src\core\engines\qwen\diagnostics.py`
```py
import torch
import logging
from typing import Dict, Union

# Configure logging for the diagnostics module
logger = logging.getLogger(__name__)

def check_gpu_capacity(required_vram_gb: float = 12.0) -> Dict[str, Union[bool, float, str]]:
    """
    Analyzes the host hardware to determine if it supports High-Fidelity (BF16) inference.
    
    This check is primarily based on TOTAL VRAM capacity to classify the hardware tier.
    It serves as a gatekeeper for enabling the 'High-Fidelity Mode' UI option.
    
    Args:
        required_vram_gb (float): The minimum total VRAM (in GB) required to be considered "capable"
                                  of running the model in native BF16 precision without quantization.
                                  Defaults to 12.0 GB (Safety margin for 2B model + Context + OS overhead).

    Returns:
        dict: {
            'is_capable': bool,      # True if hardware meets the total VRAM threshold
            'total_vram_gb': float,  # Total installed video memory
            'free_vram_gb': float,   # Currently available video memory
            'device_name': str,      # Name of the GPU
            'reason': str            # Human-readable status string
        }
    """
    result = {
        'is_capable': False,
        'total_vram_gb': 0.0,
        'free_vram_gb': 0.0,
        'device_name': "CPU / Unknown",
        'reason': "CUDA not available."
    }

    if not torch.cuda.is_available():
        logger.warning("[Diagnostics] CUDA not detected. Defaulting to CPU/Low-Spec mode.")
        return result

    try:
        # We assume Device 0 for the primary inference GPU in a local sandbox environment
        device_id = 0
        properties = torch.cuda.get_device_properties(device_id)
        result['device_name'] = properties.name
        
        # Total VRAM (Physical Capacity)
        total_bytes = properties.total_memory
        result['total_vram_gb'] = round(total_bytes / (1024 ** 3), 2)

        # Free VRAM (Runtime Availability)
        # mem_get_info returns (free, total) in bytes
        free_bytes, _ = torch.cuda.mem_get_info(device_id)
        result['free_vram_gb'] = round(free_bytes / (1024 ** 3), 2)

        # Capability Decision Logic
        # We check TOTAL capacity to determine if the card is technically capable (Hardware Tier).
        # We verify if it meets the safe threshold for BF16 loading (model weights + KV cache).
        if result['total_vram_gb'] >= required_vram_gb:
            result['is_capable'] = True
            result['reason'] = f"Hardware Capable ({result['total_vram_gb']}GB Total VRAM detected)."
        else:
            result['reason'] = (
                f"Insufficient VRAM for High-Fidelity Mode. "
                f"Detected: {result['total_vram_gb']}GB. Required: {required_vram_gb}GB."
            )

        logger.info(f"[Diagnostics] GPU Check: {result['device_name']} | "
                    f"Total: {result['total_vram_gb']}GB | "
                    f"Free: {result['free_vram_gb']}GB | "
                    f"BF16 Capable: {result['is_capable']}")

    except Exception as e:
        logger.error(f"[Diagnostics] Failed to query GPU properties: {e}")
        result['reason'] = f"Driver Error: {str(e)}"

    return result
```

File 19-58: `src\core\engines\qwen\generator.py`
```py
import torch
import time
import re
import json
import logging
from typing import List, Dict, Optional, Tuple, Union
from PIL import Image
from src.config import Config
from src.core.engines.qwen.streamer import ForensicStreamer

# Configure logging
logger = logging.getLogger(__name__)

class InferenceGenerator:
    """
    Handles the execution of the inference loop.
    Separated from loading logic to focus purely on data processing,
    token generation, and output parsing.
    """

    def __init__(self, model, processor):
        self.model = model
        self.processor = processor

    def generate(self, 
                 images: List[Image.Image], 
                 prompt_text: str, 
                 system_prompt: str,
                 inference_params: Dict, 
                 granular_logger=None) -> Dict:
        """
        Executes the forward pass of the model.

        Args:
            images (List[Image.Image]): Processed PIL images.
            prompt_text (str): User/Task prompt.
            system_prompt (str): System instruction.
            inference_params (Dict): Generation hyperparameters.
            granular_logger: Optional structured logger for forensic traces.

        Returns:
            Dict: { 'text': str, 'thinking': str, 'usage': dict }
        """
        start_time = time.time()
        
        # 1. Message Templating
        messages = [
            {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
            {"role": "user", "content": [
                *[{"type": "image", "image": img} for img in images], 
                {"type": "text", "text": prompt_text}
            ]}
        ]

        # 2. Dynamic LoRA Scaling (if applicable)
        lora_scale = float(inference_params.get('lora_scale', 1.0))
        self._set_adapter_scale(lora_scale)

        # 3. Preprocessing (Tokenization)
        preproc_start = time.time()
        text_input = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        inputs = self.processor(
            text=[text_input], 
            images=images, 
            videos=None, 
            padding=True, 
            return_tensors="pt"
        ).to(self.model.device)
        
        input_tokens_count = inputs.input_ids.shape[1]
        preproc_elapsed = (time.time() - preproc_start) * 1000
        
        if granular_logger:
            img_size = images[0].size if images else (0,0)
            granular_logger.log_vlm_preprocessing(len(images), img_size, input_tokens_count, preproc_elapsed)

        # 4. Streamer Setup
        # Use runtime interval parameter, default to 30
        stream_interval = int(inference_params.get('stream_interval', 30))
        streamer = None
        if granular_logger:
            streamer = ForensicStreamer(self.processor, granular_logger, update_interval=stream_interval)

        # 5. Generation Loop
        gen_start = time.time()
        
        p_temp = inference_params.get('temperature', 0.1)
        p_top_p = inference_params.get('top_p', 0.9)
        p_max_tokens = int(inference_params.get('max_tokens', 8192))
        p_rep_penalty = float(inference_params.get('repetition_penalty', Config.INFERENCE_REPETITION_PENALTY))
        
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs, 
                max_new_tokens=p_max_tokens, 
                temperature=p_temp,
                top_p=p_top_p, 
                repetition_penalty=p_rep_penalty, 
                do_sample=True,
                streamer=streamer # Attach the heartbeat hook
            )

        # 6. Cleanup & Reset
        if lora_scale != 1.0:
            self._set_adapter_scale(1.0) # Reset to default

        # 7. Post-Processing (Decoding)
        # Trim input tokens from output to get only the new content
        generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
        output_tokens_count = generated_ids_trimmed[0].shape[0]
        
        gen_elapsed = (time.time() - gen_start) * 1000
        tps = output_tokens_count / (gen_elapsed / 1000) if gen_elapsed > 0 else 0
        
        if granular_logger:
            granular_logger.log_vlm_token_generation(output_tokens_count, p_max_tokens, gen_elapsed, tps)
        
        raw_output = self.processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )[0]

        # 8. Parsing
        clean_text, thinking = self._parse_thinking_output(raw_output)
        elapsedTotal = time.time() - start_time

        # 9. JSON Validation Log (Optional)
        if granular_logger:
            self._validate_json_log(clean_text, granular_logger)

        return {
            "text": clean_text, 
            "thinking": thinking, 
            "usage": {
                "input_tokens": input_tokens_count, 
                "output_tokens": output_tokens_count, 
                "inference_time": round(elapsedTotal, 2)
            }
        }

    def _set_adapter_scale(self, scale_factor: float):
        """Helper to dynamically adjust LoRA alpha/rank scaling at runtime."""
        # Only works if PeftModel
        if not hasattr(self.model, "active_adapter") or not self.model.active_adapter:
            return
            
        try:
            active_adapter = self.model.active_adapter
            if isinstance(active_adapter, str): # Handle single adapter case
                for module in self.model.modules():
                    if hasattr(module, "scaling") and isinstance(module.scaling, dict):
                        if active_adapter in module.scaling:
                            # Standard LoRA scaling logic: alpha / r
                            peft_config = self.model.peft_config.get(active_adapter)
                            if peft_config:
                                default_scale = peft_config.lora_alpha / peft_config.r
                                new_scale = default_scale * scale_factor
                                module.scaling[active_adapter] = new_scale
        except Exception as e:
            logger.warning(f"Failed to set LoRA scale: {e}")

    def _parse_thinking_output(self, raw_text: str) -> Tuple[str, Optional[str]]:
        """Splits raw VLM output into reasoning (<think>) and content."""
        if not raw_text: 
            return "", None

        thinking = None
        clean_text = raw_text

        # Pattern 1: Explicit XML Tags
        think_match = re.search(r'<think>(.*?)</think>', raw_text, flags=re.DOTALL | re.IGNORECASE)
        if think_match:
            thinking = think_match.group(1).strip()
            clean_text = re.sub(r'<think>.*?</think>', '', raw_text, flags=re.DOTALL | re.IGNORECASE).strip()
            return clean_text, thinking

        # Pattern 2: Fallback for models that emit raw reasoning before JSON
        # Assumes JSON starts with '{' and anything before it is reasoning
        json_start = raw_text.find('{')
        if json_start > 20: 
            thinking = raw_text[:json_start].strip()
            clean_text = raw_text[json_start:].strip()
            # Clean up common lead-in prefixes
            thinking = re.sub(r'^(thinking|analysis|reasoning|audit):\s*', '', thinking, flags=re.IGNORECASE)
            return clean_text, thinking

        return clean_text, thinking

    def _validate_json_log(self, text, logger_inst):
        """Helper to log whether valid JSON was emitted."""
        try:
            json_match = re.search(r'\{.*\}', text, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group())
                logger_inst.log_vlm_json_parse("SUCCESS", list(parsed.keys()))
            else:
                logger_inst.log_vlm_json_parse("MISSING", [], "VLM did not emit a JSON block.")
        except Exception as e:
            logger_inst.log_vlm_json_parse("FAILED", [], str(e))
```

File 20-58: `src\core\engines\qwen\loader.py`
```py
import os
import torch
import gc
import logging
from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig
from peft import PeftModel

logger = logging.getLogger(__name__)

class ModelLoader:
    """
    Handles the instantiation of the Qwen VLM and its adapters.
    Implements the Dynamic Precision logic:
    - Path A: 4-Bit NormalFloat (NF4) for VRAM-constrained systems.
    - Path B: Native BFloat16 (High-Fidelity) for capable hardware.
    """

    @staticmethod
    def _clear_vram():
        """Force garbage collection and clear CUDA cache."""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def load_base_model(self, model_id: str, high_fidelity_mode: bool = False, logger_func=None):
        """
        Loads the foundational model weights.

        Args:
            model_id (str): Hugging Face model identifier.
            high_fidelity_mode (bool): If True, loads in native BF16. Else, uses 4-bit quantization.
            logger_func (callable, optional): Callback for status updates.

        Returns:
            tuple: (model, processor)
        """
        self._clear_vram()
        cache_dir = os.path.join(os.getcwd(), 'models')
        os.makedirs(cache_dir, exist_ok=True)

        if logger_func: 
            mode_str = "HIGH-FIDELITY (BF16)" if high_fidelity_mode else "MEMORY-SAFE (4-BIT NF4)"
            logger_func(f"Initializing Base Model: {model_id} [{mode_str}]")

        try:
            # 1. Configure Quantization / Precision
            quantization_config = None
            torch_dtype = torch.bfloat16 

            if not high_fidelity_mode:
                # PATH A: Low VRAM (Default)
                # Uses BitsAndBytes for 4-bit NormalFloat quantization
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16,
                    bnb_4bit_use_double_quant=True
                )
                if logger_func: logger_func("Applying 4-bit NormalFloat quantization...")
            else:
                # PATH B: High Performance
                # Native loading. No quantization config needed, just dtype=bfloat16.
                if logger_func: logger_func("Applying native BFloat16 precision (No Quantization)...")

            # 2. Load Processor
            processor = AutoProcessor.from_pretrained(
                model_id, 
                trust_remote_code=True, 
                cache_dir=cache_dir
            )

            # 3. Load Model
            # Note: device_map="auto" handles layer placement on GPU
            model = AutoModelForImageTextToText.from_pretrained(
                model_id,
                device_map="auto",
                trust_remote_code=True,
                quantization_config=quantization_config,
                torch_dtype=torch_dtype,
                cache_dir=cache_dir,
                low_cpu_mem_usage=True
            )
            
            model.eval()
            return model, processor

        except Exception as e:
            logger.error(f"Failed to load base model {model_id}: {str(e)}")
            raise RuntimeError(f"Base Model Initialization Failure: {str(e)}")

    def mount_adapter(self, base_model, adapter_path: str, adapter_name: str, is_already_peft: bool = False, logger_func=None):
        """
        Mounts or hot-swaps a LoRA adapter onto the base model.
        
        Args:
            base_model: The loaded base model (transformers or PEFT model).
            adapter_path (str): Path to the adapter weights.
            adapter_name (str): Unique name for the adapter module.
            is_already_peft (bool): Whether the base_model is already wrapped by PeftModel.
            logger_func (callable, optional): Callback for status updates.

        Returns:
            The model with adapter active.
        """
        if logger_func: logger_func(f"Mounting LoRA adapter: {adapter_name}")

        try:
            if not is_already_peft:
                # First time mounting an adapter: Wrap the base model
                model = PeftModel.from_pretrained(base_model, adapter_path, adapter_name=adapter_name)
                return model
            else:
                # Hot-swap on existing PEFT model
                if logger_func: logger_func("Hot-swapping active LoRA weights in VRAM...")
                
                # Check if this specific adapter name is already loaded to avoid duplicates
                if adapter_name not in base_model.peft_config:
                    base_model.load_adapter(adapter_path, adapter_name=adapter_name)
                
                base_model.set_adapter(adapter_name)
                return base_model

        except Exception as e:
            logger.error(f"Failed to mount adapter {adapter_path}: {e}")
            raise RuntimeError(f"Adapter Mounting Failure: {str(e)}")

    def unload_adapter(self, model, status_callback=None):
        """Unloads all adapters and returns the base model."""
        if hasattr(model, "unload"):
            if status_callback: status_callback("Unloading adapter. Returning to Base Model state...")
            return model.unload()
        return model
```

File 21-58: `src\core\engines\qwen\streamer.py`
```py
import logging
from transformers import TextStreamer

# Configure logging
logger = logging.getLogger(__name__)

class ForensicStreamer(TextStreamer):
    """
    Custom token streamer for Hugging Face generation.
    Intercepts the token stream to provide real-time 'heartbeat' logging
    to the GranularLogger without blocking the generation process.
    """

    def __init__(self, tokenizer, granular_logger=None, update_interval: int = 40):
        """
        Args:
            tokenizer: The model's tokenizer/processor.
            granular_logger: Instance of GranularLogger for UI updates.
            update_interval (int): Log progress every N tokens.
        """
        super().__init__(tokenizer, skip_prompt=True, skip_special_tokens=True)
        self.granular_logger = granular_logger
        self.update_interval = update_interval
        self.token_count = 0
        self.last_log_count = 0
        self.is_first_chunk = True

    def on_finalized_text(self, text: str, stream_end: bool = False):
        """
        Called by the parent TextStreamer when a chunk of text is decoded.
        We override this to hook into the stream loop.
        """
        # TextStreamer prints to stdout by default. We suppress that by doing nothing here
        # regarding stdout, but we use the event to increment our counter.
        pass

    def put(self, value):
        """
        Receives input ids from the generator.
        """
        # Call parent to handle decoding state
        super().put(value)
        
        # Safety Check: Handle variable tensor dimensions
        # value can be 1D (num_tokens,) or 2D (batch, num_tokens)
        if len(value.shape) > 1:
            new_tokens = value.shape[0] * value.shape[1]
        else:
            new_tokens = value.shape[0]

        # Heuristic: Ignore the initial prompt chunk if it's large.
        # The first call to put() often contains the entire input prompt context.
        # We only want to log *newly generated* tokens.
        if self.is_first_chunk:
            self.is_first_chunk = False
            if new_tokens > 128: 
                return

        self.token_count += new_tokens

        if self.granular_logger:
            if (self.token_count - self.last_log_count) >= self.update_interval:
                self.granular_logger.log(
                    'TOKEN', 
                    f"Thinking... ({self.token_count} tokens generated)"
                )
                self.last_log_count = self.token_count

    def end(self):
        super().end()
        # Optional: Final log summary could go here, but the generator usually handles that.
```

File 22-58: `src\core\engines\qwen_client.py`
```py
import os
import logging
import json
import datetime
import gc
import torch
from typing import List, Dict, Optional, Callable, Union
from PIL import Image

from src.config import Config
from src.core.engines.qwen.loader import ModelLoader
from src.core.engines.qwen.generator import InferenceGenerator

# Configure logging
logger = logging.getLogger(__name__)

class QwenClient:
    """
    Singleton Orchestrator for the local Qwen-VL model.
    Refactored to support the 'Rehydration Pattern' where model_interaction.json
    is the Single Source of Truth (SSOT).
    """
    _instance = None
    
    # State Containers
    _model = None
    _processor = None
    _loader = None
    _generator = None
    
    # Metadata
    _current_base_id = None
    _current_adapter_name = None
    _is_peft_wrapped = False

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(QwenClient, cls).__new__(cls)
            cls._instance._loader = ModelLoader()
        return cls._instance

    def _validate_adapter_integrity(self, adapter_path: str, base_model_id: str, bypass_validation: bool = False) -> bool:
        """Pre-flight integrity and compatibility check."""
        if not os.path.exists(adapter_path):
            raise ValueError(f"Adapter directory not found at: {adapter_path}")

        config_path = os.path.join(adapter_path, 'adapter_config.json')
        
        if not os.path.exists(config_path):
            raise ValueError(f"Missing 'adapter_config.json' in {adapter_path}.")
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                adapter_config = json.load(f)
        except Exception as e:
            raise ValueError(f"Failed to parse 'adapter_config.json': {str(e)}")

        target_base = adapter_config.get('base_model_name_or_path')
        if not target_base:
            raise ValueError("Invalid adapter config: Missing 'base_model_name_or_path'.")

        target_basename = os.path.basename(os.path.normpath(target_base))
        current_basename = os.path.basename(os.path.normpath(base_model_id))
        is_match = (target_base == base_model_id) or (target_basename == current_basename)

        if not is_match:
            msg = (f"INCOMPATIBILITY DETECTED: Tuned on '{target_base}', loading on '{base_model_id}'.")
            if bypass_validation:
                logger.warning(f"[FORCE MODE] Bypassing compatibility check. {msg}")
                return True 
            else:
                raise ValueError(f"{msg} Operation aborted.")

        return True

    def load_model(self, 
                   target_model_id: str = None, 
                   adapter_path: str = None, 
                   status_callback: Optional[Callable[[str], None]] = None, 
                   logger_instance=None, 
                   bypass_validation: bool = False,
                   high_fidelity_mode: bool = False):
        """Orchestrates the loading of the VLM context."""
        model_id = target_model_id if target_model_id else Config.LOCAL_MODEL_ID
        
        if adapter_path:
            self._validate_adapter_integrity(adapter_path, model_id, bypass_validation)

        needs_base_reload = (self._model is None) or (self._current_base_id != model_id)

        if needs_base_reload:
            if self._model is not None:
                if status_callback: status_callback(f"Offloading previous base model '{self._current_base_id}'...")
                del self._model
                del self._processor
                self._model = None
                self._processor = None
                self._is_peft_wrapped = False
                self._loader._clear_vram()

            self._model, self._processor = self._loader.load_base_model(
                model_id, 
                high_fidelity_mode=high_fidelity_mode,
                logger_func=status_callback
            )
            
            self._generator = InferenceGenerator(self._model, self._processor)
            self._current_base_id = model_id
            self._current_adapter_name = None
            self._is_peft_wrapped = False

            if logger_instance:
                mode_desc = "Native BF16" if high_fidelity_mode else "4-Bit NF4"
                logger_instance.log_vlm_load_mode(mode_desc, f"{model_id} on {self._model.device}")

        new_adapter_name = os.path.basename(os.path.normpath(adapter_path)) if adapter_path else None

        if self._current_adapter_name != new_adapter_name:
            if adapter_path:
                self._model = self._loader.mount_adapter(
                    self._model, 
                    adapter_path, 
                    new_adapter_name, 
                    is_already_peft=self._is_peft_wrapped,
                    logger_func=status_callback
                )
                self._current_adapter_name = new_adapter_name
                self._is_peft_wrapped = True
                if status_callback: status_callback("✓ Adapter loaded and active.")

            elif self._is_peft_wrapped:
                self._model = self._loader.unload_adapter(self._model, status_callback)
                self._is_peft_wrapped = False
                self._current_adapter_name = None
                if status_callback: status_callback("✓ Base Model restored.")
        
        if self._generator:
            self._generator.model = self._model

        if status_callback: status_callback("✅ VLM Context Ready.")

    def generate_response(self, 
                          images: Union[Image.Image, List[Image.Image]], 
                          prompt_text: str = None, 
                          session_folder: str = None,
                          granular_logger=None,
                          inference_params: Optional[Dict] = None,
                          batch_context: Optional[Dict] = None,
                          interaction_id: str = None) -> Dict:
        """
        Public facade for the inference task. 
        Delegates execution to the internal InferenceGenerator.
        Handles structured logging to model_interaction.json (SSOT).
        """
        if self._generator is None or self._model is None:
            raise RuntimeError("VLM Pipeline Error: Model not loaded. Initialize `load_model` first.")

        if isinstance(images, Image.Image):
            images = [images]
        elif not isinstance(images, list):
            images = list(images)

        if inference_params is None:
            inference_params = {}

        final_prompt = prompt_text if prompt_text else Config.INFERENCE_PROMPT
        final_system = inference_params.get('system_prompt', Config.INFERENCE_SYSTEM_PROMPT)

        try:
            result = self._generator.generate(
                images=images,
                prompt_text=final_prompt,
                system_prompt=final_system,
                inference_params=inference_params,
                granular_logger=granular_logger
            )

            if session_folder:
                # Log usage only, no metadata duplication
                usage_data = {
                    "input_tokens": result['usage']['input_tokens'], 
                    "output_tokens": result['usage']['output_tokens'], 
                    "inference_time": result['usage']['inference_time'], 
                    "image_count": len(images)
                }
                
                self._log_interaction(
                    session_folder=session_folder, 
                    interaction_id=interaction_id,
                    prompt_text=final_prompt, 
                    response_text=result['text'], 
                    thinking=result['thinking'], 
                    usage=usage_data,
                    batch_context=batch_context
                )

            return result

        except Exception as e:
            if granular_logger: granular_logger.log_error("VLM_GENERATE_TASK", e)
            raise e

    def _log_interaction(self, session_folder: str, interaction_id: str, prompt_text: str, response_text: str, thinking: str = None, usage: dict = None, error: Exception = None, batch_context: dict = None):
        """
        Appends a new interaction entry to the 'interactions' list in model_interaction.json.
        Schema V3: No redundant metadata, explicit ID.
        """
        if not session_folder or not os.path.exists(session_folder):
            return

        log_path = os.path.join(session_folder, 'model_interaction.json')
        
        log_entry = {
            "interaction_id": interaction_id or f"int_{int(time.time()*1000)}",
            "batch_context": batch_context or {}, 
            "prompt_text": prompt_text,
            "response_text": response_text if response_text else "[Empty Output]",
            "thinking": thinking,
            "usage": usage,
            "error": str(error) if error else None
        }

        try:
            data = {}
            if os.path.exists(log_path):
                with open(log_path, 'r', encoding='utf-8') as f:
                    try:
                        data = json.load(f)
                    except json.JSONDecodeError:
                        data = {}
            
            if "interactions" not in data:
                data["interactions"] = []
                
            data["interactions"].append(log_entry)

            with open(log_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=4, ensure_ascii=False)
                
        except Exception as e:
            logger.error(f"Failed to write to model_interaction.json: {e}")
```

File 23-58: `src\core\services\logging\__init__.py`
```py
from .granular_logger import GranularLogger

__all__ = ["GranularLogger"]
```

File 24-58: `src\core\services\logging\granular_logger.py`
```py
# src/core/services/logging/granular_logger.py
"""
Forensic Logging System for Scene Detection Playground.
Implements absolute parity between disk-based persistence and real-time frontend streaming.
Allows researchers to audit the hierarchical execution of the inference pipeline.
"""

import json
import os
import time
from datetime import datetime
from typing import Optional, Dict, Any

class GranularLogger:
    """
    Structured logger that mirrors all operations to a local file and an in-memory 
    array for real-time hierarchical UI rendering in the terminal console.
    """
    
    def __init__(self, session_folder: str, filename: str = 'qwen_inference_pipeline.log'):
        """
        Initialize the forensic trace for a specific processing session.
        """
        self.session_folder = session_folder
        self.log_file = os.path.join(session_folder, filename)
        self.console_log = [] 
        self.start_time = time.time()
        self.context_stack = [] 
        
        os.makedirs(session_folder, exist_ok=True)
        self._write_header()
    
    def _write_header(self):
        """Emits the session initialization metadata."""
        header_data = {
            'session_start': datetime.now().isoformat(),
            'timestamp_unix': self.start_time,
            'session_folder': self.session_folder
        }
        # Human-readable preamble
        readable_start = f"[SESSION START] {self.session_folder} | {header_data['session_start']}"
        self.console_log.append(readable_start)
        
        with open(self.log_file, 'w', encoding='utf-8') as f:
            f.write(readable_start + "\n")
            f.write(f"  [JSON] {json.dumps(header_data)}\n")
    
    def _format_timestamp(self) -> str:
        """Calculates relative [HH:MM:SS] timestamp from session start."""
        elapsed = time.time() - self.start_time
        return datetime.fromtimestamp(self.start_time + elapsed).strftime('[%H:%M:%S]')
    
    def _get_context_prefix(self) -> str:
        """Calculates visual indentation based on context depth."""
        if not self.context_stack:
            return ""
        depth = len(self.context_stack)
        return "  " * (depth - 1)
    
    def log(self, level: str, message: str, data: Optional[Dict[str, Any]] = None, **kwargs):
        """
        The core logging primitive. Writes a line to console and file.
        """
        timestamp = self._format_timestamp()
        prefix = self._get_context_prefix()
        
        console_line = f"{timestamp} [{level}] {prefix}{message}"
        
        self.console_log.append(console_line)
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(console_line + "\n")
            if data or kwargs:
                json_entry = {
                    'timestamp': timestamp,
                    'level': level,
                    'message': message,
                    'depth': len(self.context_stack),
                    **(data or {}),
                    **kwargs
                }
                f.write(f"  [JSON] {json.dumps(json_entry)}\n")
    
    def push_context(self, stage_name: str, **metadata):
        """Pushes a new operational context onto the stack (increases indentation)."""
        self.context_stack.append({'name': stage_name, 'metadata': metadata})
        self.log('PIPELINE', f"→ {stage_name.upper()}", metadata)
    
    def pop_context(self, status: str = 'OK', **metadata):
        """Pops the current context (decreases indentation)."""
        if self.context_stack:
            ctx = self.context_stack.pop()
            elapsed_ms = metadata.pop('elapsed_ms', None)
            status_line = f"✓ {ctx['name']} ({status})"
            if elapsed_ms:
                status_line += f" - {elapsed_ms}ms"
            self.log('PIPELINE', status_line, metadata)

    # ========== COMPUTER VISION FACADES ==========
    
    def log_scene_detect_start(self, video_path: str, fps: float, total_frames: int):
        self.push_context('pyscenedetect_init', video=video_path, fps=fps, frames=total_frames)
        self.log('SCENE_DETECT', f"Loading source: {os.path.basename(video_path)} ({total_frames} frames @ {fps}fps)")
    
    def log_scene_detect_threshold(self, threshold: float):
        self.log('SCENE_DETECT', f"Shot detection threshold configured: {threshold}")
    
    def log_scene_detect_boundary(self, frame_num: int, score: float, fps: float, is_cut: bool = True):
        timestamp_sec = frame_num / fps
        self.log('SCENE_DETECT', f"🎬 Boundary detected at frame {frame_num} ({timestamp_sec:.2f}s)")
    
    def log_scene_detect_complete(self, shots_found: int, elapsed_ms: float):
        self.log('SCENE_DETECT', f"✓ Analysis complete. {shots_found} shots identified.")
        self.pop_context('COMPLETE', shots_found=shots_found, elapsed_ms=round(elapsed_ms))
    
    # ========== VLM INFERENCE FACADES ==========
    
    def log_vlm_stage_start(self, stage: str, batch_num: int, shot_range: str):
        self.push_context(f'vlm_batch_{batch_num}', stage=stage, batch_num=batch_num, shot_range=shot_range)
    
    def log_vlm_model_load(self, model_id: str, device: str, elapsed_ms: float):
        self.log('VLM', f"Model ready: {model_id.split('/')[-1]} on {device} ({elapsed_ms/1000:.1f}s)")
        
    def log_vlm_load_mode(self, mode: str, details: str):
        """Logs detailed hardware precision mode for forensic audit."""
        self.log('VLM', f"Hardware Precision Mode: [{mode}] | Context: {details}")
    
    def log_vlm_preprocessing(self, image_count: int, image_size: tuple, tokens: int, elapsed_ms: float):
        self.log('VLM', f"Preprocessing {image_count} images ({image_size[0]}x{image_size[1]}) | {tokens} input tokens")
    
    def log_vlm_token_generation(self, generated: int, max_tokens: int, elapsed_ms: float, tokens_per_sec: float):
        self.log('TOKEN', f"Generated {generated} tokens | Performance: {tokens_per_sec:.1f} tok/s")
    
    def log_vlm_json_parse(self, json_status: str, parsed_keys: list, error: Optional[str] = None):
        if error:
            self.log('ERROR', f"VLM JSON validation failed: {error}")
        else:
            self.log('VLM', f"JSON structure validated ({json_status}): {len(parsed_keys)} keys found")
    
    def log_prompt_construction(self, chunk_num: int, shot_count: int, start_id: str, end_id: str):
        self.log('PROMPT', f"Constructing payload for batch #{chunk_num}: {shot_count} shots | {start_id} to {end_id}")

    def log_vlm_break_result(self, break_at: str):
        """Human-readable narrative decision verdict."""
        decision = str(break_at).strip().lower()
        if decision == "none" or decision == "n/a":
            self.log('VLM', "Decision: [CONTINUITY]")
        else:
            self.log('VLM', f"Decision: [RUPTURE] at {break_at}")

    def log_error(self, component: str, error: Exception):
        self.log('ERROR', f"[{component}] {type(error).__name__}: {str(error)}")

    def log_retry(self, attempt: int, max_attempts: int, reason: str):
        self.log('PIPELINE', f"Recovering (Attempt {attempt}/{max_attempts}): {reason}")
```

File 25-58: `src\core\services\monitoring_service.py`
```py
import psutil
import logging
from typing import Dict, Any

try:
    import GPUtil
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False

logger = logging.getLogger(__name__)

def get_hardware_status() -> Dict[str, Any]:
    """
    Fetches real-time hardware statistics for the monitoring deck.
    Returns a dictionary containing CPU, RAM, and GPU metrics.
    """
    stats = {
        "cpu": {},
        "ram": {},
        "gpu": []
    }

    try:
        # CPU Stats
        stats["cpu"]["usage_percent"] = psutil.cpu_percent(interval=None)
        stats["cpu"]["cores"] = psutil.cpu_count(logical=True)
        stats["cpu"]["freq_current"] = psutil.cpu_freq().current if psutil.cpu_freq() else 0

        # RAM Stats
        vm = psutil.virtual_memory()
        stats["ram"]["total_gb"] = round(vm.total / (1024**3), 2)
        stats["ram"]["available_gb"] = round(vm.available / (1024**3), 2)
        stats["ram"]["used_gb"] = round(vm.used / (1024**3), 2)
        stats["ram"]["percent"] = vm.percent

        # Disk Stats
        du = psutil.disk_usage('.')
        stats["disk"] = {
            "total_gb": round(du.total / (1024**3), 2),
            "used_gb": round(du.used / (1024**3), 2),
            "free_gb": round(du.free / (1024**3), 2),
            "percent": du.percent
        }

        # GPU Stats
        try:
            # Method 1: GPUtil (Preferred)
            if GPU_AVAILABLE:
                gpus = GPUtil.getGPUs()
                for gpu in gpus:
                    stats["gpu"].append({
                        "id": gpu.id,
                        "name": gpu.name,
                        "load_percent": round(gpu.load * 100, 1),
                        "memory_total_mb": gpu.memoryTotal,
                        "memory_used_mb": gpu.memoryUsed,
                        "memory_percent": round((gpu.memoryUsed / gpu.memoryTotal) * 100, 1) if gpu.memoryTotal > 0 else 0,
                        "temperature_c": gpu.temperature
                    })
            
            # Method 2: nvidia-smi (Fallback)
            if not stats["gpu"]:
                import subprocess
                import shutil
                
                if shutil.which('nvidia-smi'):
                    cmd = [
                        'nvidia-smi', 
                        '--query-gpu=index,name,utilization.gpu,memory.total,memory.used,temperature.gpu', 
                        '--format=csv,noheader,nounits'
                    ]
                    result = subprocess.check_output(cmd, encoding='utf-8')
                    lines = result.strip().split('\n')
                    
                    for line in lines:
                        parts = [x.strip() for x in line.split(',')]
                        if len(parts) >= 6:
                            idx = parts[0]
                            name = parts[1]
                            load = float(parts[2])
                            mem_total = float(parts[3])
                            mem_used = float(parts[4])
                            temp = float(parts[5])
                            
                            stats["gpu"].append({
                                "id": int(idx),
                                "name": name,
                                "load_percent": load,
                                "memory_total_mb": mem_total,
                                "memory_used_mb": mem_used,
                                "memory_percent": round((mem_used / mem_total) * 100, 1) if mem_total > 0 else 0,
                                "temperature_c": temp
                            })

        except Exception as e:
            logger.warning(f"Failed to fetch GPU stats: {e}")
        
    except Exception as e:
        logger.error(f"Error fetching hardware stats: {e}")

    return stats
```

File 26-58: `src\http\controllers\playground\__init__.py`
```py
"""
Playground Controller Package
----------------------------
Modularized logic for the Scene Detection Sandbox:
- views: UI rendering and model discovery.
- inference: Background task execution and control.
- sessions: SSOT rehydration and archive management.
- artifacts: Log retrieval and static asset serving.
- management: Dynamic model import and adapter upload.
"""

from flask import Blueprint

# Create the Blueprint shared by all sub-modules
playground_bp = Blueprint('playground', __name__)

# Import sub-modules to register routes
# Using delayed imports to allow the blueprint object to be initialized first
from . import views
from . import inference
from . import sessions
from . import artifacts
from . import management
from . import monitoring # Register monitoring routes
```

File 27-58: `src\http\controllers\playground\artifacts.py`
```py
import os
import json
from flask import jsonify, send_from_directory
from src.config import Config
from . import playground_bp

@playground_bp.route('/api/playground/session/<session_id>/logs', methods=['GET'])
def get_session_logs(session_id):
    """
    Retrieves execution logs and raw VLM outputs.
    Supports both legacy flat logs and new SSOT interactions list.
    """
    session_dir = os.path.join(Config.PLAYGROUND_FOLDER, session_id)
    if not os.path.exists(session_dir): 
        return jsonify({'error': 'Session not found'}), 404

    pipeline_log_path = os.path.join(session_dir, 'qwen_inference_pipeline.log')
    ssot_path = os.path.join(session_dir, 'model_interaction.json')
    
    logs = {'pipeline': "", 'debug': []}

    if os.path.exists(pipeline_log_path):
        try:
            with open(pipeline_log_path, 'r', encoding='utf-8') as f: 
                logs['pipeline'] = f.read()
        except Exception as e: 
            logs['pipeline'] = f"Error reading log: {e}"

    if os.path.exists(ssot_path):
        try:
            with open(ssot_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logs['debug'] = data.get('interactions', [])
        except Exception as e:
            logs['debug'] = [{"error": f"Failed to parse SSOT: {e}"}]

    return jsonify(logs)

@playground_bp.route('/playground/<path:filename>')
def serve_playground_asset(filename):
    """Serves static files (videos and frames) from the playground data sandbox."""
    return send_from_directory(Config.PLAYGROUND_FOLDER, filename)
```

File 28-58: `src\http\controllers\playground\inference.py`
```py
import os
import uuid
import time
from flask import request, jsonify
from src.config import Config
from src.workers.executor import executor, run_playground_inference_task
from src.workers.utils import request_task_cancellation, TASK_STATUS
from . import playground_bp

@playground_bp.route('/api/playground/inference', methods=['POST'])
def trigger_inference():
    """
    Initiates a background multimodal inference task.
    """
    if 'video' not in request.files: 
        return jsonify({'error': 'No video file provided'}), 400
        
    file = request.files['video']
    if file.filename == '': 
        return jsonify({'error': 'No video file selected'}), 400

    window_size = int(request.form.get('window_size', 32))
    model_id = request.form.get('model_id')
    adapter_input = request.form.get('adapter_path') 
    adapter_path = None
    
    # Resolve hyperparams
    inference_params = {
        'temperature': float(request.form.get('temperature', Config.INFERENCE_TEMPERATURE)),
        'top_p': float(request.form.get('top_p', Config.INFERENCE_TOP_P)),
        'max_tokens': int(request.form.get('max_tokens', Config.INFERENCE_MAX_TOKENS)),
        'repetition_penalty': float(request.form.get('repetition_penalty', Config.INFERENCE_REPETITION_PENALTY)),
        'system_prompt': request.form.get('system_prompt', Config.INFERENCE_SYSTEM_PROMPT),
        'main_prompt': request.form.get('main_prompt', Config.INFERENCE_PROMPT),
        'bypass_validation': request.form.get('bypass_validation') == 'true',
        'lora_scale': float(request.form.get('lora_scale', 1.0)),
        'high_fidelity_mode': request.form.get('high_fidelity_mode') == 'true',
        
        # Operational params
        'stream_interval': int(request.form.get('stream_interval', 30))
    }
    
    if adapter_input:
        potential_path = os.path.join(Config.BASE_DIR, 'models', 'model_adapters', adapter_input)
        if os.path.exists(potential_path):
            adapter_path = potential_path
        else:
            adapter_path = adapter_input

    # Generate IDs
    task_id = str(uuid.uuid4())
    session_id = f"play_{int(time.time())}"
    
    session_folder = os.path.join(Config.PLAYGROUND_FOLDER, session_id)
    os.makedirs(session_folder, exist_ok=True)
    
    upload_path = os.path.join(session_folder, file.filename)
    file.save(upload_path)

    # Dispatch
    executor.submit(
        run_playground_inference_task, 
        task_id, 
        session_id, 
        file.filename, 
        window_size, 
        model_id,
        adapter_path,
        inference_params
    )

    return jsonify({
        'status': 'queued', 
        'task_id': task_id, 
        'model_used': model_id
    }), 202

@playground_bp.route('/api/playground/abort/<task_id>', methods=['POST'])
def abort_task(task_id):
    """
    Signals the background worker to cease execution at the next checkpoint.
    """
    request_task_cancellation(task_id)
    return jsonify({'status': 'cancel_requested', 'task_id': task_id}), 200

@playground_bp.route('/status/<task_id>')
def task_status(task_id):
    """
    Polling endpoint for task progress.
    """
    return jsonify(TASK_STATUS.get(task_id, {'state': 'PENDING', 'status': 'Queued...'}))
```

File 29-58: `src\http\controllers\playground\management.py`
```py
import os
import json
import uuid
import zipfile
import shutil
from flask import jsonify, request
from src.config import Config, AVAILABLE_MODELS
from src.workers.executor import executor, run_model_download_task
from . import playground_bp

# --- UTILITIES ---

def scan_local_models():
    """Scans the models directory for Hugging Face snapshots."""
    models_dir = os.path.join(Config.BASE_DIR, 'models')
    local_models = []
    
    # 1. Include Configured Default if present
    if hasattr(Config, 'LOCAL_MODEL_ID') and Config.LOCAL_MODEL_ID:
        local_models.append(Config.LOCAL_MODEL_ID)

    if os.path.exists(models_dir):
        try:
            for item in os.listdir(models_dir):
                full_path = os.path.join(models_dir, item)
                if os.path.isdir(full_path):
                    if item.startswith('.') or item == 'model_adapters':
                        continue
                    
                    # Convert folder name back to HF ID format
                    # e.g., models--org--repo -> org/repo
                    model_name = item
                    if item.startswith("models--"):
                        clean_name = item[8:] # remove prefix
                        model_name = clean_name.replace("--", "/")
                    
                    if model_name not in local_models:
                        local_models.append(model_name)
        except OSError as e:
            print(f"[Model Scan] Error: {e}")

    return sorted(list(set(local_models)))

def scan_local_adapters():
    """Scans the adapters directory for valid config files."""
    adapters_dir = os.path.join(Config.BASE_DIR, 'models', 'model_adapters')
    local_adapters = []
    
    if os.path.exists(adapters_dir):
        try:
            for item in os.listdir(adapters_dir):
                adapter_path = os.path.join(adapters_dir, item)
                config_path = os.path.join(adapter_path, 'adapter_config.json')
                
                # We only list folders that actually look like adapters
                if os.path.isdir(adapter_path) and os.path.exists(config_path):
                    try:
                        with open(config_path, 'r', encoding='utf-8') as f:
                            config_data = json.load(f)
                        local_adapters.append({
                            'id': item,
                            'name': item,
                            'path': item,
                            'base_model': config_data.get('base_model_name_or_path', 'unknown'),
                            'rank': config_data.get('r', 'N/A'),
                            'alpha': config_data.get('lora_alpha', 'N/A')
                        })
                    except Exception as json_err:
                        # Add broken adapters so user knows they exist but are invalid
                        local_adapters.append({
                            'id': item,
                            'name': f"{item} (Metadata Error)",
                            'path': item,
                            'base_model': 'unknown',
                            'rank': '?',
                            'alpha': '?'
                        })
        except OSError as e:
            print(f"[Adapter Scan] Error: {e}")
            
    return local_adapters

# --- ENDPOINTS ---

@playground_bp.route('/api/playground/models/list', methods=['GET'])
def list_models():
    """Returns the combined list of Cloud and Local models."""
    local = scan_local_models()
    # Combine with Cloud defaults defined in settings/model_pricing.yaml
    combined = AVAILABLE_MODELS + local
    # Deduplicate while preserving order preference (Cloud first)
    seen = set()
    unique_models = []
    for m in combined:
        if m not in seen:
            unique_models.append(m)
            seen.add(m)
            
    return jsonify(unique_models)

@playground_bp.route('/api/playground/adapters/list', methods=['GET'])
def list_adapters():
    """Returns the list of locally available LoRA adapters."""
    return jsonify(scan_local_adapters())

@playground_bp.route('/api/playground/models/import', methods=['POST'])
def import_model():
    """Initiates a background task to download a model from Hugging Face."""
    data = request.get_json()
    model_id = data.get('model_id')
    
    if not model_id or '/' not in model_id:
        return jsonify({'error': 'Invalid Hugging Face Model ID format (expected org/repo).'}), 400

    task_id = str(uuid.uuid4())
    
    # Dispatch non-blocking download
    executor.submit(run_model_download_task, task_id, model_id.strip())

    return jsonify({
        'status': 'queued',
        'task_id': task_id,
        'message': f"Download queued for {model_id}"
    }), 202

@playground_bp.route('/api/playground/adapters/upload', methods=['POST'])
def upload_adapter():
    """
    Handles .zip upload, secure extraction, and cleanup for LoRA adapters.
    This is a synchronous operation for simplicity as zip extraction is generally fast.
    """
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400
        
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400
        
    if not file.filename.endswith('.zip'):
        return jsonify({'error': 'Only .zip archives are supported.'}), 400

    # Prepare directories
    adapters_root = os.path.join(Config.BASE_DIR, 'models', 'model_adapters')
    os.makedirs(adapters_root, exist_ok=True)

    # Sanitize filename to create folder name
    folder_name = os.path.splitext(file.filename)[0].replace(' ', '_')
    target_dir = os.path.join(adapters_root, folder_name)
    
    # Save temp zip
    temp_zip_path = os.path.join(adapters_root, file.filename)
    file.save(temp_zip_path)

    try:
        # Check if target directory already exists
        if os.path.exists(target_dir):
            # Cleanup old version to allow overwrite
            shutil.rmtree(target_dir)
        os.makedirs(target_dir)

        # Secure extraction
        with zipfile.ZipFile(temp_zip_path, 'r') as zip_ref:
            # Prevent zip bombs / path traversal
            for member in zip_ref.infolist():
                if member.filename.startswith('/') or '..' in member.filename:
                    raise ValueError(f"Malicious path detected in zip: {member.filename}")
                zip_ref.extract(member, target_dir)
        
        # Cleanup zip
        os.remove(temp_zip_path)
        
        # Verify it looks like an adapter
        if not os.path.exists(os.path.join(target_dir, 'adapter_config.json')):
             return jsonify({
                 'status': 'warning', 
                 'message': 'Upload successful, but "adapter_config.json" was not found in the root. The adapter may not be recognized.'
             }), 200

        return jsonify({
            'status': 'success', 
            'adapter_id': folder_name,
            'message': f"Adapter '{folder_name}' successfully installed."
        }), 200

    except Exception as e:
        # Cleanup on failure
        if os.path.exists(temp_zip_path):
            os.remove(temp_zip_path)
        if os.path.exists(target_dir):
            shutil.rmtree(target_dir)
            
        return jsonify({'error': f"Extraction failed: {str(e)}"}), 500
```

File 30-58: `src\http\controllers\playground\monitoring.py`
```py
from flask import jsonify
from src.http.controllers.playground import playground_bp
from src.core.services.monitoring_service import get_hardware_status

@playground_bp.route('/api/playground/hardware', methods=['GET'])
def get_hardware_metrics():
    """
    API endpoint to retrieve real-time hardware statistics.
    """
    stats = get_hardware_status()
    return jsonify(stats)
```

File 31-58: `src\http\controllers\playground\sessions.py`
```py
import os
import json
import re
import shutil
from flask import jsonify
from src.config import Config
from . import playground_bp

@playground_bp.route('/api/playground/sessions', methods=['GET'])
def list_sessions():
    """Returns the registry of archived inference runs by parsing SSOT files."""
    if not os.path.exists(Config.PLAYGROUND_FOLDER): 
        return jsonify([])

    sessions = []
    try:
        with os.scandir(Config.PLAYGROUND_FOLDER) as entries:
            for entry in entries:
                if entry.is_dir():
                    ssot_path = os.path.join(entry.path, 'model_interaction.json')
                    
                    if os.path.exists(ssot_path):
                        try:
                            with open(ssot_path, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                if "session_metadata" in data:
                                    meta = data['session_metadata']
                                    cv = data.get('shot_list', data.get('cv_manifest', []))
                                    perf = meta.get('performance', {})
                                    
                                    sessions.append({
                                        'session_id': entry.name,
                                        'timestamp': meta.get('created_at', ''),
                                        'video_filename': meta.get('video_filename', 'Unknown'),
                                        'model_id': meta.get('model_id', 'Unknown'),
                                        'adapter': meta.get('adapter'),
                                        'window_size': meta.get('window_size', 32),
                                        'shot_count': len(cv),
                                        'scene_count': meta.get('scene_count', 0),
                                        'video_duration': meta.get('video_duration', None),
                                        'duration': perf.get('total_task', 0),
                                        'inference_params': meta.get('inference_params', {})
                                    })
                        except Exception as e:
                            print(f"Skipping corrupt SSOT session {entry.name}: {e}")
                            continue
                            
                    # Fallback for Legacy Sessions (state.json)
                    legacy_path = os.path.join(entry.path, 'state.json')
                    if not os.path.exists(ssot_path) and os.path.exists(legacy_path):
                         try:
                            with open(legacy_path, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                meta = data.get('metadata', {})
                                shots = data.get('shots', [])
                                perf = meta.get('performance', {})
                                sessions.append({
                                    'session_id': entry.name,
                                    'timestamp': meta.get('timestamp', ''),
                                    'video_filename': meta.get('video_filename', 'Unknown') + " (LEGACY)",
                                    'model_id': meta.get('model_id', 'Unknown'),
                                    'adapter': meta.get('adapter'),
                                    'window_size': meta.get('window_size', 32),
                                    'shot_count': len(shots),
                                    'scene_count': len([s for s in shots if s.get('is_scene_break')]) + 1,
                                    'video_duration': meta.get('video_duration', None),
                                    'duration': perf.get('total_task', 0),
                                    'inference_params': meta.get('inference_params', {})
                                })
                         except:
                             continue

        sessions.sort(key=lambda x: x['timestamp'], reverse=True)
        return jsonify(sessions)
    except Exception as e: 
        return jsonify({'error': str(e)}), 500

@playground_bp.route('/api/playground/session/<session_id>', methods=['GET'])
def get_session_data(session_id):
    """
    Hydrates the full session state from the SSOT interaction log.
    Strictly re-derives narrative breaks by parsing interaction text.
    """
    session_dir = os.path.join(Config.PLAYGROUND_FOLDER, session_id)
    ssot_path = os.path.join(session_dir, 'model_interaction.json')
    
    if not os.path.exists(ssot_path): 
        legacy_path = os.path.join(session_dir, 'state.json')
        if os.path.exists(legacy_path):
            with open(legacy_path, 'r', encoding='utf-8') as f: 
                return jsonify(json.load(f))
        return jsonify({'error': 'Session data not found'}), 404

    try:
        with open(ssot_path, 'r', encoding='utf-8') as f: 
            data = json.load(f)

        manifest = data.get('shot_list', data.get('cv_manifest', []))
        interactions = data.get('interactions', [])
        metadata = data.get('session_metadata', {})

        # 1. Create a quick lookup map for shots
        shot_map = {shot['shot_id']: shot for shot in manifest}
        
        # 2. Reset flags (The manifest is strictly CV data now)
        for shot in manifest:
            shot['is_scene_break'] = False
            shot['scene_logic'] = {"case_type": "NARRATIVE_UNITY"}
            shot['logic_analysis'] = {"reasoning": ""}

        # 3. Dynamic Rehydration
        scene_count = 1
        
        for ix in interactions:
            resp_text = ix.get('response_text', '')
            thinking = ix.get('thinking', '')
            
            # Robust JSON extraction
            match = re.search(r'\{.*\}', resp_text, re.DOTALL)
            if match:
                try:
                    ai_result = json.loads(match.group())
                    break_at = str(ai_result.get('break_at', 'NONE')).strip()
                    
                    if break_at.upper() != 'NONE' and break_at in shot_map:
                        target_shot = shot_map[break_at]
                        target_shot['is_scene_break'] = True
                        target_shot['scene_logic'] = {"case_type": ai_result.get('case_type', 'RUPTURE')}
                        target_shot['logic_analysis'] = {"reasoning": thinking}
                        scene_count += 1
                except:
                    pass 

        response_payload = {
            "metadata": metadata,
            "shots": manifest
        }
        
        response_payload['metadata']['scene_count'] = scene_count
        
        return jsonify(response_payload)

    except Exception as e: 
        return jsonify({'error': str(e)}), 500

@playground_bp.route('/api/playground/session/<session_id>', methods=['DELETE'])
def delete_session(session_id):
    session_dir = os.path.join(Config.PLAYGROUND_FOLDER, session_id)
    if not os.path.exists(session_dir): 
        return jsonify({'error': 'Session not found'}), 404
    try:
        shutil.rmtree(session_dir)
        return jsonify({'status': 'deleted'}), 200
    except Exception as e: 
        return jsonify({'error': str(e)}), 500
```

File 32-58: `src\http\controllers\playground\views.py`
```py
import os
import json
from flask import render_template
from src.config import Config, AVAILABLE_MODELS
from src.core.engines.qwen.diagnostics import check_gpu_capacity
from . import playground_bp

@playground_bp.route('/playground')
def index():
    """
    Renders the Playground evaluation interface.
    Performs model discovery and hardware capability checks.
    """
    models_dir = os.path.join(Config.BASE_DIR, 'models')
    local_models = []

    # 1. Discover Base Models
    if hasattr(Config, 'LOCAL_MODEL_ID') and Config.LOCAL_MODEL_ID:
        local_models.append(Config.LOCAL_MODEL_ID)

    if os.path.exists(models_dir):
        try:
            for item in os.listdir(models_dir):
                full_path = os.path.join(models_dir, item)
                if os.path.isdir(full_path):
                    if item.startswith('.') or item == 'model_adapters':
                        continue
                    model_name = item
                    if item.startswith("models--"):
                        clean_name = item[8:] 
                        model_name = clean_name.replace("--", "/")
                    if model_name not in local_models:
                        local_models.append(model_name)
        except OSError as e:
            print(f"[Playground View] Error scanning models: {e}")

    local_models = sorted(list(set(local_models)))
    combined_models = AVAILABLE_MODELS + local_models
    
    # 2. Discover Adapters
    adapters_dir = os.path.join(Config.BASE_DIR, 'models', 'model_adapters')
    local_adapters = []
    
    if os.path.exists(adapters_dir):
        try:
            for item in os.listdir(adapters_dir):
                adapter_path = os.path.join(adapters_dir, item)
                config_path = os.path.join(adapter_path, 'adapter_config.json')
                
                if os.path.isdir(adapter_path) and os.path.exists(config_path):
                    try:
                        with open(config_path, 'r', encoding='utf-8') as f:
                            config_data = json.load(f)
                        local_adapters.append({
                            'id': item,
                            'name': item,
                            'path': item,
                            'base_model': config_data.get('base_model_name_or_path', 'unknown'),
                            'rank': config_data.get('r', 'N/A'),
                            'alpha': config_data.get('lora_alpha', 'N/A')
                        })
                    except Exception as json_err:
                        print(f"[Playground View] Adapter config error {item}: {json_err}")
                        local_adapters.append({
                            'id': item,
                            'name': f"{item} (Metadata Error)",
                            'path': item,
                            'base_model': 'unknown',
                            'rank': '?',
                            'alpha': '?'
                        })
        except OSError as e:
            print(f"[Playground View] Error scanning adapters: {e}")

    # 3. Defaults & Hardware Check
    inference_defaults = {
        'temperature': Config.INFERENCE_TEMPERATURE,
        'top_p': Config.INFERENCE_TOP_P,
        'max_tokens': Config.INFERENCE_MAX_TOKENS,
        'repetition_penalty': Config.INFERENCE_REPETITION_PENALTY,
        'system_prompt': Config.INFERENCE_SYSTEM_PROMPT,
        'main_prompt': Config.INFERENCE_PROMPT,
        'lora_scale': 0.1,      # UPDATED DEFAULT
        'stream_interval': 100  # UPDATED DEFAULT
    }

    gpu_status = check_gpu_capacity(required_vram_gb=12.0)

    return render_template(
        'playground.html', 
        models=combined_models, 
        adapters=local_adapters, 
        cloud_models=AVAILABLE_MODELS,
        defaults=inference_defaults,
        gpu_capabilities=gpu_status 
    )
```

File 33-58: `src\utils\file_utils.py`
```py
import os
import base64

def encode_image_to_base64(filepath):
    """
    Reads an image file from disk and returns a Base64 Data URI string.
    Supported by most multimodal LLM APIs and useful for frontend rendering 
    without static file serving.
    
    Format: "data:image/jpeg;base64,..."
    """
    if not os.path.exists(filepath):
        return None
        
    try:
        with open(filepath, "rb") as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
            # Determine mime type based on extension, default to jpeg
            ext = os.path.splitext(filepath)[1].lower()
            mime_type = "image/png" if ext == ".png" else "image/jpeg"
            return f"data:{mime_type};base64,{encoded_string}"
    except Exception as e:
        print(f"[File Utils] Failed to encode image {filepath}: {e}")
        return None

def ensure_dir(directory_path):
    """Simple utility to ensure a directory exists."""
    if not os.path.exists(directory_path):
        os.makedirs(directory_path, exist_ok=True)
```

File 34-58: `src\utils\math_utils.py`
```py
def calculate_narrative_metrics(scene_profiles):
    """
    Calculates statistical KPIs and prepares scene profiles for narrative distribution analysis.
    This logic extracts metrics like average scene length and identifies peaks 
    in scene frequency bins.
    """
    if not scene_profiles:
        return {
            'min': 0, 'max': 0, 'avg': 0, 
            'mode_range': 'N/A', 'mode_val': 0,
            'distribution': {},
            'raw_data': []
        }

    lengths = [p['length'] for p in scene_profiles]
    
    _min = min(lengths)
    _max = max(lengths)
    _avg = sum(lengths) / len(lengths)

    # Calculate Frequency Distribution (Raw Shot Counts)
    distribution = {}
    for length in lengths:
        distribution[length] = distribution.get(length, 0) + 1
    
    sorted_dist = dict(sorted(distribution.items()))

    # Group into Bins for 'Mode Range' analysis (5-shot increments)
    bins = {}
    for length in lengths:
        bin_idx = (length - 1) // 5
        bin_key = f"{(bin_idx * 5) + 1}-{(bin_idx + 1) * 5}"
        bins[bin_key] = bins.get(bin_key, 0) + 1
    
    mode_range = max(bins, key=bins.get) if bins else "0-0"
        
    try:
        # Determine the midpoint of the most common range for numerical analysis
        mr_start, mr_end = map(int, mode_range.split('-'))
        mode_val = (mr_start + mr_end) / 2
    except:
        mode_val = 0

    return {
        'min': _min, 'max': _max, 'avg': round(_avg, 1),
        'mode_range': mode_range, 'mode_val': mode_val,
        'distribution': sorted_dist,
        'raw_data': scene_profiles
    }
```

File 35-58: `src\workers\executor.py`
```py
"""
Scene Detection Playground Executor.
Orchestrates asynchronous VLM inference tasks for model evaluation.
This module has been stripped of all production data annotation and export logic.
"""

from concurrent.futures import ThreadPoolExecutor

# Global executor for background tasks. 
# 4 workers is sufficient for local evaluation and concurrent cloud API requests.
executor = ThreadPoolExecutor(max_workers=4)

# Core status tracking and cost calculation utilities
from .utils import (
    TASK_STATUS, 
    update_status, 
    calculate_cost
)

# Load the primary Playground task
from .tasks.analysis_tasks import (
    run_playground_inference_task
)

# Load Asset Management tasks
from .tasks.asset_tasks import (
    run_model_download_task
)

__all__ = [
    "executor",
    "TASK_STATUS",
    "update_status",
    "calculate_cost",
    "run_playground_inference_task",
    "run_model_download_task"
]
```

File 36-58: `src\workers\tasks\analysis_tasks.py`
```py
import os
import json
import time
import traceback
import logging
import io
import re
from PIL import Image
from typing import List, Dict, Optional
from datetime import datetime
from src.config import Config, AVAILABLE_MODELS
from src.core.engines.cv_engine import CVEngine
from src.core.engines.qwen_client import QwenClient
from src.core.engines.gemini_client import GeminiClient
from src.core.services.logging.granular_logger import GranularLogger
from src.workers.utils import update_status, is_task_cancelled
from google.genai import types

logger_builtin = logging.getLogger(__name__)

MAX_RETRIES = 3

def run_playground_inference_task(
    task_id: str, 
    session_id: str, 
    video_filename: str, 
    window_size: int = 32, 
    model_id: str = None, 
    adapter_path: str = None,
    inference_params: Optional[Dict] = None
):
    """
    Hybrid Playground Pipeline with dynamic precision and forensic logging.
    Strict implementation of SSOT Pattern (V3 Schema).
    Enforces Fixed Visual Batch Size via Black Frame Padding.
    Supports Runtime Prompt Overrides and Operational Params Sanitization.
    """
    session_folder = os.path.join(Config.PLAYGROUND_FOLDER, session_id)
    video_path = os.path.join(session_folder, video_filename)
    log_path = os.path.join(session_folder, 'model_interaction.json')
    
    os.makedirs(session_folder, exist_ok=True)
    f_logger = GranularLogger(session_folder)
    
    if inference_params is None:
        inference_params = {}
    
    # Extract params for logic
    runtime_system = inference_params.get('system_prompt', Config.INFERENCE_SYSTEM_PROMPT)
    runtime_main_template = inference_params.get('main_prompt', Config.INFERENCE_PROMPT)
    bypass_validation = inference_params.get('bypass_validation', False)
    high_fidelity_mode = inference_params.get('high_fidelity_mode', False)

    # Sanitize params for metadata (remove operational flags)
    meta_params = inference_params.copy()
    meta_params.pop('stream_interval', None)

    try:
        timings = {}
        start_task = time.time()
        
        f_logger.push_context('inference_run', model=model_id, window=window_size, video=video_filename, adapter=adapter_path)
        f_logger.log('CLIENT', f"Runtime Params: Temp={inference_params.get('temperature')}, BypassValidation={bypass_validation}, HiFi={high_fidelity_mode}")
        update_status(task_id, 'PROGRESS', 0, 4, "Initializing Workspace", logs=f_logger.console_log)

        if is_task_cancelled(task_id):
            f_logger.log('PIPELINE', "⚠️ Process aborted by user command.")
            update_status(task_id, 'FAILURE', status="Aborted by User", logs=f_logger.console_log)
            return

        # 1. ENGINE SELECTION
        is_cloud_model = model_id and ("gemini" in model_id.lower() or model_id in AVAILABLE_MODELS)
        ai_service = None
        gemini_client = None
        
        if is_cloud_model:
            f_logger.log('CLIENT', f"Authenticating Cloud API session: {model_id}")
            gemini_client = GeminiClient()
        else:
            f_logger.log('CLIENT', "Warming local VLM context...")
            ai_service = QwenClient()
            ai_service.load_model(
                target_model_id=model_id, 
                adapter_path=adapter_path, 
                status_callback=lambda m: f_logger.log('VLM', m), 
                logger_instance=f_logger,
                bypass_validation=bypass_validation,
                high_fidelity_mode=high_fidelity_mode 
            )

        if is_task_cancelled(task_id):
            f_logger.log('PIPELINE', "⚠️ Process aborted by user command.")
            update_status(task_id, 'FAILURE', status="Aborted by User", logs=f_logger.console_log)
            return

        # 2. COMPUTER VISION STAGE
        f_logger.log('PIPELINE', "Commencing Computer Vision Stage")
        update_status(task_id, 'PROGRESS', 1, 4, "CV Analysis", logs=f_logger.console_log)
        
        shots, video_duration = CVEngine.analyze_shot_boundaries(video_path, logger=f_logger)

        if is_task_cancelled(task_id):
            f_logger.log('PIPELINE', "⚠️ Process aborted by user command.")
            update_status(task_id, 'FAILURE', status="Aborted by User", logs=f_logger.console_log)
            return

        shots = CVEngine.generate_visual_anchors(video_path, shots, session_folder, logger=f_logger)
        
        timings['cv_processing'] = round(time.time() - start_task, 2)
        
        # --- SSOT INITIALIZATION ---
        clean_manifest = []
        whitelist = ['shot_id', 'start_time', 'end_time', 'start_frame', 'end_frame', 'image_paths']
        for s in shots:
            clean_s = {k: v for k, v in s.items() if k in whitelist}
            clean_manifest.append(clean_s)

        init_data = {
            "session_metadata": {
                "session_id": session_id,
                "video_filename": video_filename,
                "video_duration": round(video_duration, 2),
                "model_id": model_id or "Unknown",
                "adapter": adapter_path,
                "created_at": datetime.now().isoformat(),
                "window_size": window_size,
                "inference_params": meta_params, # Use sanitized params
                "mode": "STANDALONE_PLAYGROUND_V3_SSOT"
            },
            "shot_list": clean_manifest, 
            "interactions": [] 
        }
        
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(init_data, f, indent=4, ensure_ascii=False)
        
        # 3. NARRATIVE ADJUDICATION STAGE
        id_to_idx = {s['shot_id']: i for i, s in enumerate(shots)}
        current_idx = 0
        batch_count = 0
        overlap_frames = int(window_size * 0.25)
        default_stride = max(1, window_size - overlap_frames)

        f_logger.log('PIPELINE', "Transitioning to Narrative Adjudication")
        update_status(task_id, 'PROGRESS', 2, 4, "Narrative Adjudication", logs=f_logger.console_log)

        while current_idx < len(shots):
            if is_task_cancelled(task_id):
                f_logger.log('PIPELINE', "⚠️ Process aborted by user command.")
                update_status(task_id, 'FAILURE', status="Aborted by User", logs=f_logger.console_log)
                return

            batch_count += 1
            chunk_end = min(current_idx + window_size, len(shots))
            chunk = shots[current_idx : chunk_end]
            chunk_ids = [s['shot_id'] for s in chunk]
            
            f_logger.log_vlm_stage_start('ADJUDICATION', batch_count, f"{chunk[0]['shot_id']} → {chunk[-1]['shot_id']}")

            if len(chunk) < 2:
                f_logger.log('VLM', "Closing stream: Single shot tail batch.")
                f_logger.pop_context('SKIP')
                break 

            # Load VALID images
            pil_images = []
            for s in chunk:
                if s.get('image_paths'):
                    img_path = os.path.join(session_folder, s['image_paths'][0])
                    with Image.open(img_path) as img:
                        pil_images.append(img.convert('RGB'))

            # Padding Logic
            num_valid = len(pil_images)
            if num_valid > 0 and num_valid < window_size:
                padding_needed = window_size - num_valid
                f_logger.log('VLM', f"Padding batch with {padding_needed} black frames (Window: {window_size})")
                ref_w, ref_h = pil_images[-1].size
                black_frame = Image.new('RGB', (ref_w, ref_h), (0, 0, 0))
                for _ in range(padding_needed):
                    pil_images.append(black_frame)

            f_logger.log_prompt_construction(batch_count, len(chunk), chunk[0]['shot_id'], chunk[-1]['shot_id'])

            retry_attempt = 0
            valid_batch = False
            detected_break_id = 'NONE'
            
            interaction_id = f"int_{batch_count:03d}"
            batch_ctx = {
                "batch_id": batch_count,
                "range_start": chunk[0]['shot_id'],
                "range_end": chunk[-1]['shot_id']
            }
            
            while retry_attempt < MAX_RETRIES and not valid_batch:
                if is_task_cancelled(task_id):
                    f_logger.log('PIPELINE', "⚠️ Process aborted by user command.")
                    update_status(task_id, 'FAILURE', status="Aborted by User", logs=f_logger.console_log)
                    return

                retry_attempt += 1
                if retry_attempt > 1: 
                    f_logger.log_retry(retry_attempt, MAX_RETRIES, "Format violation or parse failure")

                try:
                    ids_str = ", ".join(chunk_ids)
                    grounded_prompt = runtime_main_template.replace('{{VALID_SHOT_IDS}}', ids_str)

                    if is_cloud_model:
                        contents = [grounded_prompt]
                        for img in pil_images:
                            buf = io.BytesIO()
                            img.save(buf, format='JPEG', quality=85)
                            contents.append(types.Part.from_bytes(data=buf.getvalue(), mime_type="image/jpeg"))
                        
                        start_cloud = time.time()
                        resp, usage_info = gemini_client.generate_with_backoff(
                            model=model_id, 
                            contents=contents,
                            config=types.GenerateContentConfig(
                                temperature=inference_params.get('temperature', Config.INFERENCE_TEMPERATURE), 
                                max_output_tokens=inference_params.get('max_tokens', Config.INFERENCE_MAX_TOKENS),
                                top_p=inference_params.get('top_p', Config.INFERENCE_TOP_P),
                                system_instruction=runtime_system,
                                response_mime_type="application/json"
                            ),
                            session_folder=session_folder,
                            image_count=len(pil_images),
                            label=f"Batch {batch_count}",
                            prompt_text=grounded_prompt,
                            inference_params=inference_params,
                            batch_context=batch_ctx,
                            interaction_id=interaction_id 
                        )
                        
                        gen_time = time.time() - start_cloud
                        usage_info['inference_time'] = round(gen_time, 2)
                        
                        response_data = {
                            'text': resp.text, 
                            'thinking': None,
                            'usage': usage_info
                        }
                    else:
                        response_data = ai_service.generate_response(
                            pil_images, 
                            prompt_text=grounded_prompt, 
                            session_folder=session_folder, 
                            granular_logger=f_logger,
                            inference_params=inference_params,
                            batch_context=batch_ctx,
                            interaction_id=interaction_id 
                        )

                    raw_json = response_data.get('text', '{}')
                    cleaned = raw_json.replace('```json', '').replace('```', '').strip()
                    cleaned = re.sub(r'<think>.*?</think>', '', cleaned, flags=re.DOTALL | re.IGNORECASE).strip()
                    
                    match = re.search(r'\{.*\}', cleaned, re.DOTALL)
                    if not match: 
                        raise ValueError("VLM response contained no valid JSON object.")
                    
                    ai_result = json.loads(match.group())
                    raw_break_id = ai_result.get('break_at', 'NONE')
                    
                    f_logger.log_vlm_break_result(raw_break_id)

                    normalized_id = str(raw_break_id).strip().upper()
                    if normalized_id == 'NONE':
                        detected_break_id = 'NONE'
                        valid_batch = True
                    elif raw_break_id in chunk_ids:
                        if raw_break_id == chunk_ids[0]:
                            f_logger.log('ERROR', f"Invalid Shot ID '{raw_break_id}'. Reason: The anchor shot cannot be a break point.")
                        else:
                            detected_break_id = raw_break_id
                            valid_batch = True
                    else:
                        f_logger.log('ERROR', f"Invalid Shot ID '{raw_break_id}'. Reason: Shot ID not in context.")

                except Exception as e:
                    f_logger.log_error("WINDOW_INFERENCE", e)
                    continue

            if detected_break_id != 'NONE' and detected_break_id in id_to_idx:
                global_break_idx = id_to_idx[detected_break_id]
                current_idx = global_break_idx
            else:
                current_idx += default_stride

            f_logger.pop_context('SUCCESS', elapsed_ms=round((time.time() - start_task) * 1000))
            update_status(task_id, 'PROGRESS', 3, 4, f"Batch {batch_count} processed.", logs=f_logger.console_log)

        total_runtime = round(time.time() - start_task, 2)
        
        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                final_data = json.load(f)
            
            final_data['session_metadata']['performance'] = {"total_task": total_runtime}
            final_data['session_metadata']['status'] = "COMPLETED"
            
            with open(log_path, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, indent=4, ensure_ascii=False)
        except Exception as e:
            f_logger.log_error("METADATA_UPDATE", e)

        f_logger.log('PIPELINE', "Narrative Evaluation Finalized.")
        f_logger.pop_context('COMPLETE', total_time=total_runtime)
        
        update_status(task_id, 'SUCCESS', 4, 4, "Inference Complete", result={'session_id': session_id}, logs=f_logger.console_log)

    except Exception as e:
        logger_builtin.error(traceback.format_exc())
        if f_logger: 
            f_logger.log_error("PIPELINE_CRITICAL_FAILURE", e)
        update_status(task_id, 'FAILURE', status=f"Pipeline Crash: {str(e)}", logs=f_logger.console_log if f_logger else None)
```

File 37-58: `src\workers\tasks\asset_tasks.py`
```py
import os
import traceback
import logging
import time
import threading
from huggingface_hub import snapshot_download
from src.config import Config
from src.workers.utils import update_status, is_task_cancelled

logger = logging.getLogger(__name__)

def get_directory_size(path):
    """Calculates total size of a directory in bytes."""
    total_size = 0
    try:
        for dirpath, dirnames, filenames in os.walk(path):
            for f in filenames:
                fp = os.path.join(dirpath, f)
                # Skip if it is symbolic link
                if not os.path.islink(fp):
                    total_size += os.path.getsize(fp)
    except Exception:
        pass # Fail silently during calculation
    return total_size

def format_size(bytes_val):
    """Formats bytes to human readable string."""
    if bytes_val < 1024:
        return f"{bytes_val} B"
    elif bytes_val < 1024**2:
        return f"{bytes_val/1024:.1f} KB"
    elif bytes_val < 1024**3:
        return f"{bytes_val/(1024**2):.1f} MB"
    else:
        return f"{bytes_val/(1024**3):.2f} GB"

def monitor_download(task_id, folder_path, stop_event):
    """Background thread to poll directory size and update status."""
    while not stop_event.is_set():
        if os.path.exists(folder_path):
            size = get_directory_size(folder_path)
            # Only update if we have data, keeps the UI feeling 'alive'
            if size > 0:
                update_status(task_id, 'PROGRESS', 1, 3, f"Downloading... ({format_size(size)})")
        time.sleep(1.5)

def run_model_download_task(task_id: str, model_id: str):
    """
    Background task to download a Hugging Face model to the local cache.
    Fixed for Windows non-admin privileges by disabling symlinks.
    Includes active storage monitoring for UI feedback.
    """
    stop_monitor = threading.Event()
    monitor_thread = None

    try:
        update_status(task_id, 'PROGRESS', 0, 3, f"Initializing request for {model_id}...")
        
        # 1. Validation & Setup
        if not model_id:
            raise ValueError("Invalid Model ID provided.")
            
        models_root = os.path.join(Config.BASE_DIR, 'models')
        os.makedirs(models_root, exist_ok=True)
        
        # Construct target directory name manually to match HF cache structure 
        # but without using the cache system's symlink requirement.
        # Structure: models/models--org--repo
        safe_name = model_id.replace('/', '--')
        folder_name = f"models--{safe_name}"
        target_dir = os.path.join(models_root, folder_name)
        
        if is_task_cancelled(task_id):
            update_status(task_id, 'FAILURE', status="Download aborted by user.")
            return

        # 2. Execution
        update_status(task_id, 'PROGRESS', 1, 3, "Contacting Hugging Face Hub...")
        
        # Start the monitoring thread
        monitor_thread = threading.Thread(
            target=monitor_download, 
            args=(task_id, target_dir, stop_monitor),
            daemon=True
        )
        monitor_thread.start()

        # Download using local_dir + no_symlinks to avoid WinError 1314
        local_path = snapshot_download(
            repo_id=model_id,
            local_dir=target_dir,
            local_dir_use_symlinks=False, # FORCE COPY MODE
            resume_download=True
        )
        
        # Stop monitoring
        stop_monitor.set()
        if monitor_thread:
            monitor_thread.join(timeout=2.0)

        if is_task_cancelled(task_id):
            update_status(task_id, 'FAILURE', status="Download aborted by user.")
            return

        update_status(task_id, 'PROGRESS', 2, 3, "Finalizing file integrity...")
        
        # 3. Completion
        final_size = get_directory_size(target_dir)
        update_status(
            task_id, 
            'SUCCESS', 
            3, 
            3, 
            f"Download complete ({format_size(final_size)}).", 
            result={'model_id': model_id, 'local_path': local_path}
        )

    except Exception as e:
        stop_monitor.set()
        if monitor_thread:
            monitor_thread.join(timeout=1.0)
            
        logger.error(f"Download Task Failed: {traceback.format_exc()}")
        
        err_msg = str(e)
        if "401" in err_msg or "403" in err_msg:
            err_msg = "Authentication failed. Model may be gated/private or ID is incorrect."
        elif "404" in err_msg:
            err_msg = f"Model ID '{model_id}' not found on Hugging Face."
            
        update_status(
            task_id, 
            'FAILURE', 
            status=f"Error: {err_msg}"
        )
```

File 38-58: `src\workers\utils.py`
```py
import threading
from src.config import PRICING_DATA, Config

# Global registry to track the state of asynchronous background tasks.
# Supports 'logs' for real-time hierarchical terminal streaming.
TASK_STATUS = {}

# Registry for cancellation signals (Cooperative Multitasking)
TASK_CANCEL_REGISTRY = {}

# Thread-safe re-entrant lock for state updates and progress tracking.
state_lock = threading.RLock()

def update_status(task_id, state, step=0, total=4, status="", result=None, logs=None):
    """
    Updates the global task registry for frontend progress tracking.
    """
    with state_lock:
        TASK_STATUS[task_id] = {
            'state': state,
            'step': step,
            'total': total,
            'status': status,
            'result': result,
            'logs': logs or []
        }

def request_task_cancellation(task_id):
    """Sets the cancellation flag for a specific task."""
    with state_lock:
        TASK_CANCEL_REGISTRY[task_id] = True

def is_task_cancelled(task_id):
    """Checks if a cancellation signal has been received for the task."""
    with state_lock:
        return TASK_CANCEL_REGISTRY.get(task_id, False)

def cleanup_task_registry(task_id):
    """Removes task data from registries to free memory after completion."""
    with state_lock:
        if task_id in TASK_CANCEL_REGISTRY:
            del TASK_CANCEL_REGISTRY[task_id]
        # Note: We intentionally keep TASK_STATUS so the frontend can retrieve the final result.

def calculate_cost(model_id, input_tokens, output_tokens):
    """
    Calculates the financial cost of a VLM inference batch based on pricing metadata.
    Returns 0.0 for local models or unknown model IDs.
    """
    if not model_id:
        return 0.0

    # Pricing resolution
    prices = PRICING_DATA.get(model_id, PRICING_DATA.get('default', {'input_price': 0, 'output_price': 0}))
    
    # Tiered pricing support (common for Gemini Pro models)
    if "tier_1" in prices:
        tier_1 = prices['tier_1']
        tier_2 = prices['tier_2']
        if input_tokens <= tier_1['input_threshold']:
            i_price = tier_1['price_input_per_million']
            o_price = tier_1['price_output_per_million']
        else:
            i_price = tier_2['price_input_per_million']
            o_price = tier_2['price_output_per_million']
    else:
        # Standard flat pricing per million tokens
        i_price = prices.get('input_price', 0.0)
        o_price = prices.get('output_price', 0.0)

    total = ((input_tokens / 1000000) * i_price) + ((output_tokens / 1000000) * o_price)
    return round(total, 6)
```

File 39-58: `static\css\base.css`
```css
/* Global Normalization */
* {
    box-sizing: border-box;
    -webkit-font-smoothing: antialiased;
    -moz-osx-font-smoothing: grayscale;
}

body {
    font-family: 'Inter', -apple-system, system-ui, sans-serif;
    background-color: var(--bg-color);
    background-image:
        linear-gradient(rgba(255, 255, 255, 0.02) 1px, transparent 1px),
        linear-gradient(90deg, rgba(255, 255, 255, 0.02) 1px, transparent 1px);
    background-size: 40px 40px;
    color: var(--text-main);
    margin: 0;
    line-height: 1.4;
    overflow: hidden;
    /* From line 95 */
    display: flex;
    /* From line 524 */
    flex-direction: column;
    /* From line 525 */
    height: 100vh;
    /* From line 526 */
}

.hidden {
    display: none !important;
}

/* Professional Thin Scrollbars */
::-webkit-scrollbar {
    width: 6px;
    height: 6px;
}

::-webkit-scrollbar-track {
    background: transparent;
}

::-webkit-scrollbar-thumb {
    background: #333;
    border-radius: 3px;
}

::-webkit-scrollbar-thumb:hover {
    background: var(--accent-dim);
}
```

File 40-58: `static\css\components.css`
```css
/* --- GLOBAL COMPONENT FOUNDATIONS --- */

/* Standardized Typography */
.sys-label {
    font-size: 0.6rem;
    color: var(--accent);
    font-weight: 700;
    text-transform: uppercase;
    letter-spacing: 0.2em;
    display: block;
    text-shadow: 0 0 8px rgba(0, 240, 255, 0.4);
}

.config-label {
    font-size: 0.65rem;
    color: var(--text-dim);
    margin-bottom: var(--spacing-sm);
    display: block;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.1em;
}

/* Standardized Buttons */
.btn {
    height: 42px;
    padding: 0 1.5rem;
    border-radius: var(--radius-sm);
    /* Techy look */
    font-size: 0.7rem;
    font-weight: 700;
    text-transform: uppercase;
    letter-spacing: 0.1em;
    cursor: pointer;
    border: 1px solid var(--border);
    transition: all 0.2s cubic-bezier(0.2, 0.8, 0.2, 1);
    display: inline-flex;
    align-items: center;
    justify-content: center;
    gap: 10px;
    text-decoration: none;
    white-space: nowrap;
    outline: none;
    background: rgba(255, 255, 255, 0.02);
    backdrop-filter: blur(4px);
}

.btn:hover:not(:disabled) {
    transform: translateY(-2px);
    box-shadow: 0 0 15px rgba(0, 240, 255, 0.15);
    border-color: var(--accent);
    color: var(--accent);
}

.btn:active:not(:disabled) {
    transform: translateY(0);
}

.btn:disabled {
    opacity: 0.3;
    cursor: not-allowed;
    filter: grayscale(1);
}

.btn-primary {
    background: var(--accent);
    color: #000;
    border: none;
    box-shadow: 0 0 20px rgba(0, 240, 255, 0.3);
}

.btn-primary:hover:not(:disabled) {
    background: var(--accent-hover);
    color: #000;
    border: none;
    box-shadow: 0 0 30px rgba(0, 240, 255, 0.5);
}

.btn-secondary {
    background: rgba(0, 0, 0, 0.4);
    border-color: var(--border);
    color: var(--text-dim);
}

.btn-secondary:hover:not(:disabled) {
    border-color: var(--text-muted);
    color: var(--text-main);
    background: rgba(255, 255, 255, 0.05);
}

.btn-success {
    background: rgba(0, 255, 157, 0.1);
    border-color: var(--success);
    color: var(--success);
}

.btn-success:hover {
    box-shadow: 0 0 15px rgba(0, 255, 157, 0.3);
    background: var(--success);
    color: #000;
}

.btn-danger {
    background: rgba(239, 68, 68, 0.1);
    border-color: var(--danger);
    color: var(--danger);
}

.btn-danger:hover {
    box-shadow: 0 0 15px rgba(239, 68, 68, 0.3);
    background: var(--danger);
    color: #fff;
}


/* Standardized Inputs */
select.mini-select,
select.dataset-select,
select.mini-filter {
    background-color: var(--card-bg) !important;
    border: 1px solid var(--border);
    color: var(--text-main) !important;
    font-size: 0.75rem;
    height: 42px;
    padding: 0 12px;
    font-weight: 500;
    font-family: 'JetBrains Mono', monospace;
    border-radius: var(--radius-sm);
    outline: none;
    cursor: pointer;
    -webkit-appearance: none;
    -moz-appearance: none;
    appearance: none;
    transition: 0.2s;
    background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' fill='none' stroke='%2300f0ff' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    background-repeat: no-repeat;
    background-position: right 10px center;
    background-size: 11px;
    padding-right: var(--spacing-lg);
}

select option {
    background-color: #050505 !important;
    color: var(--text-main) !important;
    padding: 10px;
}

select:hover:not(:disabled) {
    border-color: var(--accent);
    background-color: rgba(255, 255, 255, 0.05) !important;
}

select:focus {
    border-color: var(--accent);
    box-shadow: 0 0 0 1px var(--accent-glow);
}

/* Custom Dashboard Button Overrides */
button#uploadBtn,
button#uploadCvOnlyBtn {
    height: 44px;
    /* Slightly taller for primary dashboard actions */
}

/* --- SYSTEM DIALOGS --- */
.system-dialog {
    background: var(--card-bg);
    border: 1px solid var(--border);
    border-radius: var(--radius-lg);
    padding: 0;
    color: var(--text-main);
    box-shadow: 0 25px 50px -12px rgba(0, 0, 0, 0.5);
    max-width: 400px;
}

.system-dialog::backdrop {
    background: rgba(0, 0, 0, 0.7);
    backdrop-filter: blur(4px);
}

.dialog-content {
    padding: 1.5rem;
}

.dialog-header {
    display: flex;
    align-items: center;
    gap: 12px;
    margin-bottom: 1rem;
    font-size: 0.7rem;
    font-weight: 900;
    text-transform: uppercase;
    letter-spacing: 0.1em;
    color: var(--accent);
}

.dialog-body {
    font-size: 0.85rem;
    line-height: 1.6;
    color: var(--text-dim);
    margin-bottom: 2rem;
}

.dialog-body b {
    color: var(--text-main);
}

.dialog-actions {
    display: flex;
    gap: 10px;
    justify-content: flex-end;
}

.dialog-actions .btn {
    width: auto;
    padding: 0 1.25rem;
}

/* --- NEW UPLOAD FILE CARD --- */
.upload-file-card {
    position: relative;
    z-index: 20;
    /* Above the hidden input */
    background: rgba(0, 0, 0, 0.6);
    border: 1px solid var(--border);
    border-radius: var(--radius-md);
    padding: 1rem;
    display: flex;
    align-items: flex-start;
    gap: 12px;
    width: 100%;
    backdrop-filter: blur(8px);
    text-align: left !important;
}

.upload-file-card.active {
    border-color: var(--accent);
    box-shadow: 0 0 15px rgba(0, 240, 255, 0.1);
}

.file-icon-box {
    width: 40px;
    height: 40px;
    background: rgba(255, 255, 255, 0.05);
    border-radius: 6px;
    display: flex;
    align-items: center;
    justify-content: center;
    color: var(--accent);
    font-size: 1.2rem;
    flex-shrink: 0;
}

.file-info-Stack {
    display: flex;
    flex-direction: column;
    gap: 4px;
    flex: 1;
    overflow: hidden;
}

.file-name {
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.75rem;
    color: var(--text-main);
    font-weight: 700;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
}

.file-meta {
    font-size: 0.65rem;
    color: var(--text-dim);
    display: flex;
    gap: 8px;
    align-items: center;
}

.file-remove-btn {
    background: transparent;
    border: none;
    color: var(--text-muted);
    cursor: pointer;
    font-size: 1rem;
    padding: 4px;
    transition: 0.2s;
    display: flex;
    align-items: center;
    justify-content: center;
}

.file-remove-btn:hover {
    color: var(--break-border);
}

.spinner {
    display: inline-block;
    width: 14px;
    height: 14px;
    border: 2px solid rgba(255, 255, 255, .2);
    border-radius: 50%;
    border-top-color: #fff;
    animation: spin 0.8s linear infinite;
}

@keyframes spin {
    to {
        transform: rotate(360deg);
    }
}
```

File 41-58: `static\css\layout.css`
```css
/* --- HEADER COMMAND CENTER --- */
header {
    height: var(--header-height);
    background-color: var(--sidebar-bg);
    display: flex;
    justify-content: flex-start;
    align-items: center;
    padding: 0 1.5rem;
    border-bottom: 1px solid var(--border);
    z-index: 100;
    flex-shrink: 0;
    gap: 2rem;
}

.project-info {
    display: flex;
    flex-direction: column;
    max-width: 400px;
    overflow: hidden;
}

#playgroundTitle {
    font-size: 0.9rem;
    color: var(--accent);
    margin: 2px 0 0 0;
    font-weight: 800;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
}

/* Global HUD Stats */
.stats-hud {
    display: flex;
    align-items: center;
    gap: 1.5rem;
    background: rgba(0, 0, 0, 0.3);
    padding: 0.5rem 1.5rem;
    border-radius: 10px;
    border: 1px solid var(--border-light);
    margin-left: auto;
    transition: opacity 0.3s ease;
}

.hud-item {
    display: flex;
    flex-direction: column;
    align-items: center;
}

.hud-val {
    font-size: 0.9rem;
    font-weight: 800;
    color: var(--text-main);
    font-family: 'JetBrains Mono', monospace;
}

.hud-lbl {
    font-size: 0.5rem;
    font-weight: 800;
    color: var(--text-muted);
    text-transform: uppercase;
    letter-spacing: 0.08em;
}

.hud-val.highlight {
    color: var(--success);
}

.hud-divider {
    width: 1px;
    height: 20px;
    background: var(--border);
    margin: 0 0.25rem;
}

/* --- MAIN WORKSPACE SPLIT --- */
main {
    display: flex;
    flex: 1;
    overflow: hidden;
}

/* Video Player Column Styling */
.video-column {
    width: 35vw;
    display: flex;
    flex-direction: column;
    background-color: #000;
    border-right: 1px solid var(--border);
}

.video-container {
    flex: 1;
    position: relative;
    display: flex;
    align-items: center;
    justify-content: center;
    padding: 1.5rem;
}

video {
    max-width: 100%;
    max-height: 100%;
    border-radius: 4px;
    box-shadow: 0 20px 50px rgba(0, 0, 0, 0.8);
}

/* Navigation Deck (Playhead Controls) */
.control-deck {
    height: var(--control-deck-height);
    background-color: var(--sidebar-bg);
    border-top: 1px solid var(--border);
    display: flex;
    justify-content: center;
    align-items: center;
    gap: 1rem;
    flex-shrink: 0;
}

.nav-btn {
    background: var(--item-bg);
    border: 1px solid var(--border-light);
    color: var(--text-main);
    width: 54px;
    height: 38px;
    border-radius: 8px;
    font-size: 1rem;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.15s ease;
    box-shadow: var(--shadow-sm);
}

.nav-btn:hover {
    background: var(--accent);
    color: #000;
    border-color: var(--accent);
    transform: scale(1.05);
}

.nav-group {
    display: flex;
    gap: 4px;
}

.deck-separator {
    width: 1px;
    height: 24px;
    background: var(--border);
    margin: 0 1.5rem;
}

/* Sidebar Framework */
.sidebar {
    width: 65vw;
    background-color: var(--sidebar-bg);
    display: flex;
    flex-direction: column;
}

/* STICKY HEADER & ACTION BAR */
.sidebar-sticky-header {
    backdrop-filter: blur(12px);
    background: rgba(17, 17, 20, 0.85);
    border-bottom: 1px solid var(--border);
    padding: 1rem 1.25rem;
    flex-shrink: 0;
    z-index: 50;
    position: sticky;
    top: 0;
    display: flex;
    flex-direction: column;
    gap: 0.8rem;
}

.sidebar-title {
    font-size: 0.65rem;
    color: var(--text-muted);
    text-transform: uppercase;
    letter-spacing: 0.2em;
    font-weight: 900;
    display: flex;
    align-items: center;
    gap: 12px;
}

.sidebar-actions {
    display: grid;
    grid-template-columns: 1fr 120px;
    gap: 6px;
    align-items: stretch;
}
```

File 42-58: `static\css\modules\playground.css`
```css
/**
 * Playground Specific Styles.
 * Aesthetic: Dark Tech / Glassmorphism.
 */

/* Status Indicators */
#saveStatus {
    font-size: 0.6rem;
    color: var(--text-dim);
    font-weight: 900;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    margin-right: 4px;
    white-space: nowrap;
}

/* Headers */
h2 {
    font-size: 0.7rem;
    text-transform: uppercase;
    letter-spacing: 0.15em;
    color: var(--text-muted);
    border-bottom: 1px solid var(--border);
    padding-bottom: var(--spacing-sm);
    margin-top: 0;
    margin-bottom: var(--spacing-md);
    font-weight: 900;
}

/* --- UPLOAD COMPONENT --- */
.upload-area {
    display: flex !important;
    flex-direction: column !important;
    align-items: center !important;
    justify-content: center !important;
    min-height: 140px !important;
    border: 2px dashed var(--border) !important;
    padding: 2rem !important;
    border-radius: var(--radius-lg) !important;
    cursor: pointer !important;
    position: relative !important;
    margin-top: 1rem !important;
    text-align: center !important;
    background: rgba(255, 255, 255, 0.01) !important;
    transition: all 0.2s cubic-bezier(0.4, 0, 0.2, 1) !important;
}

.upload-area:hover {
    border-color: var(--accent) !important;
    background: rgba(56, 189, 248, 0.03) !important;
}

#videoInput {
    position: absolute !important;
    width: 100% !important;
    height: 100% !important;
    top: 0 !important;
    left: 0 !important;
    opacity: 0 !important;
    cursor: pointer !important;
    z-index: 10 !important;
    font-size: 0 !important;
}

.upload-icon {
    font-size: 1.8rem !important;
    color: var(--text-muted) !important;
    margin-bottom: 0.5rem !important;
    pointer-events: none !important;
}

.upload-text {
    font-size: 0.8rem !important;
    color: var(--text-dim) !important;
    font-weight: 600 !important;
    pointer-events: none !important;
    line-height: 1.4 !important;
}

/* --- QUEUE LOGIC --- */
.queue-container {
    margin-top: 1.5rem !important;
    max-height: 240px !important;
    overflow-y: auto !important;
    display: flex !important;
    flex-direction: column !important;
    gap: 0.5rem !important;
    border-top: 1px solid var(--border) !important;
    padding-top: 1.25rem !important;
}

.queue-item {
    background: var(--card-bg) !important;
    border: 1px solid var(--border) !important;
    border-radius: 6px !important;
    padding: 0.75rem 1rem !important;
    display: flex !important;
    flex-direction: column !important;
    gap: 8px !important;
}

.queue-item.active {
    border-color: var(--accent) !important;
}

.queue-item.success {
    border-color: var(--success) !important;
}

.queue-item.failure {
    border-color: var(--break-border) !important;
}

.f-name {
    font-weight: 800 !important;
    font-family: 'JetBrains Mono', monospace !important;
    color: #fff !important;
}

.f-status {
    font-size: 0.6rem !important;
    text-transform: uppercase !important;
    font-weight: 900 !important;
    color: var(--text-muted) !important;
}

.mini-progress {
    width: 100% !important;
    height: 4px !important;
    background: #000 !important;
    border-radius: 2px !important;
    overflow: hidden !important;
}

.mini-bar {
    height: 100% !important;
    background: var(--accent) !important;
    width: 0%;
    transition: width 0.3s ease !important;
}


/* HIGH-CONTRAST OVERLAY */
#videoOverlay {
    position: absolute;
    top: 2.5rem;
    left: 2.5rem;
    pointer-events: none;
    z-index: 10;
}

#overlayShotId {
    background: var(--glass-bg);
    backdrop-filter: blur(8px);
    padding: 6px 12px;
    border-radius: 6px;
    color: var(--accent);
    font-weight: 800;
    font-family: monospace;
    font-size: 0.9rem;
    border: 1px solid var(--accent);
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.5);
    display: none;
}

/* MAIN SCROLLABLE GRID */
#shotGrid {
    flex: 1;
    overflow-y: auto;
    padding: 1.25rem;
    display: grid;
    grid-template-columns: repeat(5, 1fr);
    gap: 1rem;
    align-content: start;
}

#shotGrid::-webkit-scrollbar {
    width: 6px;
}

#shotGrid::-webkit-scrollbar-track {
    background: var(--sidebar-bg);
}

#shotGrid::-webkit-scrollbar-thumb {
    background: #475569;
    border-radius: 3px;
}

/* --- REFINED SCENE UNIT HEADERS --- */
.scene-header-card {
    grid-column: 1 / -1;
    background: linear-gradient(90deg, rgba(16, 185, 129, 0.08) 0%, rgba(24, 24, 27, 0.4) 100%);
    backdrop-filter: blur(8px);
    border-left: 3px solid var(--success);
    border-top: 1px solid var(--border-light);
    border-right: 1px solid var(--border-light);
    border-bottom: 1px solid var(--border-light);
    padding: 0.75rem 1.25rem;
    border-radius: 8px;
    margin-top: 2rem;
    margin-bottom: 0.5rem;
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
    transition: all 0.2s ease;
}

.scene-header-card:hover {
    background: linear-gradient(90deg, rgba(16, 185, 129, 0.12) 0%, rgba(24, 24, 27, 0.6) 100%);
    border-color: rgba(16, 185, 129, 0.3);
}

.scene-title {
    display: flex;
    justify-content: space-between;
    align-items: center;
    width: 100%;
}

.scene-title select.mini-select {
    flex: 0 1 240px;
    height: 26px;
    background-color: #09090b;
    border-color: var(--border);
    font-weight: 800;
}

.scene-info {
    display: flex;
    align-items: center;
    gap: 1rem;
}

.scene-index {
    font-size: 0.9rem;
    font-weight: 900;
    color: var(--success);
    text-transform: uppercase;
    letter-spacing: 0.05em;
    font-family: 'JetBrains Mono', monospace;
}

.scene-meta {
    font-size: 0.65rem;
    color: var(--text-dim);
    font-weight: 700;
    padding-left: 1rem;
    border-left: 1px solid var(--border);
    font-family: 'JetBrains Mono', monospace;
}

.reasoning-toggle-btn {
    font-size: 0.55rem;
    font-weight: 800;
    color: var(--text-muted);
    background: rgba(255, 255, 255, 0.03);
    border: 1px solid var(--border);
    padding: 4px 10px;
    border-radius: 4px;
    cursor: pointer;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    transition: all 0.15s ease;
}

.reasoning-toggle-btn:hover {
    color: var(--accent);
    border-color: var(--accent);
    background: rgba(56, 189, 248, 0.05);
}

.scene-toolbar {
    display: flex;
    gap: 8px;
    margin-top: 12px;
    padding-top: 10px;
    border-top: 1px solid rgba(255, 255, 255, 0.05);
    justify-content: flex-end;
}

.scene-toolbar-btn {
    background: none;
    border: 1px solid var(--border);
    color: var(--text-muted);
    width: 28px;
    height: 28px;
    border-radius: 4px;
    display: flex;
    align-items: center;
    justify-content: center;
    cursor: pointer;
    transition: all 0.15s ease;
    font-size: 0.75rem;
}

.scene-toolbar-btn:hover {
    color: var(--accent);
    border-color: var(--accent);
    background: rgba(56, 189, 248, 0.1);
}

.reasoning-box {
    display: none;
    font-size: 0.65rem;
    color: var(--text-dim);
    background: rgba(0, 0, 0, 0.4);
    padding: 1rem;
    border-radius: 6px;
    margin-top: 0.5rem;
    line-height: 1.5;
    overflow-y: auto;
    max-height: 140px;
    padding-right: 6px;
    border-left: none;
}

.reasoning-box.open {
    display: block;
}

.reasoning-box.scene-level {
    border-left: 2px solid var(--success);
    color: var(--text-main);
    font-size: 0.7rem;
    border-radius: 0;
    padding-left: 1rem;
    background: rgba(0, 0, 0, 0.2);
}

.reasoning-box::-webkit-scrollbar {
    width: 3px;
}

.reasoning-box::-webkit-scrollbar-thumb {
    background: var(--border);
    border-radius: 2px;
}

/* --- SHOT CARDS --- */
.shot-card {
    background: var(--card-bg);
    border-radius: var(--radius-md);
    overflow: hidden;
    border: 1px solid var(--border);
    transition: all 0.2s cubic-bezier(0.4, 0, 0.2, 1);
    cursor: pointer;
    display: flex;
    flex-direction: column;
    min-height: 220px;
    backdrop-filter: blur(4px);
    position: relative;
    box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
}

.shot-card::before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    width: 2px;
    height: 100%;
    background: var(--border);
    transition: 0.2s;
    z-index: 5;
}

.shot-card.is-dirty {
    border-left: none;
}

.shot-card.is-dirty::before {
    background: var(--warning);
    box-shadow: 0 0 10px var(--warning);
}

.shot-card:hover {
    border-color: var(--accent);
    transform: translateY(-4px);
    box-shadow: var(--shadow-lg), 0 0 15px rgba(0, 240, 255, 0.1);
    background: rgba(255, 255, 255, 0.03);
}

.shot-card:hover::before {
    background: var(--accent);
    box-shadow: 0 0 8px var(--accent);
}

.shot-card.active {
    border-color: var(--accent);
    box-shadow: 0 0 0 1px var(--accent), 0 0 30px var(--accent-glow);
    z-index: 10;
}

.shot-card.is-break {
    border-color: var(--break-border);
}

.card-header {
    padding: 0.5rem 0.8rem;
    background: rgba(0, 0, 0, 0.6);
    display: flex;
    justify-content: space-between;
    align-items: center;
    font-size: 0.65rem;
    font-family: 'JetBrains Mono', monospace;
    font-weight: 700;
    color: var(--text-dim);
    border-bottom: 1px solid var(--border-light);
}

.thumb-container {
    width: 100%;
    aspect-ratio: 16/9;
    background: #000;
    position: relative;
    display: flex;
    justify-content: center;
    align-items: center;
    overflow: hidden;
    border-bottom: 1px solid var(--border-light);
}

.thumb-container img {
    width: 100%;
    height: 100%;
    object-fit: contain;
    z-index: 2;
    opacity: 0.8;
    transition: 0.2s;
}

.shot-card:hover .thumb-container img {
    opacity: 1;
}

.overlay-toggle-btn {
    position: absolute;
    bottom: 8px;
    right: 8px;
    z-index: 20;
    background: rgba(0, 0, 0, 0.8);
    backdrop-filter: blur(4px);
    border: 1px solid var(--accent);
    color: var(--accent);
    width: 24px;
    height: 24px;
    border-radius: 4px;
    cursor: pointer;
    transition: all 0.2s;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 0.7rem;
    box-shadow: 0 0 10px rgba(0, 240, 255, 0.2);
}

.overlay-toggle-btn:hover {
    background: var(--accent);
    color: #000;
    box-shadow: 0 0 15px var(--accent-glow);
}

.control-row {
    padding: 6px 10px;
    background: rgba(0, 0, 0, 0.4);
    border-bottom: 1px solid var(--border-light);
    display: flex;
    flex-direction: row;
    align-items: center;
    gap: 8px;
    height: 36px;
}

.control-row select.mini-select {
    flex: 1;
    height: 24px;
    background-color: rgba(255, 255, 255, 0.03);
    border-color: var(--border);
    font-size: 0.65rem;
    border-radius: 2px;
}

.toggle-input {
    width: 14px;
    height: 14px;
    cursor: pointer;
    accent-color: var(--break-border);
    margin: 0;
}

.card-body {
    padding: 0.8rem;
    flex: 1;
    display: flex;
    flex-direction: column;
    gap: 0.8rem;
    background: linear-gradient(180deg, rgba(255, 255, 255, 0.01) 0%, rgba(0, 0, 0, 0.2) 100%);
}

.meta-row {
    display: flex;
    flex-direction: column;
    gap: 2px;
}

.meta-label {
    font-size: 0.5rem;
    font-weight: 800;
    color: var(--accent-dim);
    text-transform: uppercase;
    letter-spacing: 0.1em;
    display: block;
}

.meta-value {
    font-size: 0.7rem;
    color: var(--text-main);
    line-height: 1.3;
    font-weight: 600;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
    text-shadow: 0 0 5px rgba(0, 0, 0, 0.5);
}

.scroll-box {
    max-height: 60px;
    overflow-y: auto;
    font-size: 0.65rem;
    color: var(--text-dim);
    background: rgba(0, 0, 0, 0.4);
    padding: 6px 8px;
    border-radius: 4px;
    line-height: 1.5;
    border: 1px solid var(--border-light);
    font-family: 'JetBrains Mono', monospace;
}

.scroll-box::-webkit-scrollbar {
    width: 2px;
}

.scroll-box::-webkit-scrollbar-thumb {
    background: var(--accent-dim);
}

.metadata-view,
.reasoning-view {
    display: flex;
    flex-direction: column;
    gap: 8px;
    height: 100%;
}

.reasoning-view {
    display: none;
}

.shot-card.view-reasoning .metadata-view {
    display: none;
}

.shot-card.view-reasoning .reasoning-view {
    display: flex;
}

/* --- VIEW STATE MANAGEMENT --- */
#mainContainer {
    position: relative;
    width: 100%;
    height: 100%;
    overflow: hidden;
    background-color: transparent;
}

.view-state {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    padding: 24px;
    display: none;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    opacity: 0;
    transition: opacity 0.4s cubic-bezier(0.4, 0, 0.2, 1);
}

.view-state.active {
    display: flex;
    opacity: 1;
    z-index: 10;
}

.view-state.results-layout {
    flex-direction: row;
    align-items: stretch;
    justify-content: flex-start;
    padding: 0;
}

/* --- STATE 1: MISSION CONTROL LAYOUT --- */
.mission-control-layout {
    display: grid;
    grid-template-columns: 2fr 2fr 1fr;
    gap: 20px;
    width: 100%;
    height: 100%;
    min-height: 0;
}

.mc-column {
    display: flex;
    flex-direction: column;
    gap: 20px;
    min-height: 0;
}

.mc-panel {
    background: var(--card-bg);
    border: var(--glass-border);
    backdrop-filter: var(--glass-backdrop);
    border-radius: var(--radius-lg);
    box-shadow: var(--shadow-md);
    display: flex;
    flex-direction: column;
    overflow: hidden;
    position: relative;
}

.mc-panel::before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 2px;
    background: linear-gradient(90deg, transparent, var(--accent), transparent);
    opacity: 0.3;
}

.panel-header {
    background: rgba(0, 0, 0, 0.2);
    padding: 1rem 1.25rem;
    border-bottom: 1px solid var(--border);
    font-size: 0.65rem;
    font-weight: 800;
    color: var(--text-muted);
    text-transform: uppercase;
    letter-spacing: 0.15em;
    display: flex;
    align-items: center;
    gap: 8px;
    flex-shrink: 0;
}

.panel-header i {
    color: var(--accent);
    opacity: 0.8;
}

.asset-hub {
    flex-shrink: 0;
}

.asset-actions {
    padding: 1rem;
    display: flex;
    gap: 8px;
    border-bottom: 1px solid var(--border-light);
}

.asset-stats {
    padding: 0.8rem 1.25rem;
    display: flex;
    justify-content: space-between;
    font-size: 0.6rem;
    color: var(--text-dim);
    font-family: 'JetBrains Mono', monospace;
}

.asset-stats .val {
    color: var(--text-main);
    font-weight: 700;
}

.archive-hub {
    flex: 1;
    min-height: 0;
}

.archive-list-container {
    flex: 1;
    overflow-y: auto;
    padding: 1rem;
    display: flex;
    flex-direction: column;
    gap: 8px;
    background: rgba(0, 0, 0, 0.1);
}

.mc-canvas {
    gap: 0;
    background: rgba(255, 255, 255, 0.01);
    border-radius: var(--radius-lg);
    border: 1px solid var(--border);
    padding: 2rem;
    overflow-y: auto;
    justify-content: flex-start;
}

.mc-step {
    margin-bottom: 2rem;
    padding-bottom: 2rem;
    border-bottom: 1px dashed var(--border);
    animation: fadeIn 0.4s ease-out;
}

.mc-step.no-border {
    border-bottom: none;
    margin-bottom: 0;
    padding-bottom: 0;
}

.step-label {
    font-size: 0.6rem;
    font-weight: 900;
    color: var(--accent-dim);
    margin-bottom: 1rem;
    letter-spacing: 0.2em;
    text-transform: uppercase;
}

.step-content {
    padding-left: 0.5rem;
}

.parameter-grid {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 1.5rem;
}

.param-group {
    background: rgba(0, 0, 0, 0.2);
    border: 1px solid var(--border);
    border-radius: var(--radius-md);
    padding: 1rem;
    display: flex;
    flex-direction: column;
    gap: 12px;
}

.param-group.full-width {
    grid-column: 1 / -1;
}

.group-title {
    font-size: 0.55rem;
    color: var(--text-muted);
    text-transform: uppercase;
    font-weight: 700;
    letter-spacing: 0.05em;
    padding-bottom: 8px;
    border-bottom: 1px solid var(--border-light);
    margin-bottom: 4px;
}

.p-row {
    display: flex;
    justify-content: space-between;
    align-items: center;
    gap: 1rem;
}

.p-label {
    font-size: 0.65rem;
    color: var(--text-dim);
    font-weight: 600;
}

.p-input {
    background: transparent;
    border: 1px solid var(--border);
    color: var(--accent);
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.75rem;
    padding: 4px 8px;
    border-radius: 4px;
    width: 80px;
    text-align: right;
    transition: 0.2s;
}

.p-input:focus {
    border-color: var(--accent);
    outline: none;
    background: rgba(0, 0, 0, 0.3);
}

.p-grid-2 {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 1rem;
}

.launch-btn {
    width: 100%;
    height: 64px;
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    gap: 4px;
    border-radius: var(--radius-md);
    background: var(--accent);
    color: #000;
    border: none;
    box-shadow: 0 0 30px rgba(0, 240, 255, 0.2);
    transition: all 0.2s;
}

.launch-btn:hover:not(:disabled) {
    transform: translateY(-2px);
    box-shadow: 0 0 50px rgba(0, 240, 255, 0.4);
    background: #fff;
}

.launch-btn:disabled {
    background: #1a1a1c;
    color: #444;
    cursor: not-allowed;
    box-shadow: none;
}

.launch-text {
    font-size: 0.9rem;
    font-weight: 900;
    letter-spacing: 0.1em;
}

.launch-sub {
    font-size: 0.6rem;
    font-weight: 600;
    opacity: 0.7;
    text-transform: uppercase;
}

.inspector-panel {
    flex: 1;
}

.inspector-content {
    padding: 1.5rem;
    font-size: 0.7rem;
    line-height: 1.6;
    color: var(--text-dim);
}

.placeholder-state {
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    height: 100%;
    opacity: 0.3;
    text-align: center;
    gap: 1rem;
    margin-top: 4rem;
}

.placeholder-state i {
    font-size: 2rem;
}

.info-tile {
    margin-bottom: 1.5rem;
    animation: fadeIn 0.2s ease;
}

.info-label {
    font-size: 0.55rem;
    color: var(--text-muted);
    text-transform: uppercase;
    font-weight: 800;
    margin-bottom: 4px;
    display: block;
}

.info-val {
    font-size: 0.8rem;
    color: var(--text-main);
    font-weight: 600;
    font-family: 'JetBrains Mono', monospace;
    word-break: break-all;
}

.info-desc {
    font-size: 0.7rem;
    color: var(--text-dim);
    margin-top: 4px;
    padding-top: 8px;
    border-top: 1px solid var(--border-light);
}

.help-panel {
    height: 180px;
    flex-shrink: 0;
}

.help-content {
    padding: 1rem;
    display: flex;
    flex-direction: column;
    gap: 8px;
}

.help-item {
    display: flex;
    justify-content: space-between;
    font-size: 0.65rem;
    border-bottom: 1px solid var(--border-light);
    padding-bottom: 4px;
}

.help-item .key {
    color: var(--text-muted);
}

.help-item .val {
    color: var(--accent);
    font-weight: 700;
}

.session-item {
    background: rgba(255, 255, 255, 0.02);
    border: 1px solid var(--border);
    padding: 0.8rem;
    border-radius: var(--radius-sm);
    cursor: pointer;
    transition: all 0.15s ease;
    display: flex;
    flex-direction: column;
    gap: 6px;
    position: relative;
    overflow: hidden;
}

.session-item::after {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    width: 2px;
    height: 100%;
    background: var(--accent);
    opacity: 0;
    transition: 0.2s;
}

.session-item:hover {
    border-color: var(--accent);
    background: rgba(255, 255, 255, 0.04);
}

.session-item:hover::after {
    opacity: 1;
}

.s-top {
    display: flex;
    justify-content: space-between;
    align-items: flex-start;
}

.s-name {
    font-weight: 700;
    font-size: 0.75rem;
    color: var(--text-main);
    font-family: 'JetBrains Mono', monospace;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
    max-width: 160px;
}

.s-date {
    font-size: 0.6rem;
    color: var(--text-muted);
}

.s-meta {
    display: flex;
    flex-wrap: wrap;
    gap: 4px;
}

.s-tag {
    font-size: 0.55rem;
    padding: 2px 6px;
    border-radius: 2px;
    background: rgba(0, 0, 0, 0.3);
    color: var(--text-dim);
    border: 1px solid var(--border-light);
}

/* --- STATE 2: TERMINAL SIMULATION --- */
/* --- STATE 2: TERMINAL SIMULATION --- */
.terminal-card {
    /* Width handled by grid */
    background: #050505;
    border: 1px solid var(--border);
    border-radius: var(--radius-md);
    box-shadow: 0 40px 100px rgba(0, 0, 0, 0.9), 0 0 0 1px rgba(255, 255, 255, 0.05);
    display: flex;
    flex-direction: column;
    overflow: hidden;
}

.terminal-header {
    background: #111;
    padding: 0.8rem 1.2rem;
    display: flex;
    align-items: center;
    justify-content: center;
    position: relative;
    border-bottom: 1px solid var(--border);
}

.term-dots {
    position: absolute;
    left: 1rem;
    display: flex;
    gap: 8px;
}

.term-dots span {
    width: 10px;
    height: 10px;
    border-radius: 50%;
    opacity: 0.6;
}

.term-dots span:nth-child(1) {
    background: #ff5f56;
}

.term-dots span:nth-child(2) {
    background: #ffbd2e;
}

.term-dots span:nth-child(3) {
    background: #27c93f;
}

.term-title {
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.75rem;
    color: var(--text-dim);
    letter-spacing: 0.05em;
}

.terminal-body {
    padding: 2rem;
    overflow-y: auto;
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.85rem;
    color: #a1a1aa;
    display: flex;
    flex-direction: column;
    gap: 8px;
    background: linear-gradient(180deg, rgba(5, 5, 5, 1) 0%, rgba(10, 10, 10, 1) 100%);
    height: 400px;
}

.ts {
    color: var(--accent-dim);
    margin-right: 12px;
    opacity: 0.7;
}

.progress-container {
    height: 3px;
    background: #000;
    width: 100%;
}

.progress-bar {
    height: 100%;
    background: var(--accent);
    width: 0%;
    transition: width 0.3s ease-out;
    box-shadow: 0 0 10px var(--accent);
}

.abort-container {
    width: 100%;
    max-width: 1600px;
    /* Match monitor deck */
    display: flex;
    justify-content: flex-end;
    margin: 0 auto 1rem auto;
    padding: 0 24px;
}

#abortBtn {
    font-size: 0.65rem;
    height: 32px;
    padding: 0 16px;
    background: rgba(239, 68, 68, 0.1);
    color: var(--danger);
    border: 1px solid var(--danger);
    transition: all 0.2s ease;
}

#abortBtn:hover:not(:disabled) {
    background: var(--danger);
    color: #fff;
    box-shadow: 0 0 15px rgba(239, 68, 68, 0.4);
}

#abortBtn:disabled {
    opacity: 0.5;
    cursor: not-allowed;
    filter: grayscale(1);
}

.read-only-grid .toggle-input {
    display: none !important;
}

.read-only-grid .mini-select {
    pointer-events: none;
    background: transparent !important;
    border: none !important;
    padding: 0 !important;
    background-image: none !important;
    color: var(--accent) !important;
    font-weight: 700 !important;
    text-transform: uppercase;
    font-size: 0.65rem !important;
    text-align: right;
}

.read-only-grid .overlay-toggle-btn {
    display: none !important;
}

.read-only-grid .shot-card:hover {
    transform: none !important;
    box-shadow: none !important;
    border-color: var(--border) !important;
}

.read-only-grid .shot-card.active {
    border-color: var(--accent) !important;
    box-shadow: 0 0 0 1px var(--accent), 0 0 20px var(--accent-glow) !important;
    z-index: 5;
}

.read-only-grid {
    padding: 2rem;
    gap: 2rem;
}

.debug-panel {
    position: absolute;
    top: 110px;
    left: 0;
    right: 0;
    bottom: 0;
    background: var(--bg-color);
    z-index: 100;
    display: none;
    flex-direction: column;
    border-top: 1px solid var(--border);
    animation: slideUp 0.3s cubic-bezier(0.4, 0, 0.2, 1);
}

.debug-panel.active {
    display: flex;
}

.debug-tabs {
    display: flex;
    background: rgba(0, 0, 0, 0.3);
    border-bottom: 1px solid var(--border);
    padding: 0 1rem;
}

.debug-tab {
    padding: 0.75rem 1.25rem;
    font-size: 0.65rem;
    font-weight: 800;
    text-transform: uppercase;
    color: var(--text-muted);
    cursor: pointer;
    border-bottom: 2px solid transparent;
    transition: all 0.2s;
}

.debug-tab.active {
    color: var(--accent);
    border-bottom-color: var(--accent);
    background: var(--accent-glow);
}

.debug-content {
    flex: 1;
    overflow-y: auto;
    padding: 1.5rem;
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.75rem;
    background: var(--bg-color);
}

.pipeline-log-text {
    white-space: pre-wrap;
    line-height: 1.6;
    color: #a1a1aa;
}

.pipeline-log-text .ts {
    color: var(--accent-dim);
    margin-right: 8px;
}

.debug-entry {
    border: 1px solid var(--border);
    border-radius: 6px;
    margin-bottom: 1.5rem;
    background: rgba(255, 255, 255, 0.02);
    overflow: hidden;
    display: flex;
    flex-direction: column;
}

.entry-header {
    background: rgba(0, 0, 0, 0.4);
    padding: 0.5rem 1rem;
    border-bottom: 1px solid var(--border);
    display: flex;
    justify-content: space-between;
    font-size: 0.6rem;
    font-weight: 800;
    color: var(--text-dim);
}

.micro-tabs {
    display: flex;
    border-bottom: 1px solid var(--border-light);
    background: rgba(0, 0, 0, 0.2);
}

.micro-tab {
    padding: 8px 16px;
    font-size: 0.6rem;
    font-weight: 700;
    color: var(--text-muted);
    cursor: pointer;
    border-right: 1px solid var(--border-light);
}

.micro-tab.active {
    color: var(--accent);
    background: var(--accent-glow);
}

.micro-pane {
    display: none;
    padding: 1rem;
    background: #000;
    color: var(--text-dim);
    white-space: pre-wrap;
    line-height: 1.5;
    min-height: 120px;
}

.micro-pane.active {
    display: block;
}

@keyframes slideUp {
    from {
        transform: translateY(20px);
        opacity: 0;
    }

    to {
        transform: translateY(0);
        opacity: 1;
    }
}

@keyframes fadeIn {
    from {
        opacity: 0;
    }

    to {
        opacity: 1;
    }
}

/* =========================================
   MONITORING DECK (Hardware Stats)
   ========================================= */
.monitor-deck {
    display: grid;
    grid-template-columns: 280px 1fr;
    /* 2 Columns now */
    gap: 32px;
    width: 100%;
    max-width: 1400px;
    height: 100%;
    overflow: hidden;
    margin: 0 auto;
    padding: 0 24px;
    align-items: center;
    /* Center Vertically */
}

.monitor-flank.left {
    display: flex;
    flex-direction: column;
    gap: 16px;
    padding: 24px 0;
    max-height: 100%;
    overflow-y: auto;
}

/* Scrollbar for left flank if it gets too tall */
.monitor-flank.left::-webkit-scrollbar {
    width: 0px;
    background: transparent;
}

.terminal-wrapper {
    display: flex;
    flex-direction: column;
    gap: 12px;
    height: 100%;
    justify-content: center;
}

.abort-container {
    width: 100%;
    display: flex;
    justify-content: flex-end;
    margin: 0;
    padding: 0;
}

.terminal-card {
    background: #050505;
    border: 1px solid var(--border);
    border-radius: var(--radius-md);
    box-shadow: 0 40px 100px rgba(0, 0, 0, 0.9), 0 0 0 1px rgba(255, 255, 255, 0.05);
    display: flex;
    flex-direction: column;
    overflow: hidden;
    position: relative;
    height: 500px;
    /* Fixed optimal height */
    max-height: 80vh;
}

.monitor-widget {
    background: rgba(0, 0, 0, 0.4);
    border: 1px solid var(--border);
    border-radius: 6px;
    padding: 12px;
    display: flex;
    flex-direction: column;
    gap: 8px;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);
}

.monitor-widget.primary {
    border-color: var(--accent-dim);
    background: rgba(0, 255, 157, 0.03);
}

.monitor-label {
    font-size: 0.65rem;
    font-weight: 600;
    color: var(--text-muted);
    letter-spacing: 0.05em;
    text-transform: uppercase;
    display: flex;
    align-items: center;
    gap: 6px;
}

.monitor-main {
    display: flex;
    flex-direction: column;
    gap: 4px;
}

.monitor-val {
    font-family: 'JetBrains Mono', monospace;
    font-size: 1.2rem;
    font-weight: 700;
    color: var(--text-main);
}

.monitor-bar-track {
    height: 4px;
    background: rgba(255, 255, 255, 0.1);
    border-radius: 2px;
    overflow: hidden;
    margin-top: 4px;
}

.monitor-bar-fill {
    height: 100%;
    background: var(--accent);
    width: 0%;
    transition: width 0.5s ease;
}

.monitor-detail {
    font-size: 0.6rem;
    color: var(--text-dim);
    font-family: 'JetBrains Mono', monospace;
    text-align: right;
}

/* Ensure progress bar is tight to bottom */
.progress-container {
    height: 4px;
    background: #000;
    width: 100%;
    margin-top: auto;
    /* Push to bottom */
}

/* Responsive adjustments */
@media (max-width: 1200px) {
    .monitor-deck {
        grid-template-columns: 240px 1fr;
    }
}
```

File 43-58: `static\css\variables.css`
```css
/**
 * Dark Tech 2026 Design System.
 * Aesthetic: Void Black, Neon Cyan, Glassmorphism, Precision Engineering.
 */
:root {
    /* --- COLOR PALETTE (Void & Neon) --- */
    --bg-color: #030303;
    /* Deepest Void */
    --bg-grid: #08080a;
    /* Subtle Grid Background */

    --sidebar-bg: rgba(10, 10, 12, 0.6);
    /* Glassy Dark */
    --card-bg: rgba(18, 18, 20, 0.4);
    /* High Transparency Glass */

    --item-bg: rgba(30, 30, 35, 0.3);
    --item-hover: rgba(255, 255, 255, 0.03);

    /* Semantic Accents (Neon Cyan & Electric Blue) */
    --accent: #00f0ff;
    /* Cyberpunk Cyan */
    --accent-dim: #007a82;
    --accent-glow: rgba(0, 240, 255, 0.15);
    --accent-hover: #66f5ff;

    --purple-accent: #bc13fe;
    /* Secondary Neon */

    /* Functional Colors */
    --success: #00ff9d;
    /* Neon Green */
    --warning: #ffb800;
    /* Tech Amber */
    --danger: #ff003c;
    /* Sci-Fi Red */
    --break-border: #ff0055;

    /* Borders & Glass */
    --border: rgba(255, 255, 255, 0.08);
    --border-light: rgba(255, 255, 255, 0.03);
    --border-active: var(--accent);

    --glass-bg: rgba(10, 10, 12, 0.7);
    --glass-border: 1px solid rgba(255, 255, 255, 0.08);
    --glass-backdrop: blur(12px);

    /* Text Stack */
    --text-main: #ffffff;
    --text-dim: #888890;
    --text-muted: #555560;
    --text-on-accent: #000000;

    /* --- SPACING SYSTEM --- */
    --spacing-xs: 4px;
    --spacing-sm: 8px;
    --spacing-md: 16px;
    --spacing-lg: 24px;
    --spacing-xl: 32px;

    /* --- RADIUS (Slightly Sharper) --- */
    --radius-lg: 16px;
    --radius-md: 8px;
    --radius-sm: 4px;

    /* --- LAYOUT CONSTANTS --- */
    --header-height: 80px;
    --sidebar-width: 380px;
    --control-deck-height: 80px;

    /* --- EFFECTS --- */
    --shadow-sm: 0 4px 12px rgba(0, 0, 0, 0.5);
    --shadow-md: 0 8px 24px rgba(0, 0, 0, 0.6);
    --shadow-lg: 0 16px 40px rgba(0, 0, 0, 0.8);
    --shadow-glow: 0 0 20px rgba(0, 240, 255, 0.1);
}
```

File 44-58: `static\js\lib\api_client.js`
```js
/**
 * ApiClient.js
 * Centralized wrapper for HTTP requests.
 * Handles headers, error parsing, and common configuration.
 */

class ApiClient {
    constructor(baseUrl = '') {
        this.baseUrl = baseUrl;
    }

    /**
     * Generic fetch wrapper.
     * @param {string} endpoint - The API endpoint.
     * @param {object} options - Fetch options.
     * @returns {Promise<any>} - The JSON response.
     */
    async request(endpoint, options = {}) {
        const url = `${this.baseUrl}${endpoint}`;

        const defaultHeaders = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        };

        const config = {
            ...options,
            headers: {
                ...defaultHeaders,
                ...options.headers
            }
        };

        // Don't set Content-Type for FormData (let browser handle boundary automatically)
        if (options.body instanceof FormData) {
            delete config.headers['Content-Type'];
        }

        try {
            const response = await fetch(url, config);

            if (!response.ok) {
                // Attempt to parse error message from JSON body, fallback to status text
                const errorBody = await response.json().catch(() => ({}));
                const errorMessage = errorBody.error || errorBody.message || response.statusText;
                throw new Error(errorMessage);
            }

            // Return empty object for 204 No Content, otherwise parse JSON
            if (response.status === 204) return {};
            return await response.json();

        } catch (error) {
            console.error(`[API Client] Request failed for ${endpoint}:`, error);
            throw error;
        }
    }

    get(endpoint, headers = {}) {
        return this.request(endpoint, { method: 'GET', headers });
    }

    post(endpoint, body, headers = {}) {
        const isFormData = body instanceof FormData;
        return this.request(endpoint, {
            method: 'POST',
            headers,
            body: isFormData ? body : JSON.stringify(body)
        });
    }

    put(endpoint, body, headers = {}) {
        return this.request(endpoint, {
            method: 'PUT',
            headers,
            body: JSON.stringify(body)
        });
    }

    delete(endpoint, headers = {}) {
        return this.request(endpoint, { method: 'DELETE', headers });
    }
}

export const api = new ApiClient();
```

File 45-58: `static\js\modules\playground\forensic_terminal.js`
```js
/**
 * Playground Debug Console Module.
 * Handles top-level switching between Pipeline and Interaction logs,
 * and manages micro-tabs within each interaction card for deep inspection.
 */

/**
 * Renders the debug content into the provided container based on active tab.
 * @param {HTMLElement} container - The content div of the debug panel.
 * @param {Object} logs - { pipeline: string, debug: array }
 * @param {string} activeTab - 'pipeline' or 'logs'
 */
export function renderDebugPanelContent(container, logs, activeTab = 'pipeline') {
    if (!container) return;
    container.innerHTML = '';

    if (activeTab === 'pipeline') {
        const pre = document.createElement('div');
        pre.className = 'pipeline-log-text';
        pre.innerHTML = formatPipelineLog(logs.pipeline);
        container.appendChild(pre);
    }
    else {
        const wrapper = document.createElement('div');
        if (!logs.debug || logs.debug.length === 0) {
            wrapper.innerHTML = '<div class="sys-label" style="text-align:center; padding:3rem;">No interaction data captured yet.</div>';
        } else {
            logs.debug.forEach((entry, idx) => {
                wrapper.appendChild(createDebugEntry(entry, idx));
            });
        }
        container.appendChild(wrapper);
    }
}

/**
 * Formats pipeline log text with timestamp highlighting.
 */
function formatPipelineLog(text) {
    if (!text) return '<div class="sys-label">No pipeline trace found.</div>';
    // Highlight timestamps in the log text
    return text.replace(/\[(\d{2}:\d{2}:\d{2})\]/g, '<span class="ts">[$1]</span>');
}

/**
 * Creates a visual card for a single VLM interaction entry with integrated micro-tabs.
 * Tabs: Prompt (Input), Thinking (Process), Response (Output).
 */
function createDebugEntry(entry, index) {
    const card = document.createElement('div');
    card.className = 'debug-entry';

    const time = entry.timestamp ? new Date(entry.timestamp).toLocaleTimeString() : 'Unknown Time';

    // Metadata Header & Interaction Index
    card.innerHTML = `
        <div class="entry-header">
            <span>INTERACTION #${index + 1}</span>
            <span>${time}</span>
        </div>
        <div class="entry-body">
            <!-- Metadata Bar -->
            <div style="padding: 0.75rem 1rem; border-bottom: 1px solid var(--border-light); font-size: 0.6rem; color: var(--text-dim); display: flex; gap: 1.5rem;">
                <div>ID: <span class="json-string">"${entry.model}"</span></div>
                <div>TIME: <span class="json-num">${entry.usage?.inference_time || 0}s</span></div>
                <div>TOKENS: <span class="json-num">${entry.usage?.input_tokens || 0}</span> in / <span class="json-num">${entry.usage?.output_tokens || 0}</span> out</div>
            </div>

            <!-- Micro Tabs Navigation -->
            <div class="micro-tabs">
                <div class="micro-tab" data-target="prompt">Prompt</div>
                <div class="micro-tab" data-target="thinking">Thinking</div>
                <div class="micro-tab active" data-target="response">Response</div>
            </div>

            <!-- Micro Panes (Content) -->
            <div class="micro-content">
                <div class="micro-pane" data-pane="prompt">${entry.prompt_text || "No prompt data available."}</div>
                <div class="micro-pane" data-pane="thinking">${entry.thinking || "No thinking logs emitted for this batch."}</div>
                <div class="micro-pane active" data-pane="response">${entry.response_text || "[Empty Response]"}</div>
            </div>
        </div>
    `;

    // Logic: Localized Micro-Tab Switching
    const tabs = card.querySelectorAll('.micro-tab');
    const panes = card.querySelectorAll('.micro-pane');

    tabs.forEach(tab => {
        tab.onclick = () => {
            const target = tab.dataset.target;

            // Sync Tab styles
            tabs.forEach(t => t.classList.toggle('active', t === tab));

            // Sync Pane visibility
            panes.forEach(p => {
                const isTarget = p.dataset.pane === target;
                p.classList.toggle('active', isTarget);
            });
        };
    });

    return card;
}
```

File 46-58: `static\js\modules\playground\inference_timeline.js`
```js
/**
 * InferenceTimeline.js
 * Playground Timeline Module.
 * Responsible for rendering the forensic shot grid and grouping results into
 * narrative scene blocks based on VLM output.
 */

import { store } from './store.js';

const getGridContainer = () => document.getElementById('shotGrid');

/**
 * Main entry point for rendering the narrative results.
 * Groups technical shots into logical scenes and injects them into the DOM.
 */
export function renderTimeline(sessionId, onShotClick) {
    const gridContainer = getGridContainer();
    if (!gridContainer) return;
    gridContainer.innerHTML = '';

    let sceneBuffer = [];
    let sceneCount = 1;

    store.inferenceResults.forEach((shot, idx) => {
        // Narrative boundary logic: if shot is marked as break, start a new scene block
        if (shot.is_scene_break && idx !== 0) {
            injectSceneBlock(gridContainer, sceneBuffer, sceneCount++, sessionId, onShotClick);
            sceneBuffer = [];
        }
        sceneBuffer.push(shot);
    });

    // Flush the final scene block
    if (sceneBuffer.length > 0) {
        injectSceneBlock(gridContainer, sceneBuffer, sceneCount, sessionId, onShotClick);
    }
}

/**
 * Internal helper to create a scene container with a header and its constituent shot cards.
 */
function injectSceneBlock(container, shots, index, sessionId, onShotClick) {
    if (!shots.length) return;

    const head = shots[0];
    const tail = shots[shots.length - 1];
    const duration = tail.end_time - head.start_time;

    // Determine unity logic label from the head shot (defaulting to a generic unity tag)
    const unityLabel = (head.scene_logic && head.scene_logic.case_type)
        ? head.scene_logic.case_type.replace(/_/g, ' ')
        : 'NARRATIVE UNITY';

    // 1. Construct Scene Header
    const header = document.createElement('div');
    header.className = 'scene-header-card';
    header.innerHTML = `
        <div class="scene-title">
            <div class="scene-info">
                <span class="scene-index">Scene ${index}</span>
                <span class="scene-meta">${shots.length} shots | ${formatTime(duration)}</span>
            </div>
            <div class="sys-label" style="color: var(--success); font-weight: 900; font-size: 0.55rem;">
                <i class="fa-solid fa-link" style="margin-right: 4px;"></i> ${unityLabel}
            </div>
        </div>
    `;
    container.appendChild(header);

    // 2. Construct Shot Cards
    shots.forEach(shot => {
        const card = document.createElement('div');
        card.className = `shot-card ${shot.is_scene_break ? 'is-break' : ''}`;
        card.id = `card-${shot.shot_id}`;
        card.dataset.time = shot.start_time;

        // Resolve thumbnail URL from the session frames directory
        let thumbUrl = "";
        if (shot.image_paths && shot.image_paths.length > 0) {
            const filename = shot.image_paths[0].split(/[/\\]/).pop();
            thumbUrl = `/playground/${sessionId}/frames/${filename}`;
        }

        // Logic for displaying the VLM reasoning trace on boundary shots
        let reasoningHtml = '';
        if (shot.is_scene_break && shot.logic_analysis && shot.logic_analysis.reasoning) {
            const logicText = shot.logic_analysis.reasoning.replace(/\n/g, '<br>');
            reasoningHtml = `
                <div class="card-body">
                    <div class="meta-row">
                        <span class="meta-label" style="color: var(--break-border); opacity: 1;">Forensic Audit Logic</span>
                        <div class="scroll-box" style="border-left: 2px solid var(--break-border); background: rgba(0,0,0,0.4); color: var(--text-main); max-height: 300px;">
                            ${logicText}
                        </div>
                    </div>
                </div>
            `;
        }

        card.innerHTML = `
            <div class="card-header">
                <span>${shot.shot_id}</span>
                <span>${formatTime(shot.start_time)}</span>
            </div>
            <div class="thumb-container">
                <img src="${thumbUrl}" alt="Frame" onerror="this.style.display='none'">
            </div>
            ${reasoningHtml}
        `;

        // Interactivity: Bind seek to shot
        card.onclick = () => onShotClick(shot);
        container.appendChild(card);
    });
}

/**
 * Synchronizes visual selection in the timeline grid with the current video playhead.
 * @param {string} id - The Shot ID to highlight.
 */
export function updateActiveHighlight(id) {
    const gridContainer = getGridContainer();
    if (!gridContainer) return;

    // Clear existing selection
    gridContainer.querySelectorAll('.shot-card.active').forEach(el => {
        el.classList.remove('active');
    });

    // Apply highlight and scroll into view
    const target = document.getElementById(`card-${id}`);
    if (target) {
        target.classList.add('active');
        target.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
    }
}

/**
 * Utility: Formats seconds into a standard MM:SS timecode.
 */
function formatTime(seconds) {
    if (!seconds || isNaN(seconds)) return "0:00";
    const m = Math.floor(seconds / 60);
    const s = Math.floor(seconds % 60);
    return `${m}:${s.toString().padStart(2, '0')}`;
}
```

File 47-58: `static\js\modules\playground\managers\adapter_manager.js`
```js
/**
 * Adapter Manager
 * Handles the logic for fetching, filtering, selecting, and displaying adapter options.
 * Integrates with the new Inspector Panel for context-aware feedback.
 */

import * as api from '../service.js';
import * as ui from '../view.js'; // Import view to trigger inspector updates

export class AdapterManager {
    constructor() {
        this.els = {
            modelSelect: document.getElementById('modelSelect'),
            adapterInput: document.getElementById('adapterPath'),
            adapterMetaHint: document.getElementById('adapterMetaHint'),
            adapterContainer: document.getElementById('adapterContainer'),
            forceLoadCheckbox: document.getElementById('forceLoadAdapter')
        };

        // Cache for adapter metadata
        this.adapterCache = window.ADAPTER_MANIFEST || [];

        this.init();
    }

    init() {
        if (this.els.modelSelect) {
            this.els.modelSelect.addEventListener('change', () => this.updateOptions());
        }

        if (this.els.adapterInput) {
            this.els.adapterInput.addEventListener('change', () => this.handleSelection());
            // Add focus/hover hooks for Inspector
            this.els.adapterInput.addEventListener('focus', () => this.handleSelection());
            this.els.adapterInput.addEventListener('mouseenter', () => this.handleSelection());
        }

        if (this.els.forceLoadCheckbox) {
            this.els.forceLoadCheckbox.addEventListener('change', () => this.updateOptions());
        }

        // Initial state check
        this.updateOptions();
    }

    /**
     * Re-fetches the adapter list from the server and updates the UI.
     * Called by AssetManager after a successful upload.
     */
    async refreshList() {
        try {
            const adapters = await api.fetchAdapters();
            this.adapterCache = adapters;
            this.updateOptions();
            return true;
        } catch (e) {
            console.error("[AdapterManager] Failed to refresh list:", e);
            return false;
        }
    }

    /**
     * Determines if validation should be bypassed.
     */
    isForceMode() {
        return this.els.forceLoadCheckbox && this.els.forceLoadCheckbox.checked;
    }

    /**
     * Updates the Inspector panel based on current selection.
     */
    handleSelection() {
        const sel = this.els.adapterInput;
        if (!sel || sel.selectedIndex < 0) return;

        const opt = sel.options[sel.selectedIndex];

        if (sel.value === "") {
            // Default "None" selected
            ui.updateInspector('param', {
                label: "LoRA Adapter",
                range: "N/A",
                desc: "No adapter selected. Model will run in base configuration without specialized tuning."
            });
        } else {
            // Actual adapter selected
            const meta = {
                name: opt.textContent.split('(')[0].trim(),
                rank: opt.dataset.rank,
                alpha: opt.dataset.alpha,
                id: sel.value
            };
            ui.updateInspector('adapter', meta);
        }
    }

    /**
     * Updates the adapter dropdown options based on the selected base model
     * and the state of the Force Load checkbox.
     */
    updateOptions() {
        const selectedModel = this.els.modelSelect.value;
        const isCloud = window.CLOUD_MODELS && window.CLOUD_MODELS.includes(selectedModel);

        // Reset UI State
        if (this.els.adapterInput) {
            this.els.adapterInput.innerHTML = '<option value="">None (Base Model)</option>';
            this.els.adapterInput.disabled = true;
            this.els.adapterInput.value = "";
        }
        this.hideHint();

        // 1. Cloud Model or No Selection -> Hide Container
        if (!selectedModel || isCloud) {
            if (this.els.adapterContainer) this.els.adapterContainer.style.display = 'none';
            return;
        }

        // 2. Local Model Selected -> Show Container
        if (this.els.adapterContainer) this.els.adapterContainer.style.display = 'block';

        const isForce = this.isForceMode();

        // Helper: Get filename from path
        const getBasename = (path) => path ? path.split(/[/\\]/).pop() : '';
        const targetBaseName = getBasename(selectedModel);

        // Filter Logic
        let availableAdapters = [];

        if (isForce) {
            // FORCE MODE: Show everything
            availableAdapters = this.adapterCache;
        } else {
            // SAFE MODE: Filter by compatibility
            availableAdapters = this.adapterCache.filter(adapter => {
                if (!adapter.base_model) return false;
                const adapterBaseName = getBasename(adapter.base_model);
                // Strict match OR basename match
                return adapter.base_model === selectedModel || adapterBaseName === targetBaseName;
            });
        }

        // Populate Dropdown
        if (availableAdapters.length > 0) {
            this.els.adapterInput.disabled = false;

            availableAdapters.forEach(adapter => {
                const opt = document.createElement('option');
                opt.value = adapter.path;

                let label = adapter.name;
                if (adapter.rank && adapter.rank !== 'N/A') {
                    label += ` (r=${adapter.rank}, α=${adapter.alpha})`;
                }

                // Visual indicator for force mode mismatch
                if (isForce) {
                    const adapterBaseName = getBasename(adapter.base_model);
                    const isMatch = adapter.base_model === selectedModel || adapterBaseName === targetBaseName;
                    if (!isMatch) {
                        label = `⚠️ ${label}`;
                        opt.style.color = '#ffb800'; // Warning color
                    }
                }

                opt.textContent = label;
                opt.dataset.rank = adapter.rank;
                opt.dataset.alpha = adapter.alpha;

                this.els.adapterInput.appendChild(opt);
            });
        } else {
            const opt = document.createElement('option');
            opt.textContent = isForce ? "-- Archive Empty --" : "-- No compatible adapters found --";
            opt.disabled = true;
            this.els.adapterInput.appendChild(opt);
        }
    }

    hideHint() {
        if (this.els.adapterMetaHint) {
            this.els.adapterMetaHint.style.display = 'none';
            this.els.adapterMetaHint.textContent = '';
        }
    }
}
```

File 48-58: `static\js\modules\playground\managers\asset_manager.js`
```js
/**
 * Asset Manager
 * Handles the dynamic import of Hugging Face models and the upload of LoRA adapters.
 * Manages the shared asset progress bar within the new modal system.
 */

import * as api from '../service.js';

export class AssetManager {
    /**
     * @param {Object} callbacks - Hooks to refresh other UI components.
     * @param {Function} callbacks.onModelAdded - Called when a new model is successfully imported.
     * @param {Function} callbacks.onAdapterAdded - Called when a new adapter is successfully uploaded.
     */
    constructor(callbacks = {}) {
        this.callbacks = callbacks;
        this.POLL_INTERVAL = 1000;

        this.els = {
            // Import Modal Inputs
            hfModelIdInput: document.getElementById('hfModelId'),
            importBtn: document.getElementById('importModelBtn'),
            modelSelect: document.getElementById('modelSelect'),

            // Import Progress
            importContainer: document.getElementById('importProgressContainer'),
            importBar: document.getElementById('importProgressBar'),
            importStatus: document.getElementById('importStatusText'),
            importPercent: document.getElementById('importPercentText'),

            // Upload Modal Inputs
            adapterInput: document.getElementById('adapterUploadInput'),
            adapterDropZone: document.getElementById('adapterDropZone'),

            // Upload Progress
            uploadContainer: document.getElementById('uploadProgressContainer'),
            uploadBar: document.getElementById('uploadProgressBar'),
            uploadStatus: document.getElementById('uploadStatusText'),
            uploadPercent: document.getElementById('uploadPercentText')
        };

        this.init();
    }

    init() {
        if (this.els.importBtn) {
            this.els.importBtn.addEventListener('click', () => this.handleModelImport());
        }

        if (this.els.adapterInput) {
            this.els.adapterInput.addEventListener('change', (e) => this.handleAdapterUpload(e));
        }

        // Setup Drag & Drop for Adapter Upload
        if (this.els.adapterDropZone) {
            this.setupAdapterDnD();
        }
    }

    setupAdapterDnD() {
        const zone = this.els.adapterDropZone;

        zone.ondragover = (e) => {
            e.preventDefault();
            zone.style.borderColor = 'var(--accent)';
            zone.style.background = 'rgba(0, 240, 255, 0.05)';
        };

        zone.ondragleave = () => {
            zone.style.borderColor = 'var(--border)';
            zone.style.background = '';
        };

        zone.ondrop = (e) => {
            e.preventDefault();
            zone.style.borderColor = 'var(--border)';
            zone.style.background = '';

            if (e.dataTransfer.files.length) {
                const file = e.dataTransfer.files[0];
                this.handleAdapterFile(file);
            }
        };
    }

    // --- MODEL IMPORT LOGIC ---

    async handleModelImport() {
        const modelId = this.els.hfModelIdInput.value.trim();
        if (!modelId) {
            alert("Please enter a valid Hugging Face Model ID (e.g., google/siglip-so400m).");
            return;
        }

        this.setImportBusy(true);
        this.updateImportProgress(0, "Initiating download request...");

        try {
            const data = await api.importModel(modelId);
            // Start polling the background worker
            this.pollDownloadStatus(data.task_id);
        } catch (err) {
            this.endImportWithError(`Import failed: ${err.message}`);
        }
    }

    pollDownloadStatus(taskId) {
        const intervalId = setInterval(async () => {
            try {
                const status = await api.pollTaskStatus(taskId);

                if (status.state === 'PROGRESS') {
                    // Map step 0-3 to 0-100% roughly
                    let pct = 0;
                    if (status.step === 1) pct = 25;
                    if (status.step === 2) pct = 75;

                    this.updateImportProgress(pct, status.status || "Downloading assets...");
                }

                if (status.state === 'SUCCESS') {
                    clearInterval(intervalId);
                    this.updateImportProgress(100, "Download complete. Updating registry...");
                    await this.refreshModelList(status.result.model_id);
                } else if (status.state === 'FAILURE') {
                    clearInterval(intervalId);
                    this.endImportWithError(status.status);
                }

            } catch (err) {
                console.warn("[AssetManager] Poll error:", err);
            }
        }, this.POLL_INTERVAL);
    }

    async refreshModelList(newModelId) {
        try {
            const models = await api.fetchModels();

            // Rebuild the select dropdown
            this.els.modelSelect.innerHTML = '<option value="" disabled>-- Select Base Model --</option>';

            models.forEach(m => {
                const opt = document.createElement('option');
                opt.value = m;
                opt.textContent = m;
                if (m === newModelId) opt.selected = true;
                this.els.modelSelect.appendChild(opt);
            });

            // Notify listeners
            this.els.modelSelect.dispatchEvent(new Event('change'));

            this.endImportWithSuccess(`Model '${newModelId}' is ready.`);
            this.els.hfModelIdInput.value = '';

        } catch (err) {
            this.endImportWithError("Failed to refresh model list.");
        }
    }

    // --- ADAPTER UPLOAD LOGIC ---

    handleAdapterUpload(e) {
        const file = e.target.files[0];
        if (file) this.handleAdapterFile(file);
    }

    async handleAdapterFile(file) {
        // Basic client-side validation
        if (file.name.split('.').pop().toLowerCase() !== 'zip') {
            alert("Only .zip files are allowed.");
            this.els.adapterInput.value = '';
            return;
        }

        this.setUploadBusy(true);
        this.updateUploadProgress(10, `Uploading ${file.name}...`);

        const formData = new FormData();
        formData.append('file', file);

        try {
            this.updateUploadProgress(50, "Extracting on server...");
            const result = await api.uploadAdapter(formData);

            this.updateUploadProgress(100, "Installation complete.");

            if (this.callbacks.onAdapterAdded) {
                await this.callbacks.onAdapterAdded();
            }

            this.endUploadWithSuccess(result.message);

        } catch (err) {
            this.endUploadWithError(`Upload failed: ${err.message}`);
        } finally {
            this.els.adapterInput.value = '';
        }
    }

    // --- UI HELPERS (IMPORT) ---

    setImportBusy(isBusy) {
        if (this.els.importBtn) this.els.importBtn.disabled = isBusy;
        if (this.els.hfModelIdInput) this.els.hfModelIdInput.disabled = isBusy;

        if (isBusy) {
            this.els.importContainer.style.display = 'block';
            this.els.importStatus.style.color = 'var(--text-dim)';
        }
    }

    updateImportProgress(percent, text) {
        this.els.importBar.style.width = `${percent}%`;
        this.els.importPercent.textContent = `${percent}%`;
        if (text) this.els.importStatus.textContent = text;
    }

    endImportWithSuccess(msg) {
        this.updateImportProgress(100, msg);
        this.els.importStatus.style.color = 'var(--success)';
        this.els.importBar.style.background = 'var(--success)';

        setTimeout(() => {
            this.resetImportUI();
            // Close modal via global helper if available, or just reset state
            const modal = document.getElementById('importModal');
            if (modal && typeof modal.close === 'function') modal.close();
        }, 2000);
    }

    endImportWithError(msg) {
        this.els.importBar.style.width = '100%';
        this.els.importBar.style.background = 'var(--break-border)';
        this.els.importStatus.textContent = msg;
        this.els.importStatus.style.color = 'var(--break-border)';

        setTimeout(() => {
            this.resetImportUI();
        }, 4000);
    }

    resetImportUI() {
        this.setImportBusy(false);
        this.els.importContainer.style.display = 'none';
        this.els.importBar.style.width = '0%';
        this.els.importBar.style.background = 'var(--accent)';
        this.els.importPercent.textContent = '0%';
        this.els.importStatus.textContent = 'Idle';
    }

    // --- UI HELPERS (UPLOAD) ---

    setUploadBusy(isBusy) {
        if (this.els.adapterInput) this.els.adapterInput.disabled = isBusy;

        if (isBusy) {
            this.els.uploadContainer.style.display = 'block';
            this.els.uploadStatus.style.color = 'var(--text-dim)';
        }
    }

    updateUploadProgress(percent, text) {
        this.els.uploadBar.style.width = `${percent}%`;
        this.els.uploadPercent.textContent = `${percent}%`;
        if (text) this.els.uploadStatus.textContent = text;
    }

    endUploadWithSuccess(msg) {
        this.updateUploadProgress(100, msg);
        this.els.uploadStatus.style.color = 'var(--success)';
        this.els.uploadBar.style.background = 'var(--success)';

        setTimeout(() => {
            this.resetUploadUI();
            const modal = document.getElementById('uploadModal');
            if (modal && typeof modal.close === 'function') modal.close();
        }, 2000);
    }

    endUploadWithError(msg) {
        this.els.uploadBar.style.width = '100%';
        this.els.uploadBar.style.background = 'var(--break-border)';
        this.els.uploadStatus.textContent = msg;
        this.els.uploadStatus.style.color = 'var(--break-border)';

        setTimeout(() => {
            this.resetUploadUI();
        }, 4000);
    }

    resetUploadUI() {
        this.setUploadBusy(false);
        this.els.uploadContainer.style.display = 'none';
        this.els.uploadBar.style.width = '0%';
        this.els.uploadBar.style.background = 'var(--accent)';
        this.els.uploadPercent.textContent = '0%';
        this.els.uploadStatus.textContent = 'Idle';
    }
}
```

File 49-58: `static\js\modules\playground\managers\file_manager.js`
```js
/**
 * File Manager
 * Handles video file selection, drag-and-drop zones, and UI card rendering.
 * Manages the client-side state of the uploaded asset within the new Workflow Canvas.
 */

import { store } from '../store.js';

export class FileManager {
    constructor(processBtnCallback) {
        this.els = {
            fileInput: document.getElementById('videoInput'),
            dropZone: document.getElementById('dropZone'),
            dropPrompt: document.getElementById('dropPrompt')
            // Note: Process/Launch button is managed via callback now
        };

        // Callback to enable the "Run Inference" button when a file is ready
        this.enableProcessBtn = processBtnCallback;

        this.init();
    }

    init() {
        if (this.els.fileInput) {
            this.els.fileInput.addEventListener('change', (e) => this.handleFileSelect(e));
        }
        this.setupDragAndDrop();
    }

    setupDragAndDrop() {
        if (!this.els.dropZone) return;

        this.els.dropZone.ondragover = (e) => {
            e.preventDefault();
            this.els.dropZone.style.borderColor = 'var(--accent)';
            this.els.dropZone.style.backgroundColor = 'rgba(0, 240, 255, 0.05)';
        };
        this.els.dropZone.ondragleave = () => {
            this.els.dropZone.style.borderColor = '';
            this.els.dropZone.style.backgroundColor = '';
        };
        this.els.dropZone.ondrop = (e) => {
            e.preventDefault();
            this.els.dropZone.style.borderColor = '';
            this.els.dropZone.style.backgroundColor = '';
            if (e.dataTransfer.files.length) {
                this.els.fileInput.files = e.dataTransfer.files;
                this.handleFileSelect({ target: this.els.fileInput });
            }
        };
    }

    handleFileSelect(e) {
        const file = e.target.files[0];
        if (file) {
            store.videoFile = file;
            store.videoUrl = URL.createObjectURL(file);

            if (this.enableProcessBtn) this.enableProcessBtn(true);
            this.renderActiveFile(file);
        }
    }

    renderActiveFile(file) {
        if (this.els.dropPrompt) this.els.dropPrompt.classList.add('hidden');

        let fileCard = document.getElementById('activeFileCard');
        if (!fileCard) {
            fileCard = document.createElement('div');
            fileCard.id = 'activeFileCard';
            fileCard.className = 'upload-file-card active';
            this.els.dropZone.appendChild(fileCard);
        }

        const sizeMB = (file.size / (1024 * 1024)).toFixed(2);

        // Metadata Probe
        const tempVideo = document.createElement('video');
        tempVideo.preload = 'metadata';
        tempVideo.onloadedmetadata = () => {
            const duration = tempVideo.duration.toFixed(1);
            this.updateCardContent(fileCard, file.name, sizeMB, duration + 's');
            URL.revokeObjectURL(tempVideo.src);
        };
        tempVideo.onerror = () => {
            this.updateCardContent(fileCard, file.name, sizeMB, '???');
        };
        tempVideo.src = URL.createObjectURL(file);
    }

    updateCardContent(card, name, size, dur) {
        card.innerHTML = `
            <div class="file-icon-box"><i class="fa-solid fa-file-video"></i></div>
            <div class="file-info-stack">
                <div class="file-name" title="${name}">${name}</div>
                <div class="file-meta"><span>${size} MB</span><span style="color:var(--border);">|</span><span>${dur}</span></div>
            </div>
            <button class="file-remove-btn" id="removeFileBtn" title="Remove File"><i class="fa-solid fa-xmark"></i></button>
        `;

        const btn = document.getElementById('removeFileBtn');
        if (btn) {
            btn.onclick = (e) => {
                e.preventDefault();
                e.stopPropagation();
                this.clearActiveFile();
            };
        }
    }

    clearActiveFile() {
        store.videoFile = null;
        if (store.videoUrl) URL.revokeObjectURL(store.videoUrl);
        store.videoUrl = null;

        this.els.fileInput.value = '';

        const fileCard = document.getElementById('activeFileCard');
        if (fileCard) fileCard.remove();

        if (this.els.dropPrompt) this.els.dropPrompt.classList.remove('hidden');
        if (this.enableProcessBtn) this.enableProcessBtn(false);
    }
}
```

File 50-58: `static\js\modules\playground\managers\inference_manager.js`
```js
/**
 * Inference Manager
 * Orchestrates the execution of the multimodal pipeline.
 * Collects UI parameters (including LoRA Scale, High Fidelity Mode, & Repetition Penalty), 
 * submits the job, polls for status, manages abortion, and finalizes the result.
 */

import { store } from '../store.js';
import * as api from '../service.js';
import * as ui from '../view.js';
import * as timeline from '../inference_timeline.js';
import * as player from '../player_view.js';

const POLLING_INTERVAL = 1500;

export class InferenceManager {
    constructor() {
        this.currentTaskId = null; // Track active task for abortion

        this.els = {
            modelSelect: document.getElementById('modelSelect'),
            adapterInput: document.getElementById('adapterPath'),
            windowInput: document.getElementById('windowSize'),

            // Hyperparameters
            paramTemp: document.getElementById('paramTemp'),
            paramTopP: document.getElementById('paramTopP'),
            paramMaxTokens: document.getElementById('paramMaxTokens'),
            paramSystem: document.getElementById('paramSystem'),
            paramMain: document.getElementById('paramMain'),
            paramLoraScale: document.getElementById('paramLoraScale'),
            paramRepPenalty: document.getElementById('paramRepPenalty'),
            streamInterval: document.getElementById('streamInterval'),

            // Toggles
            forceCheckbox: document.getElementById('forceLoadAdapter'),
            hifiCheckbox: document.getElementById('highFidelityMode'),

            // Actions
            processBtn: document.getElementById('processBtn'),
            abortBtn: document.getElementById('abortBtn')
        };

        this.init();
    }

    init() {
        if (this.els.processBtn) {
            this.els.processBtn.addEventListener('click', () => this.startInference());
        }
        if (this.els.abortBtn) {
            this.els.abortBtn.addEventListener('click', () => this.handleAbort());
        }
    }

    async startInference() {
        if (!store.videoFile) return;

        // Reset Abort State
        this.currentTaskId = null;
        if (this.els.abortBtn) {
            this.els.abortBtn.disabled = false;
            this.els.abortBtn.innerHTML = '<i class="fa-solid fa-ban"></i> Stop Execution';
        }

        ui.toggleViewState('processing');
        ui.updateProgressBar(0);

        // --- Log Start Parameters ---
        let msg = "Dispatching multimodal payload...";

        const isForce = this.els.forceCheckbox && this.els.forceCheckbox.checked;
        const isHifi = this.els.hifiCheckbox && this.els.hifiCheckbox.checked && !this.els.hifiCheckbox.disabled;

        if (isForce || isHifi) {
            let modes = [];
            if (isForce) modes.push("FORCE ADAPTER");
            if (isHifi) modes.push("HIGH-FIDELITY (BF16)");
            ui.logConsole(`⚠️ ACTIVE MODES: ${modes.join(', ')}`, false, true);
        } else {
            ui.logConsole(msg);
        }

        const startTime = Date.now();
        const formData = new FormData();

        // Core Inputs
        formData.append('video', store.videoFile);
        formData.append('model_id', this.els.modelSelect.value);
        formData.append('window_size', this.els.windowInput.value);

        // Hyperparameters
        formData.append('temperature', this.els.paramTemp.value);
        formData.append('top_p', this.els.paramTopP.value);
        formData.append('max_tokens', this.els.paramMaxTokens.value);

        // System Prompt: Use override if exists, else it might fall back to default backend logic
        if (this.els.paramSystem) {
            formData.append('system_prompt', this.els.paramSystem.value);
        }

        // Stream Interval (Operational Param)
        if (this.els.streamInterval) {
            formData.append('stream_interval', this.els.streamInterval.value);
        }

        // Main Prompt Override
        if (this.els.paramMain) {
            formData.append('main_prompt', this.els.paramMain.value);
        }

        if (this.els.paramRepPenalty) {
            formData.append('repetition_penalty', this.els.paramRepPenalty.value);
        }

        if (this.els.paramLoraScale) {
            formData.append('lora_scale', this.els.paramLoraScale.value);
        }

        if (isForce) {
            formData.append('bypass_validation', 'true');
        }

        if (isHifi) {
            formData.append('high_fidelity_mode', 'true');
        }

        const adapterPath = this.els.adapterInput ? this.els.adapterInput.value.trim() : '';
        if (adapterPath) {
            formData.append('adapter_path', adapterPath);
        }

        try {
            const data = await api.triggerInferenceApi(formData);
            this.currentTaskId = data.task_id;
            this.pollStatus(data.task_id, startTime);
        } catch (err) {
            ui.logConsole(`[FATAL] ${err.message}`, false, true);
        }
    }

    async handleAbort() {
        if (!this.currentTaskId) return;

        if (this.els.abortBtn) {
            this.els.abortBtn.disabled = true;
            this.els.abortBtn.innerHTML = '<i class="fa-solid fa-spinner fa-spin"></i> Stopping...';
        }

        ui.logConsole("⚠️ Sending abort signal to backend...", false, true);

        try {
            await api.abortTaskApi(this.currentTaskId);
        } catch (err) {
            ui.logConsole(`[ERROR] Failed to send abort signal: ${err.message}`, false, true);
        }
    }

    pollStatus(taskId, startTime) {
        const interval = setInterval(async () => {
            try {
                const status = await api.pollTaskStatus(taskId);

                if (status.logs && Array.isArray(status.logs) && status.logs.length > 0) {
                    ui.logConsole(status.logs);
                } else if (status.status) {
                    ui.logConsole(status.status);
                }

                if (status.step && status.total) {
                    const pct = (status.step / status.total) * 100;
                    ui.updateProgressBar(pct);
                }

                if (status.state === 'SUCCESS') {
                    clearInterval(interval);
                    const totalTime = ((Date.now() - startTime) / 1000).toFixed(2);
                    await this.finalizeInference(status.result.session_id, totalTime);
                } else if (status.state === 'FAILURE') {
                    clearInterval(interval);
                    ui.logConsole(`[STOPPED] ${status.status}`, false, true);
                }
            } catch (err) {
                console.warn("[POLL] Connecting...", err);
            }
        }, POLLING_INTERVAL);
    }

    async finalizeInference(sessionId, totalTime) {
        ui.logConsole("Rehydrating session state from SSOT...", false, true);
        try {
            const data = await api.fetchSessionData(sessionId);

            store.inferenceResults = data.shots;
            store.activeSessionId = sessionId;

            timeline.renderTimeline(sessionId, (shot) => player.seekToShot(shot));
            player.initPlayer(store.videoUrl);
            player.syncPlayer();

            ui.updateHUD(`${totalTime}s`, this.els.modelSelect.value);
            ui.toggleViewState('results');
        } catch (err) {
            ui.logConsole(`[ERROR] Failed to hydrate session: ${err.message}`, false, true);
        }
    }
}
```

File 51-58: `static\js\modules\playground\managers\session_manager.js`
```js
/**
 * Session Manager
 * Handles the loading, deletion, and hydration of archived inference sessions.
 * Manages the sidebar history list and state restoration.
 */

import { store } from '../store.js';
import * as api from '../service.js';
import * as ui from '../view.js';
import * as timeline from '../inference_timeline.js';
import * as player from '../player_view.js';

export class SessionManager {
    constructor() {
        this.init();
    }

    init() {
        // Load initial list
        this.refreshList();
    }

    async refreshList() {
        try {
            const sessions = await api.fetchSessions();
            ui.renderSessionList(
                sessions,
                (id) => this.hydrateSession(id), // On Select
                (id) => this.deleteSession(id)   // On Delete
            );
        } catch (err) {
            ui.logConsole(err.message, false, true);
        }
    }

    async deleteSession(id) {
        if (!confirm("Permanently delete this session and its assets?")) return;
        try {
            await api.deleteSessionApi(id);
            this.refreshList();
        } catch (err) {
            alert(err.message);
        }
    }

    async hydrateSession(sessionId) {
        try {
            ui.logConsole(`[ARCHIVE] Retrieving state for ${sessionId}...`);
            const data = await api.fetchSessionData(sessionId);

            store.inferenceResults = data.shots;
            store.activeSessionId = sessionId;

            // Restore Player & Timeline
            player.initPlayer(`/playground/${sessionId}/${data.metadata.video_filename}`);
            player.syncPlayer();
            timeline.renderTimeline(sessionId, (shot) => player.seekToShot(shot));

            // Restore HUD
            const perf = data.metadata.performance || {};
            const duration = perf.total_task ? perf.total_task.toFixed(1) + 's' : 'PERSISTED';
            ui.updateHUD(duration, data.metadata.model_id);

            // Switch View
            ui.toggleViewState('results');

        } catch (err) {
            console.error(err);
            alert("Failed to load archive: " + err.message);
        }
    }
}
```

File 52-58: `static\js\modules\playground\monitoring_deck.js`
```js
/**
 * Hardware Monitoring Deck
 * Handles polling and rendering of system statistics (CPU, RAM, GPU).
 */
export class MonitoringDeck {
    constructor() {
        this.pollInterval = 2000; // 2 seconds
        this.isPolling = false;

        // DOM Elements
        this.cpuVal = document.getElementById('monitorCpuVal');
        this.cpuBar = document.getElementById('monitorCpuBar');

        this.ramVal = document.getElementById('monitorRamVal');
        this.ramBar = document.getElementById('monitorRamBar');
        this.ramDetail = document.getElementById('monitorRamDetail');

        this.gpuContainer = document.getElementById('monitorGpuContainer');
    }

    start() {
        if (this.isPolling) return;
        this.isPolling = true;
        this.poll();
        this.timer = setInterval(() => this.poll(), this.pollInterval);
    }

    stop() {
        this.isPolling = false;
        if (this.timer) clearInterval(this.timer);
    }

    async poll() {
        try {
            const response = await fetch('/api/playground/hardware');
            if (!response.ok) return;
            const stats = await response.json();
            this.render(stats);
        } catch (e) {
            console.error("Hardware monitor poll failed:", e);
        }
    }

    render(stats) {
        // CPU
        if (stats.cpu) {
            const cpuPct = stats.cpu.usage_percent;
            if (this.cpuVal) this.cpuVal.innerText = `${cpuPct}%`;
            if (this.cpuBar) this.cpuBar.style.width = `${cpuPct}%`;
        }

        // RAM
        if (stats.ram) {
            const ramPct = stats.ram.percent;
            if (this.ramVal) this.ramVal.innerText = `${ramPct}%`;
            if (this.ramBar) this.ramBar.style.width = `${ramPct}%`;
            if (this.ramDetail) this.ramDetail.innerText = `${stats.ram.used_gb} / ${stats.ram.total_gb} GB`;
        }

        // Disk
        if (stats.disk) {
            const diskPct = stats.disk.percent;
            const diskVal = document.getElementById('monitorDiskVal');
            const diskBar = document.getElementById('monitorDiskBar');
            const diskDetail = document.getElementById('monitorDiskDetail');

            if (diskVal) diskVal.innerText = `${diskPct}%`;
            if (diskBar) diskBar.style.width = `${diskPct}%`;
            if (diskDetail) diskDetail.innerText = `${stats.disk.used_gb} / ${stats.disk.total_gb} GB`;
        }

        // GPU
        if (stats.gpu && stats.gpu.length > 0 && this.gpuContainer) {
            this.gpuContainer.innerHTML = ''; // Clear previous
            stats.gpu.forEach(gpu => {
                const vramUsed = (gpu.memory_used_mb / 1024).toFixed(1);
                const vramTotal = (gpu.memory_total_mb / 1024).toFixed(1);

                const gpuWidget = document.createElement('div');
                gpuWidget.className = 'monitor-widget';
                gpuWidget.innerHTML = `
                    <div class="monitor-label">
                        <i class="fa-solid fa-microchip"></i> GPU ${gpu.id}: ${gpu.name}
                    </div>
                    <div class="monitor-main">
                        <div class="monitor-val">${gpu.load_percent}%</div>
                        <div class="monitor-bar-track">
                            <div class="monitor-bar-fill" style="width: ${gpu.load_percent}%"></div>
                        </div>
                    </div>
                    <div class="monitor-detail">
                        VRAM: ${vramUsed} / ${vramTotal} GB  (${gpu.temperature_c}°C)
                    </div>
                `;
                this.gpuContainer.appendChild(gpuWidget);
            });
            this.gpuContainer.style.display = 'flex';
        } else if (this.gpuContainer) {
            this.gpuContainer.style.display = 'none';
        }
    }
}
```

File 53-58: `static\js\modules\playground\player_view.js`
```js
/**
 * PlayerView.js
 * Playground Video Player Module.
 * Manages media playback, high-contrast overlay synchronization, 
 * and playback-state navigation (skipping shots and scenes).
 */

import { store } from './store.js';
import { updateActiveHighlight } from './inference_timeline.js';

let videoPlayer = null;
let overlay = null;

/**
 * Initializes the video element with a source.
 * @param {string} src - The URL of the video file to load.
 */
export function initPlayer(src) {
    videoPlayer = document.getElementById('videoPlayer');
    overlay = document.getElementById('overlayShotId');

    if (!videoPlayer) return;
    videoPlayer.src = src;
    videoPlayer.load();
}

/**
 * Attaches the timecode synchronization loop.
 * Updates the timeline UI and the on-screen overlay as the video plays.
 */
export function syncPlayer() {
    if (!videoPlayer) return;

    videoPlayer.ontimeupdate = () => {
        const t = videoPlayer.currentTime;

        // Perform range scan to identify the shot currently under the playhead
        const shotIdx = store.inferenceResults.findIndex(s =>
            t >= s.start_time && t < s.end_time
        );

        if (shotIdx !== -1 && shotIdx !== store.currentShotIndex) {
            store.currentShotIndex = shotIdx;
            const activeShot = store.inferenceResults[shotIdx];

            // Update global storyboard highlight
            updateActiveHighlight(activeShot.shot_id);

            // Update high-contrast HUD overlay
            if (overlay) {
                overlay.textContent = activeShot.shot_id;
                overlay.style.display = 'block';
            }
        }
    };
}

/**
 * Navigates the playhead by a relative number of technical shots.
 * @param {number} offset - Relative index change (e.g., -1 for previous shot).
 */
export function skipToShot(offset) {
    if (!store.inferenceResults.length || !videoPlayer) return;

    const newIdx = Math.max(0, Math.min(
        store.currentShotIndex + offset,
        store.inferenceResults.length - 1
    ));

    const targetShot = store.inferenceResults[newIdx];
    videoPlayer.currentTime = targetShot.start_time;

    // Ensure UI selection is synced even if video is paused
    if (videoPlayer.paused) {
        updateActiveHighlight(targetShot.shot_id);
    }
}

/**
 * Jumps the playhead to the nearest narrative scene boundary.
 * @param {number} offset - Direction of search (1 for next, -1 for previous).
 */
export function skipToScene(offset) {
    if (!store.inferenceResults.length || !videoPlayer) return;

    let idx = store.currentShotIndex + offset;
    while (idx >= 0 && idx < store.inferenceResults.length) {
        if (store.inferenceResults[idx].is_scene_break) {
            const targetShot = store.inferenceResults[idx];
            videoPlayer.currentTime = targetShot.start_time;

            if (videoPlayer.paused) {
                updateActiveHighlight(targetShot.shot_id);
            }
            return;
        }
        idx += (offset > 0 ? 1 : -1);
    }
}

/**
 * Direct seek to a specific shot's start time.
 * @param {Object} shot - The shot metadata object.
 */
export function seekToShot(shot) {
    if (!videoPlayer) return;
    videoPlayer.currentTime = shot.start_time;

    // Manual UI sync for paused state
    if (videoPlayer.paused) {
        updateActiveHighlight(shot.shot_id);
    }
}
```

File 54-58: `static\js\modules\playground\playground_bootstrap.js`
```js
/**
 * PlaygroundBootstrap.js
 * Application Entry Point.
 * Initializes the domain-specific managers and binds global event listeners.
 */

import { store } from './store.js';
import * as ui from './view.js';
import * as api from './service.js';
import * as debugConsole from './forensic_terminal.js';

// Import New Modular Managers
import { AdapterManager } from './managers/adapter_manager.js';
import { FileManager } from './managers/file_manager.js';
import { SessionManager } from './managers/session_manager.js';
import { InferenceManager } from './managers/inference_manager.js';
import { AssetManager } from './managers/asset_manager.js';
import { MonitoringDeck } from './monitoring_deck.js';

let activeLogs = { pipeline: "", debug: [] };
let activeDebugTab = 'pipeline';

// State Managers
let adapterMgr, fileMgr, sessionMgr, inferenceMgr, assetMgr, monitorMgr;

/**
 * Entry point: Bootstraps the modular system.
 */
function init() {
    console.log("[Playground] Bootstrapping Modular Architecture...");

    // Initialize Managers
    adapterMgr = new AdapterManager();
    inferenceMgr = new InferenceManager();
    sessionMgr = new SessionManager();
    monitorMgr = new MonitoringDeck();

    // Start Hardware Monitoring
    monitorMgr.start();

    // File Manager needs access to enable/disable the Inference Process Button
    fileMgr = new FileManager((isEnabled) => {
        // Find the launch button in Step 4
        const btn = document.querySelector('.launch-btn');
        if (btn) {
            btn.disabled = !isEnabled;
            const subText = btn.querySelector('.launch-sub');
            if (subText) subText.textContent = isEnabled ? "Ready to Launch" : "Awaiting Configuration";
        }
    });

    // Asset Manager handles dynamic imports and uploads
    // It provides callbacks to refresh other components when assets change
    assetMgr = new AssetManager({
        onAdapterAdded: async () => {
            console.log("[Bootstrap] Adapter added, refreshing list...");
            await adapterMgr.refreshList();
        }
    });

    setupGlobalControls();
    setupInspectorListeners();
    setupModalListeners();
    setupDebugListeners();

    console.log("[Playground] Systems Online.");
}

/**
 * Bindings for Modal interactions (Asset Hub).
 */
function setupModalListeners() {
    // Import Modal
    const importBtn = document.getElementById('openImportModalBtn');
    const closeBtns = document.querySelectorAll('.modal-close-btn');

    if (importBtn) {
        importBtn.onclick = () => ui.toggleModal('importModal', true);
    }

    // Upload Modal
    const uploadBtn = document.getElementById('openUploadModalBtn');
    if (uploadBtn) {
        uploadBtn.onclick = () => ui.toggleModal('uploadModal', true);
    }

    // Generic Close Handler
    closeBtns.forEach(btn => {
        btn.onclick = () => {
            const modalId = btn.dataset.modal;
            ui.toggleModal(modalId, false);
        };
    });
}

/**
 * Bindings for Context Inspector (Column 3).
 * Attaches hover/focus events to data-inspect elements.
 */
function setupInspectorListeners() {
    const inspectables = document.querySelectorAll('[data-inspect]');

    inspectables.forEach(el => {
        const handleInspect = () => {
            const type = el.dataset.inspect;
            let data = {};

            switch (type) {
                case 'temp':
                    data = { label: "Temperature", range: "0.0 - 2.0", desc: "Controls randomness. Lower values are more deterministic; higher values are more creative." };
                    ui.updateInspector('param', data);
                    break;
                case 'top_p':
                    data = { label: "Top P (Nucleus)", range: "0.0 - 1.0", desc: "Limits the token pool to the top cumulative probability P. Filters out unlikely tokens." };
                    ui.updateInspector('param', data);
                    break;
                case 'max_tokens':
                    data = { label: "Max Output Tokens", range: "128 - 32k", desc: "Hard limit on response length. Set high (8192+) for Chain-of-Thought reasoning." };
                    ui.updateInspector('param', data);
                    break;
                case 'rep_penalty':
                    data = { label: "Repetition Penalty", range: "1.0 - 2.0", desc: "Penalizes tokens that have already appeared. Helps prevent looping in visual descriptions." };
                    ui.updateInspector('param', data);
                    break;
                case 'lora_scale':
                    data = { label: "LoRA Scale Factor", range: "0.0 - 2.0", desc: "Multiplies the influence of the adapter weights. >1.0 amplifies the specific training behavior." };
                    ui.updateInspector('param', data);
                    break;
                case 'hifi_mode':
                    data = { label: "High-Fidelity Mode", range: "Boolean", desc: "Loads model in native BFloat16 precision instead of 4-bit NF4 quantization. Requires ~2x VRAM." };
                    ui.updateInspector('param', data);
                    break;
                case 'force_mode':
                    data = { label: "Force Load", range: "Boolean", desc: "Bypasses safety checks preventing mismatched adapters (e.g. Qwen adapter on Llama base)." };
                    ui.updateInspector('param', data);
                    break;
                case 'model_select':
                    if (el.value) ui.updateInspector('model', { id: el.value });
                    break;
                // Add more cases as needed
            }
        };

        el.addEventListener('focus', handleInspect);
        el.addEventListener('mouseenter', handleInspect);

        // Special case for selects to update on change
        if (el.tagName === 'SELECT') {
            el.addEventListener('change', () => {
                if (el.id === 'modelSelect') ui.updateInspector('model', { id: el.value });
                // Adapter changes handled in adapter_manager via manual call
            });
        }
    });
}

/**
 * Global UI Bindings (Reset, Window Size, etc.)
 */
function setupGlobalControls() {
    const els = {
        resetBtn: document.getElementById('resetPlaygroundBtn'),
        winInc: document.getElementById('winInc'),
        winDec: document.getElementById('winDec'),
        windowInput: document.getElementById('windowSize'),
        promptToggle: document.getElementById('promptToggle'),
        promptBody: document.getElementById('promptBody')
    };

    if (els.resetBtn) {
        els.resetBtn.addEventListener('click', () => {
            store.reset();
            ui.resetHUD();
            ui.toggleViewState('config');
            sessionMgr.refreshList();
            fileMgr.clearActiveFile();
            adapterMgr.updateOptions(); // Reset filters
        });
    }

    if (els.winInc) {
        els.winInc.onclick = () => {
            const val = parseInt(els.windowInput.value);
            if (val < 128) els.windowInput.value = val + 8;
        };
    }

    if (els.winDec) {
        els.winDec.onclick = () => {
            const val = parseInt(els.windowInput.value);
            if (val > 8) els.windowInput.value = val - 8;
        };
    }

    if (els.promptToggle) {
        els.promptToggle.onclick = () => {
            const isHidden = els.promptBody.style.display === 'none';
            els.promptBody.style.display = isHidden ? 'block' : 'none';
            const icon = els.promptToggle.querySelector('i');
            if (icon) {
                icon.className = isHidden ? 'fa-solid fa-chevron-up' : 'fa-solid fa-chevron-down';
            }
        };
    }

    window.skipToShot = (offset) => import('./player_view.js').then(m => m.skipToShot(offset));
    window.skipToScene = (offset) => import('./player_view.js').then(m => m.skipToScene(offset));
}

/**
 * Bindings for Debug Console.
 */
function setupDebugListeners() {
    const toggleBtn = document.getElementById('toggleDebugBtn');
    const debugContent = document.getElementById('debugContent');
    const debugTabs = document.querySelectorAll('.debug-tab');

    if (toggleBtn) {
        toggleBtn.addEventListener('click', async () => {
            const isNowVisible = ui.toggleDebugPanel();
            if (isNowVisible && store.activeSessionId) {
                try {
                    activeLogs = await api.fetchSessionLogs(store.activeSessionId);
                    debugConsole.renderDebugPanelContent(debugContent, activeLogs, activeDebugTab);
                } catch (err) {
                    if (debugContent) debugContent.innerHTML = `<div class="sys-label" style="color:var(--break-border)">${err.message}</div>`;
                }
            }
        });
    }

    debugTabs.forEach(tab => {
        tab.addEventListener('click', () => {
            activeDebugTab = tab.dataset.log;
            ui.setActiveDebugTab(activeDebugTab);
            debugConsole.renderDebugPanelContent(debugContent, activeLogs, activeDebugTab);
        });
    });
}

// Bootstrap
document.addEventListener('DOMContentLoaded', init);
```

File 55-58: `static\js\modules\playground\service.js`
```js
/**
 * Service.js
 * Playground API Service.
 * Pure network layer for communicating with evaluation endpoints.
 * This module encapsulates all Playground-specific asynchronous logic.
 */

import { api } from '../../lib/api_client.js';

// --- SESSION MANAGEMENT ---

/**
 * Fetches the list of archived inference sessions from the server.
 * @returns {Promise<Array>} List of session metadata objects.
 */
export async function fetchSessions() {
    return await api.get('/api/playground/sessions');
}

/**
 * Permanently deletes a session and its associated frame data from the server.
 * @param {string} sessionId 
 */
export async function deleteSessionApi(sessionId) {
    return await api.delete(`/api/playground/session/${sessionId}`);
}

/**
 * Retrieves the full state.json (shots + metadata) for a specific session.
 * @param {string} sessionId 
 * @returns {Promise<Object>} The session state object.
 */
export async function fetchSessionData(sessionId) {
    return await api.get(`/api/playground/session/${sessionId}`);
}

/**
 * Retrieves procedural execution logs and raw VLM outputs for forensic analysis.
 * @param {string} sessionId 
 * @returns {Promise<Object>} Object containing 'pipeline' text and 'debug' JSONL entries.
 */
export async function fetchSessionLogs(sessionId) {
    return await api.get(`/api/playground/session/${sessionId}/logs`);
}


// --- INFERENCE PIPELINE ---

/**
 * Initiates the multimodal inference task for an uploaded video.
 * @param {FormData} formData - Contains the video file and configuration parameters.
 * @returns {Promise<Object>} Contains task_id and session_id.
 */
export async function triggerInferenceApi(formData) {
    return await api.post('/api/playground/inference', formData);
}

/**
 * Signals the server to gracefully stop a running task.
 * @param {string} taskId
 * @returns {Promise<Object>} Confirmation status.
 */
export async function abortTaskApi(taskId) {
    return await api.post(`/api/playground/abort/${taskId}`);
}

/**
 * Polls the global task registry for the current status of an inference job.
 * @param {string} taskId 
 * @returns {Promise<Object>} Current task state, step, and real-time logs.
 */
export async function pollTaskStatus(taskId) {
    return await api.get(`/status/${taskId}`);
}


// --- ASSET MANAGEMENT (MODELS & ADAPTERS) ---

/**
 * Fetches the dynamic list of available VLM models (cloud + local).
 * @returns {Promise<Array<string>>} List of model IDs.
 */
export async function fetchModels() {
    return await api.get('/api/playground/models/list');
}

/**
 * Fetches the dynamic list of locally installed LoRA adapters.
 * @returns {Promise<Array<Object>>} List of adapter metadata objects.
 */
export async function fetchAdapters() {
    return await api.get('/api/playground/adapters/list');
}

/**
 * Triggers a background task to download a Hugging Face model.
 * @param {string} modelId - The Hugging Face repo ID (e.g., 'org/model').
 * @returns {Promise<Object>} Contains task_id.
 */
export async function importModel(modelId) {
    return await api.post('/api/playground/models/import', { model_id: modelId });
}

/**
 * Uploads a zip file containing a LoRA adapter for extraction.
 * @param {FormData} formData - Contains the 'file' input.
 * @returns {Promise<Object>} Status and adapter ID.
 */
export async function uploadAdapter(formData) {
    return await api.post('/api/playground/adapters/upload', formData);
}
```

File 56-58: `static\js\modules\playground\store.js`
```js
/**
 * Store.js
 * Playground State Store.
 * Centralized source of truth for the inference session.
 * This module manages reactive data for the timeline, player synchronization, 
 * and session metadata.
 */

export const store = {
    // Media & Assets
    videoFile: null,
    videoUrl: null,

    // Session Context
    activeSessionId: null,
    inferenceResults: [], // Array of shot metadata with is_scene_break flags

    // Navigation State
    currentShotIndex: -1,

    // Performance & Model Tracking
    totalInferenceTime: "0.0s",
    activeModelName: "NONE",

    /**
     * Resets the store to initial values for a clean task run.
     * Called when the user returns to the configuration screen.
     */
    reset() {
        this.videoFile = null;
        this.videoUrl = null;
        this.activeSessionId = null;
        this.inferenceResults = [];
        this.currentShotIndex = -1;
        this.totalInferenceTime = "0.0s";
        this.activeModelName = "NONE";
    }
};
```

File 57-58: `static\js\modules\playground\view.js`
```js
/**
 * View.js
 * Playground UI Module.
 * Handles DOM manipulation, view state transitions, hierarchical terminal 
 * rendering, and history list synchronization.
 */

import { store } from './store.js';

// DOM Element Registry
const getViews = () => ({
    config: document.getElementById('configView'),
    processing: document.getElementById('processingView'),
    results: document.getElementById('resultsView')
});

const getHud = () => ({
    container: document.getElementById('simHud'),
    time: document.getElementById('inferenceTime'),
    badge: document.getElementById('modeBadge')
});

const getDebug = () => ({
    panel: document.getElementById('debugPanel'),
    btn: document.getElementById('toggleDebugBtn'),
    tabs: document.querySelectorAll('.debug-tab')
});

const getInspector = () => ({
    content: document.getElementById('inspectorContent')
});

let lastRenderedLogCount = 0;

/**
 * Transitions between the three primary application states.
 * @param {string} state - 'config', 'processing', or 'results'
 */
export function toggleViewState(state) {
    const views = getViews();
    Object.values(views).forEach(v => {
        if (v) v.classList.remove('active');
    });
    if (views[state]) {
        views[state].classList.add('active');
    }

    if (state !== 'results') {
        hideDebugPanel();
    }

    if (state === 'config') {
        lastRenderedLogCount = 0;
        const consoleEl = document.getElementById('consoleLog');
        if (consoleEl) consoleEl.innerHTML = '';

        // Reset Inspector on config return
        const inspector = getInspector();
        if (inspector.content) {
            inspector.content.innerHTML = `
                <div class="placeholder-state">
                    <i class="fa-solid fa-crosshairs"></i>
                    <p>Hover over a parameter or select an asset to view technical details.</p>
                </div>
            `;
        }
    }
}

/**
 * Updates the content of the Context Inspector Panel (Column 3).
 * @param {string} type - The category of information ('model', 'adapter', 'param').
 * @param {Object} data - The data object to render.
 */
export function updateInspector(type, data) {
    const container = getInspector().content;
    if (!container) return;

    let html = '';

    if (type === 'model') {
        // Find VRAM capability match
        // Note: GPU caps are passed via server template, but we can infer or pass via JS global
        html = `
            <div class="info-tile">
                <span class="info-label">Selected Checkpoint</span>
                <span class="info-val" style="color:var(--accent);">${data.id}</span>
                <div class="info-desc">
                    Base Foundation Model. Defines the primary reasoning capabilities and visual encoder resolution.
                </div>
            </div>
            <div class="info-tile">
                <span class="info-label">Family</span>
                <span class="info-val">${data.id.toLowerCase().includes('qwen') ? 'Qwen2-VL' : 'Unknown / Cloud'}</span>
            </div>
        `;
    }
    else if (type === 'adapter') {
        html = `
            <div class="info-tile">
                <span class="info-label">Active LoRA Adapter</span>
                <span class="info-val" style="color:var(--purple-accent);">${data.name}</span>
            </div>
            <div class="info-tile">
                <span class="info-label">Training Rank (r)</span>
                <span class="info-val">${data.rank || 'N/A'}</span>
            </div>
            <div class="info-tile">
                <span class="info-label">Alpha Scaling (α)</span>
                <span class="info-val">${data.alpha || 'N/A'}</span>
            </div>
            <div class="info-desc">
                Targeted fine-tuning layer. Modifies attention weights to specialize in narrative boundary detection.
            </div>
        `;
    }
    else if (type === 'param') {
        html = `
            <div class="info-tile">
                <span class="info-label">Parameter Focus</span>
                <span class="info-val" style="color:var(--success);">${data.label}</span>
            </div>
            <div class="info-tile">
                <span class="info-label">Typical Range</span>
                <span class="info-val">${data.range}</span>
            </div>
            <div class="info-desc">
                ${data.desc}
            </div>
        `;
    }

    container.innerHTML = html;
}

/**
 * Helper to open/close system modal dialogs.
 */
export function toggleModal(modalId, visible) {
    const dialog = document.getElementById(modalId);
    if (!dialog) return;

    if (visible) {
        dialog.showModal();
    } else {
        dialog.close();
    }
}

/**
 * Toggles the visibility of the overlay debug panel.
 */
export function toggleDebugPanel() {
    const debug = getDebug();
    if (!debug.panel) return false;
    const isActive = debug.panel.classList.toggle('active');
    if (debug.btn) debug.btn.classList.toggle('btn-primary', isActive);
    return isActive;
}

export function hideDebugPanel() {
    const debug = getDebug();
    if (debug.panel) debug.panel.classList.remove('active');
    if (debug.btn) debug.btn.classList.remove('btn-primary');
}

export function setActiveDebugTab(logType) {
    const debug = getDebug();
    debug.tabs.forEach(tab => {
        const isMatch = tab.dataset.log === logType;
        tab.classList.toggle('active', isMatch);
    });
}

export function updateHUD(duration, modelName) {
    const hud = getHud();
    if (hud.container) {
        hud.container.style.opacity = '1';
        hud.time.textContent = duration;
        hud.badge.textContent = modelName.split('/').pop();
    }
}

export function resetHUD() {
    const hud = getHud();
    if (hud.container) hud.container.style.opacity = '0';
}

/**
 * Hierarchical Terminal Renderer.
 */
export function logConsole(content, isSuccess = false, isError = false) {
    const consoleEl = document.getElementById('consoleLog');
    if (!consoleEl) return;

    if (Array.isArray(content)) {
        const newLines = content.slice(lastRenderedLogCount);
        if (newLines.length === 0) return;

        newLines.forEach(line => renderStyledLine(consoleEl, line));
        lastRenderedLogCount = content.length;
    }
    else {
        const hasLevelTag = /\[(CLIENT|PIPELINE|SCENE_DETECT|VLM|TOKEN|PROMPT|ERROR)\]/.test(content);
        const timestamp = new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit', second: '2-digit' });

        const finalLine = hasLevelTag
            ? `[${timestamp}] ${content}`
            : `[${timestamp}] [CLIENT] ${content}`;

        renderStyledLine(consoleEl, finalLine, isSuccess, isError);
    }

    consoleEl.scrollTop = consoleEl.scrollHeight;
}

function renderStyledLine(container, rawLine, forceSuccess = false, forceError = false) {
    const div = document.createElement('div');
    div.className = 'log-line';

    let styled = rawLine.replace(/\[(\d{2}:\d{2}:\d{2})\]/g, '<span class="ts">[$1]</span>');

    const levelColors = {
        'CLIENT': 'var(--accent)',
        'PIPELINE': '#818cf8',
        'SCENE_DETECT': '#10b981',
        'PROMPT': '#fbbf24',
        'VLM': '#a78bfa',
        'TOKEN': '#22d3ee',
        'ERROR': 'var(--break-border)'
    };

    Object.entries(levelColors).forEach(([level, color]) => {
        const regex = new RegExp(`\\[${level}\\]`, 'g');
        styled = styled.replace(regex, `<span style="color:${color}; font-weight:900;">[${level}]</span>`);
    });

    styled = styled.replace(/→/g, '<span style="color:var(--accent); font-weight:bold;">→</span>');
    styled = styled.replace(/✓/g, '<span style="color:var(--success); font-weight:bold;">✓</span>');
    styled = styled.replace(/🎬/g, '<span style="filter: drop-shadow(0 0 2px var(--accent))">🎬</span>');

    if (forceSuccess || styled.includes('COMPLETE') || styled.includes('Inference Complete') || styled.includes('✓')) {
        div.style.color = 'var(--success)';
    }
    if (forceError || styled.includes('[ERROR]') || styled.includes('FAILURE')) {
        div.style.color = 'var(--break-border)';
    }

    div.innerHTML = styled;
    container.appendChild(div);
}

export function updateProgressBar(pct) {
    const progressBar = document.getElementById('progressBar');
    if (progressBar) progressBar.style.width = `${pct}%`;
}

/**
 * Renders the session history archive.
 * Displays ALL configuration parameters in the s-meta block.
 */
export function renderSessionList(sessions, onSelect, onDelete) {
    const listContainer = document.getElementById('sessionList');
    if (!listContainer) return;

    listContainer.innerHTML = '';

    if (!sessions.length) {
        listContainer.innerHTML = `
            <div class="sys-label" style="text-align:center; padding:3rem; opacity:0.3;">
                Evaluation archive is empty.
            </div>`;
        return;
    }

    sessions.forEach(s => {
        const item = document.createElement('div');
        item.className = 'session-item';

        const date = new Date(s.timestamp).toLocaleString();
        const modelName = s.model_id.split('/').pop();

        const formatDuration = (sec) => {
            if (sec === undefined || sec === null) return '00:00:00.000';
            const h = Math.floor(sec / 3600);
            const m = Math.floor((sec % 3600) / 60);
            const s = Math.floor(sec % 60);
            const ms = Math.round((sec % 1) * 1000);
            const pad = (n, z = 2) => n.toString().padStart(z, '0');
            return `${pad(h)}:${pad(m)}:${pad(s)}.${pad(ms, 3)}`;
        };

        const runtime = s.duration ? formatDuration(s.duration) : 'ARCHIVED';
        const videoDur = s.video_duration ? formatDuration(s.video_duration) : null;

        const p = s.inference_params || {};
        let paramTags = '';

        // Core Hyperparameters
        if (p.temperature !== undefined) paramTags += `<span class="s-tag">T:${p.temperature}</span>`;
        if (p.top_p !== undefined) paramTags += `<span class="s-tag">P:${p.top_p}</span>`;
        if (p.max_tokens !== undefined) paramTags += `<span class="s-tag">Tk:${p.max_tokens}</span>`;
        if (p.repetition_penalty !== undefined) paramTags += `<span class="s-tag">Rep:${p.repetition_penalty}</span>`;

        // LoRA Scaling
        if (p.lora_scale !== undefined && p.lora_scale !== 1.0) {
            paramTags += `<span class="s-tag">Scale:${p.lora_scale}</span>`;
        }

        // Special Modes
        if (p.high_fidelity_mode) {
            paramTags += `<span class="s-tag" style="background:var(--success); color:#000; border-color:var(--success); font-weight:800;">HiFi</span>`;
        }

        if (p.bypass_validation) {
            paramTags += `<span class="s-tag" style="background:var(--break-border); color:#fff; border-color:var(--break-border);">FORCE</span>`;
        }

        item.innerHTML = `
            <div class="s-top">
                <span class="s-name" title="${s.video_filename}">${s.video_filename}</span>
                <div style="display:flex; align-items:center; gap:8px;">
                    <span class="s-date">${date}</span>
                    <i class="fa-solid fa-trash" style="font-size:0.7rem; color:var(--text-muted); cursor:pointer;" 
                       title="Delete Session" data-delete="${s.session_id}"></i>
                </div>
            </div>
            <div class="s-meta">
                <span class="s-tag model" title="${s.model_id}">${modelName}</span>
                ${s.adapter ? `<span class="s-tag" style="background:#4c1d95; color:#ddd6fe;" title="${s.adapter}">Adapter: ${s.adapter.split(/[\\\\/]/).pop()}</span>` : ''}
                ${videoDur ? `<span class="s-tag">Dur: ${videoDur}</span>` : ''}
                <span class="s-tag">Win: ${s.window_size}</span>
                <span class="s-tag">Run: ${runtime}</span>
                ${paramTags}
            </div>
        `;

        const delBtn = item.querySelector('[data-delete]');
        delBtn.onclick = (e) => {
            e.stopPropagation();
            onDelete(s.session_id);
        };

        item.onclick = () => onSelect(s.session_id);
        listContainer.appendChild(item);
    });
}
```

File 58-58: `templates\playground.html`
```html
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Playground | Scene Detection Sandbox</title>

    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <!-- Design System & Components -->
    <link rel="stylesheet" href="{{ url_for('static', filename='css/variables.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/modules/playground.css') }}">
</head>

<body>

    <header>
        <button class="btn btn-secondary" onclick="window.location.reload()" title="New Session"
            style="width: 40px; height: 40px; padding: 0; flex-shrink: 0;">
            <i class="fa-solid fa-rotate-right"></i>
        </button>

        <div class="project-info">
            <div class="sys-label">Multimodal Evaluation sandbox // Standalone</div>
            <h1 id="playgroundTitle">Scene Detection Playground</h1>
        </div>

        <div class="stats-hud" id="simHud" style="opacity: 0;">
            <div class="hud-item">
                <span class="hud-val" id="inferenceTime">0.0s</span>
                <span class="hud-lbl">Runtime</span>
            </div>
            <div class="hud-divider"></div>
            <div class="hud-item">
                <span class="hud-val highlight" id="modeBadge">VLM</span>
                <span class="hud-lbl">Reasoner</span>
            </div>
        </div>
    </header>

    <script>
        window.CLOUD_MODELS = {{ cloud_models | tojson }};
        window.ADAPTER_MANIFEST = {{ adapters | tojson }};
    </script>

    <main id="mainContainer">

        <!-- STATE 1: MISSION CONTROL CONFIGURATION -->
        <div id="configView" class="view-state active">
            <div class="mission-control-layout">

                <!-- COLUMN 1: ASSET HUB (Persistent) -->
                <div class="mc-column mc-sidebar">
                    <div class="mc-panel asset-hub">
                        <div class="panel-header">
                            <i class="fa-solid fa-database"></i> Asset Library
                        </div>
                        <div class="asset-actions">
                            <button id="openImportModalBtn" class="btn btn-secondary btn-sm" style="flex:1;">
                                <i class="fa-solid fa-cloud-arrow-down"></i> Import Model
                            </button>
                            <button id="openUploadModalBtn" class="btn btn-secondary btn-sm" style="flex:1;">
                                <i class="fa-solid fa-upload"></i> Upload LoRA
                            </button>
                        </div>
                        <div class="asset-stats">
                            <div class="stat-row">
                                <span>Base Models</span>
                                <span class="val">{{ models|length }}</span>
                            </div>
                            <div class="stat-row">
                                <span>Adapters</span>
                                <span class="val">{{ adapters|length }}</span>
                            </div>
                        </div>
                    </div>

                    <div class="mc-panel archive-hub">
                        <div class="panel-header">
                            <i class="fa-solid fa-clock-rotate-left"></i> Mission Archive
                        </div>
                        <div class="archive-list-container" id="sessionList">
                            <!-- Populated by JS -->
                            <div class="sys-label" style="text-align: center; padding: 2rem; opacity: 0.5;">
                                <i class="fa-solid fa-spinner fa-spin"></i><br>Loading Archive...
                            </div>
                        </div>
                    </div>
                </div>

                <!-- COLUMN 2: WORKFLOW CANVAS (Transient) -->
                <div class="mc-column mc-canvas">

                    <!-- Step 1: Reasoner -->
                    <div class="mc-step">
                        <div class="step-label">01 // REASONER CONFIGURATION</div>
                        <div class="step-content">
                            <div class="form-group">
                                <label class="config-label">Base Model Checkpoint</label>
                                <select id="modelSelect" class="dataset-select" style="width: 100%;"
                                    data-inspect="model_select">
                                    <option value="" disabled selected>-- Select Base Model --</option>
                                    {% for model in models %}
                                    <option value="{{ model }}">{{ model }}</option>
                                    {% endfor %}
                                </select>
                            </div>

                            <div id="adapterContainer" class="form-group" style="margin-top: 12px; display: none;">
                                <div style="display:flex; justify-content:space-between; align-items:flex-end;">
                                    <label class="config-label">LoRA Adapter</label>
                                    <div class="force-mode-container" style="border:none; margin:0; padding:0;">
                                        <input type="checkbox" id="forceLoadAdapter" class="force-checkbox">
                                        <label for="forceLoadAdapter" class="force-label" style="margin-left:6px;"
                                            data-inspect="force_mode">Force Load</label>
                                    </div>
                                </div>
                                <select id="adapterPath" class="dataset-select" style="width: 100%;" disabled
                                    data-inspect="adapter_select">
                                    <option value="">Select Base Model First</option>
                                </select>
                                <div id="adapterMetaHint" class="sys-label"
                                    style="font-size:0.55rem; color:var(--text-muted); margin-top:4px; display:none;">
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- Step 2: Media -->
                    <div class="mc-step">
                        <div class="step-label">02 // SOURCE MEDIA</div>
                        <div class="step-content">
                            <div class="upload-area" id="dropZone" style="min-height: 100px; padding: 1.5rem;">
                                <div id="dropPrompt">
                                    <i class="fa-solid fa-film upload-icon" style="font-size: 1.2rem;"></i>
                                    <div class="upload-text">Drag & Drop Video Source</div>
                                </div>
                                <input type="file" id="videoInput" accept="video/*">
                            </div>
                        </div>
                    </div>

                    <!-- Step 3: Parameters -->
                    <div class="mc-step">
                        <div class="step-label">03 // EXECUTION PARAMETERS</div>
                        <div class="step-content parameter-grid">

                            <!-- Sampling Group -->
                            <div class="param-group">
                                <div class="group-title">Sampling Strategy</div>
                                <div class="p-row">
                                    <label class="p-label">Temperature</label>
                                    <input type="number" id="paramTemp" class="p-input"
                                        value="{{ defaults.temperature }}" step="0.1" min="0" max="2"
                                        data-inspect="temp">
                                </div>
                                <div class="p-row">
                                    <label class="p-label">Top P</label>
                                    <input type="number" id="paramTopP" class="p-input" value="{{ defaults.top_p }}"
                                        step="0.05" min="0" max="1" data-inspect="top_p">
                                </div>
                            </div>

                            <!-- Generation Group -->
                            <div class="param-group">
                                <div class="group-title">Generation Control</div>
                                <div class="p-row">
                                    <label class="p-label">Max Tokens</label>
                                    <input type="number" id="paramMaxTokens" class="p-input"
                                        value="{{ defaults.max_tokens }}" step="128" data-inspect="max_tokens">
                                </div>
                                <div class="p-row">
                                    <label class="p-label">Rep. Penalty</label>
                                    <input type="number" id="paramRepPenalty" class="p-input"
                                        value="{{ defaults.repetition_penalty }}" step="0.05" min="1.0" max="2.0"
                                        data-inspect="rep_penalty">
                                </div>
                            </div>

                            <!-- Context Group -->
                            <div class="param-group full-width">
                                <div class="group-title">Context & Scaling</div>
                                <div class="p-row" style="margin-bottom: 8px;">
                                    <label class="p-label">Window Size</label>
                                    <div class="stepper-group" style="flex:1;">
                                        <button id="winDec" class="step-btn" type="button"><i
                                                class="fa-solid fa-minus"></i></button>
                                        <input type="number" id="windowSize" class="stepper-input" value="32" min="8"
                                            max="128" readonly data-inspect="window_size">
                                        <button id="winInc" class="step-btn" type="button"><i
                                                class="fa-solid fa-plus"></i></button>
                                    </div>
                                </div>
                                <div class="p-grid-2">
                                    <div>
                                        <label class="p-label">LoRA Scale</label>
                                        <input type="number" id="paramLoraScale" class="p-input"
                                            value="{{ defaults.lora_scale }}" step="0.1" min="0.0" max="2.0"
                                            data-inspect="lora_scale">
                                    </div>
                                    <div>
                                        <label class="p-label">Stream Rate</label>
                                        <input type="number" id="streamInterval" class="p-input"
                                            value="{{ defaults.stream_interval }}" step="10" min="10" max="500"
                                            data-inspect="stream_rate">
                                    </div>
                                </div>
                            </div>

                            <!-- Hardware Mode -->
                            <div class="param-group full-width"
                                style="border-color: var(--border-light); background: rgba(0,0,0,0.2);">
                                <div class="force-mode-container" style="border:none; margin:0; padding:0;">
                                    <input type="checkbox" id="highFidelityMode" class="force-checkbox hifi-checkbox" {%
                                        if not gpu_capabilities.is_capable %}disabled{% endif %}>
                                    <div style="display:flex; flex-direction:column; margin-left: 8px;">
                                        <label for="highFidelityMode" class="force-label"
                                            data-inspect="hifi_mode">High-Fidelity Mode (BF16)</label>
                                        <span style="font-size:0.55rem; color:var(--text-muted);">
                                            {% if gpu_capabilities.is_capable %}
                                            Capable ({{ gpu_capabilities.total_vram_gb }}GB VRAM)
                                            {% else %}
                                            Unavailable (< 12GB VRAM) {% endif %} </span>
                                    </div>
                                </div>
                            </div>

                            <!-- Prompts (Collapsible) -->
                            <div class="param-group full-width">
                                <div class="group-title" id="promptToggle"
                                    style="cursor: pointer; display:flex; justify-content:space-between;">
                                    <span>System & Context Prompts</span>
                                    <i class="fa-solid fa-chevron-down"></i>
                                </div>
                                <div id="promptBody" style="display:none; margin-top: 10px;">
                                    <label class="p-label">System Prompt</label>
                                    <textarea id="paramSystem" class="control-input"
                                        style="height: 60px; font-size: 0.6rem; margin-bottom: 8px;">{{ defaults.system_prompt }}</textarea>

                                    <label class="p-label">Main Template</label>
                                    <textarea id="paramMain" class="control-input"
                                        style="height: 60px; font-size: 0.6rem;">{{ defaults.main_prompt }}</textarea>
                                </div>
                            </div>

                        </div>
                    </div>

                    <!-- Step 4: Launch -->
                    <div class="mc-step no-border">
                        <button id="processBtn" class="btn btn-primary launch-btn" disabled>
                            <span class="launch-text">INITIALIZE MISSION</span>
                            <span class="launch-sub">Awaiting Configuration</span>
                        </button>
                    </div>

                </div>

                <!-- COLUMN 3: INSPECTOR (Contextual) -->
                <div class="mc-column mc-inspector">
                    <div class="mc-panel inspector-panel">
                        <div class="panel-header">
                            <i class="fa-solid fa-circle-info"></i> Context Inspector
                        </div>
                        <div id="inspectorContent" class="inspector-content">
                            <!-- Dynamic Content -->
                            <div class="placeholder-state">
                                <i class="fa-solid fa-crosshairs"></i>
                                <p>Hover over a parameter or select an asset to view technical details.</p>
                            </div>
                        </div>
                    </div>

                    <div class="mc-panel help-panel">
                        <div class="panel-header">
                            <i class="fa-solid fa-book-open"></i> Quick Reference
                        </div>
                        <div class="help-content">
                            <div class="help-item">
                                <span class="key">Chunking</span>
                                <span class="val">Sliding window analysis</span>
                            </div>
                            <div class="help-item">
                                <span class="key">Protocol</span>
                                <span class="val">5-Stage Forensic Audit</span>
                            </div>
                        </div>
                    </div>
                </div>

            </div>
        </div>

        <!-- STATE 2: PIPELINE MONITORING -->
        <div id="processingView" class="view-state">
            <div class="monitor-deck">
                <!-- Left Flank: System Vitals -->
                <div class="monitor-flank left">
                    <!-- CPU Widget -->
                    <div class="monitor-widget primary">
                        <div class="monitor-label">
                            <i class="fa-solid fa-microchip"></i> CPU CORE
                        </div>
                        <div class="monitor-main">
                            <div class="monitor-val" id="monitorCpuVal">--%</div>
                            <div class="monitor-bar-track">
                                <div class="monitor-bar-fill" id="monitorCpuBar" style="width: 0%"></div>
                            </div>
                        </div>
                    </div>

                    <!-- RAM Widget -->
                    <div class="monitor-widget">
                        <div class="monitor-label">
                            <i class="fa-solid fa-memory"></i> SYSTEM RAM
                        </div>
                        <div class="monitor-main">
                            <div class="monitor-val" id="monitorRamVal">--%</div>
                            <div class="monitor-bar-track">
                                <div class="monitor-bar-fill" id="monitorRamBar" style="width: 0%"></div>
                            </div>
                        </div>
                        <div class="monitor-detail" id="monitorRamDetail">-- / -- GB</div>
                    </div>

                    <!-- Disk Widget -->
                    <div class="monitor-widget">
                        <div class="monitor-label">
                            <i class="fa-solid fa-hard-drive"></i> STORAGE (LOCAL)
                        </div>
                        <div class="monitor-main">
                            <div class="monitor-val" id="monitorDiskVal">--%</div>
                            <div class="monitor-bar-track">
                                <div class="monitor-bar-fill" id="monitorDiskBar" style="width: 0%"></div>
                            </div>
                        </div>
                        <div class="monitor-detail" id="monitorDiskDetail">-- / -- GB</div>
                    </div>

                    <!-- GPU Container (Moved from Right) -->
                    <div id="monitorGpuContainer" style="display: flex; flex-direction: column; gap: 12px;">
                        <!-- GPU specific widgets injected by JS -->
                    </div>
                </div>

                <!-- Center: Terminal Area -->
                <div class="terminal-wrapper">
                    <!-- Stop Button (Moved here) -->
                    <div class="abort-container">
                        <button id="abortBtn" class="btn btn-danger">
                            <i class="fa-solid fa-ban"></i> Stop Execution
                        </button>
                    </div>

                    <!-- Terminal Card -->
                    <div class="terminal-card">
                        <div class="terminal-header">
                            <div class="term-dots"><span></span><span></span><span></span></div>
                            <span class="term-title">playground_inference_pipeline.log</span>
                        </div>
                        <div class="terminal-body" id="consoleLog"></div>
                        <div class="progress-container">
                            <div class="progress-bar" id="progressBar"></div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- STATE 3: RESULTS -->
        <div id="resultsView" class="view-state results-layout">
            <div class="video-column">
                <div class="video-container">
                    <video id="videoPlayer" controls preload="auto"></video>
                    <div id="videoOverlay"><span id="overlayShotId"></span></div>
                </div>
                <div class="control-deck">
                    <div class="nav-group">
                        <button class="nav-btn" onclick="window.skipToScene(-1)" title="Prev Scene"><i
                                class="fa-solid fa-backward-fast"></i></button>
                        <button class="nav-btn" onclick="window.skipToShot(-1)" title="Prev Shot"><i
                                class="fa-solid fa-backward-step"></i></button>
                    </div>
                    <div class="deck-separator"></div>
                    <div class="nav-group">
                        <button class="nav-btn" onclick="window.skipToShot(1)" title="Next Shot"><i
                                class="fa-solid fa-forward-step"></i></button>
                        <button class="nav-btn" onclick="window.skipToScene(1)" title="Next Scene"><i
                                class="fa-solid fa-forward-fast"></i></button>
                    </div>
                </div>
            </div>

            <div class="sidebar" style="position: relative;">
                <div class="sidebar-sticky-header">
                    <div
                        style="display:flex; justify-content:space-between; align-items:center; margin-bottom: 0.8rem;">
                        <span class="sys-label" style="color: var(--accent);">VLM OUTPUT // EVALUATION</span>
                        <div class="sidebar-title">Forensic Narrative Audit</div>
                    </div>
                    <div class="sidebar-actions" style="display: grid; grid-template-columns: 1fr 120px; gap: 8px;">
                        <button id="resetPlaygroundBtn" class="btn btn-secondary">
                            <i class="fa-solid fa-rotate-left"></i> Return to Config
                        </button>
                        <button id="toggleDebugBtn" class="btn btn-secondary">
                            <i class="fa-solid fa-terminal"></i> Debug
                        </button>
                    </div>
                </div>
                <div id="shotGrid" class="read-only-grid"></div>
                <div id="debugPanel" class="debug-panel">
                    <div class="debug-tabs">
                        <div class="debug-tab active" data-log="pipeline">Pipeline Trace</div>
                        <div class="debug-tab" data-log="logs">VLM Interaction Logs</div>
                    </div>
                    <div id="debugContent" class="debug-content"></div>
                </div>
            </div>
        </div>
    </main>

    <!-- MODALS -->

    <!-- Import Model Modal -->
    <dialog id="importModal" class="system-dialog">
        <div class="dialog-content">
            <div class="dialog-header">
                <i class="fa-solid fa-cloud-arrow-down"></i> Import from HuggingFace
            </div>
            <div class="dialog-body">
                <div class="form-group">
                    <label class="config-label">Model ID</label>
                    <input type="text" id="hfModelId" class="control-input" placeholder="e.g. google/siglip-so400m">
                    <span style="font-size: 0.55rem; color: var(--text-muted); margin-top: 4px;">Target model will be
                        cached to local storage.</span>
                </div>

                <!-- Import Progress -->
                <div id="importProgressContainer" style="margin-top: 16px; display: none;">
                    <div style="display: flex; justify-content: space-between; margin-bottom: 4px;">
                        <span id="importStatusText"
                            style="font-size: 0.6rem; color: var(--text-dim);">Initializing...</span>
                        <span id="importPercentText" style="font-size: 0.6rem; color: var(--accent);">0%</span>
                    </div>
                    <div style="height: 4px; background: #000; border-radius: 2px; overflow: hidden;">
                        <div id="importProgressBar"
                            style="width: 0%; height: 100%; background: var(--accent); transition: width 0.3s;"></div>
                    </div>
                </div>
            </div>
            <div class="dialog-actions">
                <button class="btn btn-secondary modal-close-btn" data-modal="importModal">Cancel</button>
                <button id="importModelBtn" class="btn btn-primary">Download</button>
            </div>
        </div>
    </dialog>

    <!-- Upload Adapter Modal -->
    <dialog id="uploadModal" class="system-dialog">
        <div class="dialog-content">
            <div class="dialog-header">
                <i class="fa-solid fa-upload"></i> Upload LoRA Adapter
            </div>
            <div class="dialog-body">
                <div class="upload-area" id="adapterDropZone"
                    style="min-height: 100px; padding: 1rem; border-color: var(--border);">
                    <div style="pointer-events: none;">
                        <i class="fa-solid fa-file-zipper upload-icon" style="font-size: 1rem;"></i>
                        <div class="upload-text" style="font-size: 0.7rem;">Drop .zip file here</div>
                    </div>
                    <input type="file" id="adapterUploadInput" accept=".zip">
                </div>

                <!-- Upload Progress -->
                <div id="uploadProgressContainer" style="margin-top: 16px; display: none;">
                    <div style="display: flex; justify-content: space-between; margin-bottom: 4px;">
                        <span id="uploadStatusText"
                            style="font-size: 0.6rem; color: var(--text-dim);">Uploading...</span>
                        <span id="uploadPercentText" style="font-size: 0.6rem; color: var(--accent);">0%</span>
                    </div>
                    <div style="height: 4px; background: #000; border-radius: 2px; overflow: hidden;">
                        <div id="uploadProgressBar"
                            style="width: 0%; height: 100%; background: var(--accent); transition: width 0.3s;"></div>
                    </div>
                </div>
            </div>
            <div class="dialog-actions">
                <button class="btn btn-secondary modal-close-btn" data-modal="uploadModal">Cancel</button>
            </div>
        </div>
    </dialog>

    <script type="module"
        src="{{ url_for('static', filename='js/modules/playground/playground_bootstrap.js') }}"></script>
</body>

</html>
```

